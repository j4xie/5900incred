{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05a - 使用 LLM 生成 OCEAN Ground Truth Labels\n",
    "\n",
    "**目标**: 使用 LLama 3 (via Hugging Face) 为 500 个样本的贷款描述生成 OCEAN 人格评分\n",
    "\n",
    "## 工作流程:\n",
    "1. 从干净数据集中选择 500 个样本（平衡选择：250 违约 + 250 正常）\n",
    "2. 使用 Hugging Face Inference API + LLama 3 模型分析每个 `desc` 字段\n",
    "3. 为每个样本生成 5 个 OCEAN 人格分数 (0-1 范围)\n",
    "4. 保存为 CSV 文件用于后续 Ridge 回归权重学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "import time\n",
    "\n",
    "# 设置显示选项\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"库加载成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 加载干净的建模数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 加载干净的建模数据\nprint(\"加载干净的建模数据...\")\ndf = pd.read_csv('data/loan_clean_for_modeling.csv', low_memory=False)\n\nprint(f\"数据形状: {df.shape[0]:,} 行 × {df.shape[1]} 列\")\nprint(f\"\\n目标变量分布:\")\nprint(df['target'].value_counts())\nprint(f\"\\ndesc 字段非空样本: {df['desc'].notna().sum():,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 平衡选择 500 个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平衡选择 500 个样本 (250 违约 + 250 正常)\n",
    "print(\"选择 500 个平衡样本...\\n\")\n",
    "\n",
    "# 分离违约和正常样本\n",
    "df_charged_off = df[df['target'] == 1].copy()\n",
    "df_fully_paid = df[df['target'] == 0].copy()\n",
    "\n",
    "print(f\"违约样本总数: {len(df_charged_off):,}\")\n",
    "print(f\"正常样本总数: {len(df_fully_paid):,}\")\n",
    "\n",
    "# 随机选择 250 个样本每类\n",
    "np.random.seed(42)\n",
    "sample_charged_off = df_charged_off.sample(n=min(250, len(df_charged_off)), random_state=42)\n",
    "sample_fully_paid = df_fully_paid.sample(n=min(250, len(df_fully_paid)), random_state=42)\n",
    "\n",
    "# 合并\n",
    "df_sample_500 = pd.concat([sample_charged_off, sample_fully_paid], ignore_index=False)\n",
    "df_sample_500 = df_sample_500.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n选中样本总数: {len(df_sample_500):,}\")\n",
    "print(f\"目标变量分布:\")\n",
    "print(df_sample_500['target'].value_counts())\n",
    "\n",
    "# 检查 desc 字段\n",
    "print(f\"\\ndesc 字段非空样本: {df_sample_500['desc'].notna().sum():,}\")\n",
    "\n",
    "# 显示示例\n",
    "print(\"\\n示例 desc 文本:\")\n",
    "for i in range(3):\n",
    "    desc = df_sample_500.iloc[i]['desc']\n",
    "    if pd.notna(desc):\n",
    "        desc_str = str(desc)[:200]\n",
    "        print(f\"\\n样本 {i+1}: {desc_str}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 初始化 Hugging Face Inference Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Hugging Face Inference Client\n",
    "# 注意: 需要设置环境变量 HF_TOKEN 或直接提供 token\n",
    "\n",
    "print(\"初始化 Hugging Face Inference Client...\\n\")\n",
    "\n",
    "# 方法 1: 使用环境变量 (推荐)\n",
    "import os\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"⚠️ 警告: 未找到 HF_TOKEN 环境变量\")\n",
    "    print(\"请设置环境变量: export HF_TOKEN='your_hugging_face_token'\")\n",
    "    print(\"\\n你也可以直接在下面的代码中设置 token:\")\n",
    "    print(\"hf_token = 'your_token_here'\")\n",
    "    \n",
    "    # 如果没有设置环境变量，请在这里直接输入\n",
    "    # hf_token = 'your_hugging_face_token_here'\n",
    "\n",
    "# 初始化 client\n",
    "try:\n",
    "    client = InferenceClient(token=hf_token)\n",
    "    print(\"✅ Hugging Face Client 初始化成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 初始化失败: {e}\")\n",
    "    print(\"\\n请确保:\")\n",
    "    print(\"1. 已安装 huggingface_hub: pip install huggingface_hub\")\n",
    "    print(\"2. 已设置有效的 HF token\")\n",
    "    print(\"3. LLama 3 模型访问权限已获得\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 定义 OCEAN 提取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ocean_from_llm(desc_text, client, max_retries=3):\n",
    "    \"\"\"\n",
    "    使用 LLama 3 从文本中提取 OCEAN 人格分数\n",
    "    \n",
    "    参数:\n",
    "        desc_text: 贷款申请描述文本\n",
    "        client: Hugging Face InferenceClient\n",
    "        max_retries: 最大重试次数\n",
    "    \n",
    "    返回:\n",
    "        dict: 包含 5 个 OCEAN 分数 (0-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 缺失或空文本处理\n",
    "    if pd.isna(desc_text) or str(desc_text).strip() == '':\n",
    "        return {\n",
    "            'openness': 0.5,\n",
    "            'conscientiousness': 0.5,\n",
    "            'extraversion': 0.5,\n",
    "            'agreeableness': 0.5,\n",
    "            'neuroticism': 0.5,\n",
    "            'status': 'empty_text'\n",
    "        }\n",
    "    \n",
    "    # 构建 prompt\n",
    "    prompt = f\"\"\"Analyze the following loan application description and rate the borrower's personality on the OCEAN traits (Big Five Personality Model) on a scale of 0 to 1.\n",
    "\n",
    "Description:\n{desc_text}\n\n\n---\n\nOutput ONLY a JSON object (no markdown, no extra text) with the following format:\n{{\n  \"openness\": 0.X,\n  \"conscientiousness\": 0.X,\n  \"extraversion\": 0.X,\n  \"agreeableness\": 0.X,\n  \"neuroticism\": 0.X\n}}\n\nWhere:\n- Openness (0-1): Imagination, curiosity, creativity, willingness to explore new ideas\n- Conscientiousness (0-1): Responsibility, discipline, organization, reliability\n- Extraversion (0-1): Sociability, energy, assertiveness, positive emotions\n- Agreeableness (0-1): Cooperation, trust, altruism, empathy\n- Neuroticism (0-1): Emotional instability, anxiety, vulnerability\n\nRespond with ONLY the JSON object.\"\"\"\n",
    "    \n",
    "    # 重试逻辑\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # 调用 LLama 3\n",
    "            response = client.text_generation(\n",
    "                prompt,\n",
    "                model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            \n",
    "            # 清理响应\n",
    "            response_text = str(response).strip()\n",
    "            \n",
    "            # 提取 JSON\n",
    "            # 如果包含 markdown 代码块，提取其中的内容\n",
    "            if '```json' in response_text:\n",
    "                response_text = response_text.split('```json')[1].split('```')[0]\n",
    "            elif '```' in response_text:\n",
    "                response_text = response_text.split('```')[1].split('```')[0]\n",
    "            \n",
    "            # 尝试解析 JSON\n",
    "            ocean_dict = json.loads(response_text)\n",
    "            \n",
    "            # 验证所有键都存在且值在 0-1 范围内\n",
    "            required_keys = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "            valid = True\n",
    "            \n",
    "            for key in required_keys:\n",
    "                if key not in ocean_dict:\n",
    "                    valid = False\n",
    "                    break\n",
    "                # 确保值在 0-1 范围\n",
    "                value = float(ocean_dict[key])\n",
    "                ocean_dict[key] = np.clip(value, 0, 1)\n",
    "            \n",
    "            if valid:\n",
    "                ocean_dict['status'] = 'success'\n",
    "                return ocean_dict\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"\\n❌ JSON 解析失败，返回默认值\")\n",
    "                return {\n",
    "                    'openness': 0.5,\n",
    "                    'conscientiousness': 0.5,\n",
    "                    'extraversion': 0.5,\n",
    "                    'agreeableness': 0.5,\n",
    "                    'neuroticism': 0.5,\n",
    "                    'status': 'json_parse_error'\n",
    "                }\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"\\n❌ API 调用失败: {e}\")\n",
    "                return {\n",
    "                    'openness': 0.5,\n",
    "                    'conscientiousness': 0.5,\n",
    "                    'extraversion': 0.5,\n",
    "                    'agreeableness': 0.5,\n",
    "                    'neuroticism': 0.5,\n",
    "                    'status': 'api_error'\n",
    "                }\n",
    "            # 等待后重试\n",
    "            time.sleep(2 ** attempt)  # 指数退避\n",
    "    \n",
    "    # 默认返回\n",
    "    return {\n",
    "        'openness': 0.5,\n",
    "        'conscientiousness': 0.5,\n",
    "        'extraversion': 0.5,\n",
    "        'agreeableness': 0.5,\n",
    "        'neuroticism': 0.5,\n",
    "        'status': 'max_retries_exceeded'\n",
    "    }\n",
    "\n",
    "print(\"OCEAN 提取函数定义完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 为 500 个样本生成 OCEAN 标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 OCEAN 标签\n",
    "print(\"开始为 500 个样本生成 OCEAN 标签...\\n\")\n",
    "print(\"预计时间: 取决于 API 速率限制和网络\")\n",
    "print(\"\\n进度:\")\n",
    "\n",
    "ocean_labels = []\n",
    "status_counts = {}\n",
    "\n",
    "for idx, row in df_sample_500.iterrows():\n",
    "    desc = row['desc']\n",
    "    \n",
    "    # 提取 OCEAN 分数\n",
    "    ocean_scores = extract_ocean_from_llm(desc, client)\n",
    "    status = ocean_scores.pop('status', 'unknown')\n",
    "    \n",
    "    # 统计状态\n",
    "    status_counts[status] = status_counts.get(status, 0) + 1\n",
    "    \n",
    "    ocean_labels.append(ocean_scores)\n",
    "    \n",
    "    # 进度显示\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"已处理: {idx + 1} / {len(df_sample_500)}\")\n",
    "    \n",
    "    # 速率限制保护\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        time.sleep(1)  # 每 10 个请求休息 1 秒\n",
    "\n",
    "# 转换为 DataFrame\n",
    "ocean_labels_df = pd.DataFrame(ocean_labels)\n",
    "\n",
    "print(f\"\\n✅ 标签生成完成！\")\n",
    "print(f\"\\n状态统计:\")\n",
    "for status, count in status_counts.items():\n",
    "    print(f\"  {status}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 创建最终的 Ground Truth 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并原始数据和 OCEAN 标签\n",
    "df_ground_truth = df_sample_500[['desc', 'target']].reset_index(drop=True)\n",
    "df_ground_truth = pd.concat([df_ground_truth, ocean_labels_df], axis=1)\n",
    "\n",
    "print(\"Ground Truth 数据集:\")\n",
    "print(f\"形状: {df_ground_truth.shape}\")\n",
    "print(f\"\\n列名: {list(df_ground_truth.columns)}\")\n",
    "\n",
    "# 显示统计\n",
    "print(\"\\nOCEAN 分数统计:\")\n",
    "print(df_ground_truth[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']].describe())\n",
    "\n",
    "# 按目标变量分组显示\n",
    "print(\"\\n按目标变量分组的 OCEAN 分数均值:\")\n",
    "print(df_ground_truth.groupby('target')[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']].mean())\n",
    "\n",
    "# 显示示例\n",
    "print(\"\\n示例数据 (前 5 行):\")\n",
    "display_df = df_ground_truth.copy()\n",
    "display_df['desc'] = display_df['desc'].apply(lambda x: str(x)[:100] + '...' if len(str(x)) > 100 else str(x))\n",
    "print(display_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: 保存 Ground Truth 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 Ground Truth 数据集\n",
    "output_file = 'ocean_ground_truth_500.csv'\n",
    "\n",
    "print(f\"保存 Ground Truth 数据到: {output_file}\\n\")\n",
    "df_ground_truth.to_csv(output_file, index=False)\n",
    "\n",
    "import os\n",
    "file_size = os.path.getsize(output_file) / (1024)  # KB\n",
    "print(f\"✅ 文件大小: {file_size:.2f} KB\")\n",
    "print(f\"✅ 数据行数: {len(df_ground_truth):,}\")\n",
    "print(f\"✅ 数据列数: {len(df_ground_truth.columns)}\")\n",
    "\n",
    "# 也保存为 JSON 格式（便于查看）\n",
    "json_file = 'ocean_ground_truth_500.json'\n",
    "df_ground_truth.to_json(json_file, orient='records', indent=2)\n",
    "print(f\"\\n✅ 也保存为 JSON 格式: {json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: 数据质量验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Ground Truth 数据质量验证\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1️⃣ 缺失值检查\")\n",
    "print(\"-\" * 80)\n",
    "missing_counts = df_ground_truth.isnull().sum()\n",
    "if missing_counts.sum() == 0:\n",
    "    print(\"✅ 没有缺失值\")\n",
    "else:\n",
    "    print(missing_counts)\n",
    "\n",
    "print(\"\\n2️⃣ OCEAN 分数范围验证\")\n",
    "print(\"-\" * 80)\n",
    "ocean_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "valid_range = True\n",
    "for col in ocean_cols:\n",
    "    min_val = df_ground_truth[col].min()\n",
    "    max_val = df_ground_truth[col].max()\n",
    "    in_range = (min_val >= 0) and (max_val <= 1)\n",
    "    status = \"✅\" if in_range else \"❌\"\n",
    "    print(f\"{status} {col}: [{min_val:.4f}, {max_val:.4f}]\")\n",
    "    valid_range = valid_range and in_range\n",
    "\n",
    "print(\"\\n3️⃣ 目标变量平衡检查\")\n",
    "print(\"-\" * 80)\n",
    "target_dist = df_ground_truth['target'].value_counts().sort_index()\n",
    "for target_val, count in target_dist.items():\n",
    "    label = \"Fully Paid\" if target_val == 0 else \"Charged Off\"\n",
    "    pct = count / len(df_ground_truth) * 100\n",
    "    print(f\"{label} (target={target_val}): {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n4️⃣ 文本字段验证\")\n",
    "print(\"-\" * 80)\n",
    "non_empty_desc = df_ground_truth['desc'].notna().sum()\n",
    "print(f\"非空 desc 字段: {non_empty_desc:,} / {len(df_ground_truth):,}\")\n",
    "avg_desc_len = df_ground_truth['desc'].dropna().astype(str).str.len().mean()\n",
    "print(f\"平均 desc 长度: {avg_desc_len:.0f} 字符\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if valid_range and non_empty_desc == len(df_ground_truth):\n",
    "    print(\"✅ 数据质量检查通过！可以进行下一步（Ridge 回归权重训练）\")\n",
    "else:\n",
    "    print(\"⚠️ 注意: 发现一些数据质量问题，请检查上述输出\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: 总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LLM OCEAN Ground Truth 标签生成总结\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1️⃣ 样本信息\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"总样本数: {len(df_ground_truth):,}\")\n",
    "print(f\"违约样本 (target=1): {(df_ground_truth['target']==1).sum():,}\")\n",
    "print(f\"正常样本 (target=0): {(df_ground_truth['target']==0).sum():,}\")\n",
    "\n",
    "print(\"\\n2️⃣ OCEAN 分数信息\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"特征维度: 5 (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\")\n",
    "print(f\"分数范围: [0, 1]\")\n",
    "print(f\"生成方法: LLama 3 via Hugging Face Inference API\")\n",
    "\n",
    "print(\"\\n3️⃣ 生成的文件\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"1. ocean_ground_truth_500.csv (500 行 × 7 列)\")\n",
    "print(f\"   - 列: desc, target, openness, conscientiousness, extraversion, agreeableness, neuroticism\")\n",
    "print(f\"2. ocean_ground_truth_500.json (相同数据，JSON 格式)\")\n",
    "\n",
    "print(\"\\n4️⃣ 下一步\")\n",
    "print(\"-\" * 80)\n",
    "print(\"运行 05b_train_ocean_ridge_weights.ipynb:\")\n",
    "print(\"1. 加载 ocean_ground_truth_500.csv\")\n",
    "print(\"2. 编码 PRE-LOAN 特征\")\n",
    "print(\"3. 训练 Ridge 回归模型 (alpha=0.17)\")\n",
    "print(\"4. 提取并保存权重系数\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Ground Truth 标签生成完成！\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}