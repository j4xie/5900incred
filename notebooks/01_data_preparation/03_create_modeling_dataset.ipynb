{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Create Clean Modeling Dataset",
    "",
    "**Objective**: Create a clean modeling dataset without data leakage",
    "",
    "## Processing Steps:",
    "1. Load data with desc",
    "2. Remove all POST-LOAN features (prevent data leakage)",
    "3. Remove features with coverage < 80% (quality control)",
    "4. Remove METADATA features (except desc - keep for OCEAN extraction)",
    "5. create Objectivevariable（Fully Paid vs Charged Off）",
    "6. Save Clean Dataset",
    "7. Generate quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd",
    "import numpy as np",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "import warnings",
    "warnings.filterwarnings('ignore')",
    "",
    "# display",
    "pd.set_option('display.max_columns', None)",
    "pd.set_option('display.max_rows', 100)",
    "",
    "print(\"Libraries loaded successfully！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Loading raw data print(\"Loading raw data loan.csv...\") df = pd.read_csv('../../data/loan.csv', low_memory=False) print(f\"Raw data: {df.shape[0]:,} rows × {df.shape[1]} columns\") # Filter data with desc print(\"\\nFilter data with desc...\") df_with_desc = df[ df['desc'].notna() & (df['desc'].astype(str).str.strip().str.len() > 1) ].copy() print(f\"Data with desc: {df_with_desc.shape[0]:,} rows × {df_with_desc.shape[1]} columns\") print(f\"Coverage: {len(df_with_desc)/len(df)*100:.2f}%\") # Raw data del df print(\"\\nData loading completed！\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Feature Classification",
    "",
    "Based on 02_feature_selection_and_leakage_check.ipynb analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-LOAN features (Must delete - causes data leakage)",
    "post_loan_features = [",
    "# Payment related (generated after loan issued)",
    "'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',",
    "'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee',",
    "'recoveries', 'collection_recovery_fee',",
    "'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d',",
    "'last_credit_pull_d', 'last_fico_range_high', 'last_fico_range_low',",
    "",
    "# Hardship and debt settlement (post-loan events)",
    "'hardship_flag', 'hardship_type', 'hardship_reason',",
    "'hardship_status', 'hardship_start_date', 'hardship_end_date',",
    "'hardship_loan_status', 'hardship_dpd', 'hardship_length',",
    "'hardship_amount', 'hardship_payoff_balance_amount',",
    "'deferral_term', 'payment_plan_start_date',",
    "'debt_settlement_flag', 'debt_settlement_flag_date',",
    "'settlement_status', 'settlement_date', 'settlement_amount',",
    "'settlement_percentage', 'settlement_term',",
    "",
    "# Other post-loan info",
    "'pymnt_plan', 'initial_list_status', # policy file date is post-approval",
    "'policy_code', # internal policy code",
    "]",
    "",
    "# METADATA features (No predictive value - delete, but keep desc)",
    "metadata_features = [",
    "'id', 'member_id', 'url', # ID fields",
    "'funded_amnt_inv', # duplicate of funded_amnt for investors",
    "# Note: 'desc' Keptfor OCEAN extraction",
    "]",
    "",
    "# OUTCOME feature（Objectivevariable - Special handling）",
    "outcome_features = [",
    "'loan_status', # createObjectivevariable",
    "]",
    "",
    "print(f\"POST-LOAN feature count: {len(post_loan_features)}\")",
    "print(f\"METADATA feature count: {len(metadata_features)}\")",
    "print(f\"OUTCOME feature count: {len(outcome_features)}\")",
    "print(f\"\\nKept 'desc' field for subsequent OCEAN feature extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: analysisfeatureCoverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# calculateallfeature Coverage\nprint(\"calculatefeatureCoverage...\\n\")\n\ncoverage_stats = []\n\nfor col in df_with_desc.columns:\n    non_null = df_with_desc[col].notna().sum()\n    coverage = (non_null / len(df_with_desc)) * 100\n    \n    # Determine feature type\n    if col in post_loan_features:\n        feature_type = 'POST-LOAN'\n        keep_status = ' DELETE (Leakage)'\n    elif col in metadata_features:\n        feature_type = 'METADATA'\n        keep_status = ' DELETE (No value)'\n    elif col in outcome_features:\n        feature_type = 'OUTCOME'\n        keep_status = 'WARNING: SPECIAL (Target)'\n    elif col == 'desc':\n        feature_type = 'TEXT'\n        keep_status = ' KEEP (For OCEAN)'\n    else:\n        feature_type = 'PRE-LOAN'\n        if coverage >= 80:\n            keep_status = ' KEEP (Good quality)'\n        else:\n            keep_status = f' DELETE (Coverage {coverage:.1f}% < 80%)'\n    \n    coverage_stats.append({\n        'Feature': col,\n        'Type': feature_type,\n        'Coverage%': f\"{coverage:.2f}\",\n        'Non_Null': f\"{non_null:,}\",\n        'Decision': keep_status,\n        'dtype': str(df_with_desc[col].dtype)\n    })\n\ncoverage_df = pd.DataFrame(coverage_stats)\ncoverage_df['coverage_numeric'] = coverage_df['Coverage%'].astype(float)\ncoverage_df = coverage_df.sort_values('coverage_numeric', ascending=False)\n\nprint(\"featureCoverageanalysiscompleted！\")\nprint(f\"\\nTotal features: {len(coverage_df)}\")\n\n# feature\nprint(\"\\n=\" * 80)\nprint(\"Feature Type Statistics\")\nprint(\"=\" * 80)\nprint(coverage_df['Type'].value_counts())\n\nprint(\"\\n=\" * 80)\nprint(\"Processing Decision Statistics\")\nprint(\"=\" * 80)\nkeep_count = coverage_df['Decision'].str.contains('KEEP').sum()\ndelete_count = coverage_df['Decision'].str.contains('DELETE').sum()\nspecial_count = coverage_df['Decision'].str.contains('SPECIAL').sum()\n\nprint(f\"Keptfeature: {keep_count}\")\nprint(f\"DELETEfeature: {delete_count}\")\nprint(f\"Special handling: {special_count}\")\n\n# saveCoverage\ncoverage_df.to_csv('../../feature_coverage_report.csv', index=False)\nprint(\"\\nCoverage save: feature_coverage_report.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: viewto beDELETE feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewto beDELETE feature",
    "delete_features = coverage_df[coverage_df['Decision'].str.contains('DELETE')]",
    "",
    "print(\"=\" * 80)",
    "print(f\"to beDELETE feature (Total {len(delete_features)} )\")",
    "print(\"=\" * 80)",
    "print(delete_features[['Feature', 'Type', 'Coverage%', 'Decision']].to_string(index=False))",
    "",
    "# DELETE",
    "print(\"\\n=\" * 80)",
    "print(\"DELETE analysis\")",
    "print(\"=\" * 80)",
    "",
    "leakage_delete = delete_features[delete_features['Type'] == 'POST-LOAN']",
    "metadata_delete = delete_features[delete_features['Type'] == 'METADATA']",
    "quality_delete = delete_features[",
    "(delete_features['Type'] == 'PRE-LOAN') &",
    "(delete_features['Coverage%'].astype(float) < 80)",
    "]",
    "",
    "print(f\" dataLeakageDELETE (POST-LOAN): {len(leakage_delete)} \")",
    "print(f\" DELETE (METADATA): {len(metadata_delete)} \")",
    "print(f\" DELETE (Coverage<80%): {len(quality_delete)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: DELETEfeatureandcreatecleandataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collectallto beDELETE Feature",
    "features_to_delete = delete_features['Feature'].tolist()",
    "",
    "print(f\"startDELETE {len(features_to_delete)} feature...\\n\")",
    "print(f\"DELETE Data shape: {df_with_desc.shape}\")",
    "",
    "# DELETEfeature",
    "df_clean = df_with_desc.drop(columns=features_to_delete, errors='ignore')",
    "",
    "print(f\"DELETE Data shape: {df_clean.shape}\")",
    "print(f\"Keptfeature : {df_clean.shape[1]}\")",
    "print(f\"\\nKept feature :\")",
    "print(f\"- desc (for OCEAN extraction)\")",
    "print(f\"- loan_status ( createObjectivevariable)\")",
    "print(f\"- {df_clean.shape[1] - 2} PRE-LOAN feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: createObjectivevariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view loan_status",
    "print(\"=\" * 80)",
    "print(\"loan_status distribution\")",
    "print(\"=\" * 80)",
    "print(df_clean['loan_status'].value_counts())",
    "print(f\"\\nTotal: {len(df_clean):,} records\")",
    "",
    "# create Objectivevariable",
    "# 1 = Charged Off (Charged Off), 0 = Fully Paid (Fully Paid)",
    "print(\"\\ncreateObjectivevariable...\")",
    "",
    "# Fully Paid",
    "fully_paid_statuses = ['Fully Paid', 'Current', 'In Grace Period']",
    "# Charged Off",
    "charged_off_statuses = ['Charged Off', 'Default', 'Late (31-120 days)', 'Late (16-30 days)']",
    "",
    "# Kept Fully Paid and Charged Off",
    "df_clean_binary = df_clean[",
    "(df_clean['loan_status'] == 'Fully Paid') |",
    "(df_clean['loan_status'] == 'Charged Off')",
    "].copy()",
    "",
    "print(f\"\\nData count after filtering: {len(df_clean_binary):,} \")",
    "print(\"\\nloan_status distribution ( ):\")",
    "print(df_clean_binary['loan_status'].value_counts())",
    "",
    "# createObjectivevariable",
    "df_clean_binary['target'] = (df_clean_binary['loan_status'] == 'Charged Off').astype(int)",
    "",
    "print(\"\\n=\" * 80)",
    "print(\"Objectivevariable \")",
    "print(\"=\" * 80)",
    "print(f\"target = 0 (Fully Paid): {(df_clean_binary['target']==0).sum():,}\")",
    "print(f\"target = 1 (Charged Off): {(df_clean_binary['target']==1).sum():,}\")",
    "print(f\"\\nCharged Off : {df_clean_binary['target'].mean()*100:.2f}%\")",
    "",
    "# at DELETE loan_status",
    "df_clean_binary = df_clean_binary.drop(columns=['loan_status'])",
    "",
    "print(f\"\\nFinal data shape: {df_clean_binary.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: dataTypeanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# analysisKeptfeature dataType\nprint(\"=\" * 80)\nprint(\"Keptfeature dataType \")\nprint(\"=\" * 80)\n\nnumeric_features = df_clean_binary.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_features = df_clean_binary.select_dtypes(include=['object']).columns.tolist()\n\n# Remove target and desc\nif 'target' in numeric_features:\n    numeric_features.remove('target')\nif 'desc' in categorical_features:\n    categorical_features.remove('desc')\n\nprint(f\"\\nNumeric features: {len(numeric_features)} \")\nprint(f\"Categorical features: {len(categorical_features)} \")\nprint(f\"Text features: 1 (desc)\")\nprint(f\"Objectivevariable: 1 (target)\")\n\nprint(\"\\n=\" * 80)\nprint(\"Numeric featurescolumns \")\nprint(\"=\" * 80)\nfor i, feat in enumerate(numeric_features, 1):\n    print(f\"{i:3d}. {feat}\")\n\nprint(\"\\n=\" * 80)\nprint(\"Categorical featurescolumns \")\nprint(\"=\" * 80)\nfor i, feat in enumerate(categorical_features, 1):\n    print(f\"{i:2d}. {feat}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save clean modeling dataset output_file = '../../data/loan_clean_for_modeling.csv' print(f\"Save Clean Dataset : {output_file}\") df_clean_binary.to_csv(output_file, index=False) import os file_size = os.path.getsize(output_file) / (1024 * 1024) # MB print(f\"\\nFile size: {file_size:.2f} MB\") print(f\"Data shape: {df_clean_binary.shape[0]:,} rows × {df_clean_binary.shape[1]} columns\") # savefeaturecolumns feature_lists = { 'numeric_features': numeric_features, 'categorical_features': categorical_features, 'text_feature': ['desc'], 'target': ['target'] } import json with open('../../feature_lists_clean.json', 'w') as f: json.dump(feature_lists, f, indent=2) print(\"\\nfeaturecolumns save: feature_lists_clean.json\") print(\"\\n Data cleaning completed！\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Data Quality Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"Data Cleaning Summary Report\")\nprint(\"=\" * 80)\n\nprint(\"\\n1. Dataset Size Changes\")\nprint(\"-\" * 80)\nprint(f\"Raw data : 2,260,668 rows × 145 columns\")\nprint(f\"Data with desc: {len(df_with_desc):,} rows × {len(df_with_desc.columns)} columns\")\nprint(f\"Final modeling data: {df_clean_binary.shape[0]:,} rows × {df_clean_binary.shape[1]} columns\")\nprint(f\"dataKept : {len(df_clean_binary)/2260668*100:.2f}%\")\n\nprint(\"\\n2. featureDELETE \")\nprint(\"-\" * 80)\nprint(f\"Raw feature count: 145\")\nprint(f\"DELETE feature : {len(features_to_delete)}\")\nprint(f\" - POST-LOAN (Prevent leakage): {len(leakage_delete)}\")\nprint(f\" - METADATA (No value): {len(metadata_delete)}\")\nprint(f\" - Low quality (Coverage<80%): {len(quality_delete)}\")\nprint(f\"Kept feature : {df_clean_binary.shape[1]}\")\nprint(f\" - Numeric: {len(numeric_features)}\")\nprint(f\" - Type: {len(categorical_features)}\")\nprint(f\" - Text: 1 (desc)\")\nprint(f\" - Objectivevariable: 1 (target)\")\n\nprint(\"\\n3. Objectivevariable \")\nprint(\"-\" * 80)\nprint(f\"Fully Paid (target=0): {(df_clean_binary['target']==0).sum():,} ({(df_clean_binary['target']==0).sum()/len(df_clean_binary)*100:.2f}%)\")\nprint(f\"Charged Off (target=1): {(df_clean_binary['target']==1).sum():,} ({(df_clean_binary['target']==1).sum()/len(df_clean_binary)*100:.2f}%)\")\nprint(f\"Charged Off : {df_clean_binary['target'].mean()*100:.2f}%\")\n\nprint(\"\\n4. Data Quality Check\")\nprint(\"-\" * 80)\n\n# check featuremissing\nmissing_report = []\nfor col in df_clean_binary.columns:\n    if col not in ['target', 'desc']:\n        missing_pct = (df_clean_binary[col].isna().sum() / len(df_clean_binary)) * 100\n        if missing_pct > 0:\n            missing_report.append({\n                'feature': col,\n                'Missing_%': f\"{missing_pct:.2f}\"\n            })\n\nif missing_report:\n    missing_df = pd.DataFrame(missing_report)\n    missing_df['missing_numeric'] = missing_df['Missing_%'].astype(float)\n    missing_df = missing_df.sort_values('missing_numeric', ascending=False)\n    missing_df = missing_df.drop('missing_numeric', axis=1)\n    print(f\"Features with missing values: {len(missing_report)} \")\n    print(\"\\n 10feature:\")\n    print(missing_df.head(10).to_string(index=False))\nelse:\n    print(\"All features have no missing values \")\n\nprint(\"\\n5. rows \")\nprint(\"-\" * 80)\nprint(\" Data cleaning completed， rows :\")\nprint(\"\")\nprint(\"1. 04_xgboost_baseline.ipynb\")\nprint(\" - Train XGBoost baseline model with clean data\")\nprint(\" - Without OCEAN features\")\nprint(\" - Establish performance baseline\")\nprint(\"\")\nprint(\"2. 05_ocean_feature_extraction.ipynb\")\nprint(\" - Extract OCEAN personality features from desc field\")\nprint(\" - Note train/test leakage\")\nprint(\"\")\nprint(\"3. 06_xgboost_with_ocean.ipynb\")\nprint(\" - Train complete model with OCEAN features\")\nprint(\" - Compare with baseline model performance\")\nprint(\"\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Data Overview Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create visualization fig, axes = plt.subplots(2, 2, figsize=(15, 12)) # 1. Objectivevariable ax1 = axes[0, 0] target_counts = df_clean_binary['target'].value_counts() colors = ['#2ecc71', '#e74c3c'] ax1.bar(['Fully Paid\\n(target=0)', 'Charged Off\\n(target=1)'], target_counts.values, color=colors, alpha=0.7, edgecolor='black') ax1.set_ylabel('Count', fontsize=12, fontweight='bold') ax1.set_title('Target Variable Distribution', fontsize=14, fontweight='bold') ax1.grid(axis='y', alpha=0.3) for i, v in enumerate(target_counts.values): ax1.text(i, v, f'{v:,}\\n({v/len(df_clean_binary)*100:.1f}%)', ha='center', va='bottom', fontweight='bold') # 2. featureType ax2 = axes[0, 1] feature_type_counts = [len(numeric_features), len(categorical_features), 1] feature_types = ['Numeric', 'Categorical', 'Text'] colors2 = ['#3498db', '#9b59b6', '#f39c12'] wedges, texts, autotexts = ax2.pie(feature_type_counts, labels=feature_types, autopct='%1.1f%%', colors=colors2, startangle=90, textprops={'fontweight': 'bold'}) ax2.set_title('Feature Type Distribution', fontsize=14, fontweight='bold') # 3. featureDELETE ax3 = axes[1, 0] delete_reasons = ['POST-LOAN\\n(Leakage)', 'METADATA\\n(No value)', 'Low Quality\\n(Coverage<80%)', 'Kept'] delete_counts = [len(leakage_delete), len(metadata_delete), len(quality_delete), df_clean_binary.shape[1]] colors3 = ['#e74c3c', '#95a5a6', '#e67e22', '#2ecc71'] bars = ax3.barh(delete_reasons, delete_counts, color=colors3, alpha=0.7, edgecolor='black') ax3.set_xlabel('Count', fontsize=12, fontweight='bold') ax3.set_title('Feature Retention Analysis', fontsize=14, fontweight='bold') ax3.grid(axis='x', alpha=0.3) for i, (bar, count) in enumerate(zip(bars, delete_counts)): ax3.text(count, bar.get_y() + bar.get_height()/2, f' {count}', va='center', fontweight='bold') # 4. Dataset Size Changes ax4 = axes[1, 1] data_stages = ['Original\\nDataset', 'With\\ndesc', 'Clean\\nBinary'] data_counts = [2260668, len(df_with_desc), len(df_clean_binary)] colors4 = ['#34495e', '#3498db', '#2ecc71'] bars = ax4.bar(data_stages, data_counts, color=colors4, alpha=0.7, edgecolor='black') ax4.set_ylabel('Number of Rows', fontsize=12, fontweight='bold') ax4.set_title('Dataset Size Changes', fontsize=14, fontweight='bold') ax4.grid(axis='y', alpha=0.3) for bar, count in zip(bars, data_counts): height = bar.get_height() ax4.text(bar.get_x() + bar.get_width()/2., height, f'{count:,}', ha='center', va='bottom', fontweight='bold') plt.tight_layout() plt.savefig('../../data_cleaning_summary.png', dpi=300, bbox_inches='tight') print(\"\\nVisualization saved: data_cleaning_summary.png\") plt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}