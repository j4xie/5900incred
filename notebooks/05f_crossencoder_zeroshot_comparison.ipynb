{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05f - Cross-Encoder Zero-Shot Comparison\n",
    "\n",
    "**Purpose**: Evaluate pre-trained Cross-Encoder models for OCEAN trait prediction without any fine-tuning\n",
    "\n",
    "**Why Cross-Encoder over Bi-Encoder (BGE)?**\n",
    "- **Bi-Encoder**: Encodes query and document independently → learn mapping via regression\n",
    "- **Cross-Encoder**: Jointly processes query+document → direct similarity score\n",
    "- **Advantage**: Better semantic understanding of (OCEAN definition, loan description) pairs\n",
    "\n",
    "**Models to Test**:\n",
    "1. `cross-encoder/stsb-roberta-large` - Trained on STS-B for semantic similarity\n",
    "2. `cross-encoder/stsb-roberta-base` - Base version (faster)\n",
    "3. `cross-encoder/ms-marco-MiniLM-L-6-v2` - Trained on MS MARCO passage ranking\n",
    "4. `cross-encoder/quora-distilroberta-base` - Trained on Quora question pairs\n",
    "\n",
    "**Evaluation Strategy**:\n",
    "- Zero-shot: No training required\n",
    "- Input: (OCEAN definition, loan description) pairs\n",
    "- Output: Similarity score (normalized to 0-1 for OCEAN prediction)\n",
    "- Metrics: R², RMSE, MAE vs ground truth\n",
    "\n",
    "**Expected Performance**:\n",
    "- Baseline: BGE + Elastic Net (R² 0.19-0.24)\n",
    "- Target: R² 0.20-0.35 (comparable or better than BGE)\n",
    "- If R² < 0.25: Consider LoRA fine-tuning\n",
    "\n",
    "**Estimated Time**: 10-20 minutes for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n",
      "Timestamp: 2025-10-29 14:49:11.551204\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Use sentence-transformers for Cross-Encoder models\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  LLM models: 5\n",
      "  Cross-Encoder models: 4\n",
      "  OCEAN dimensions: 5\n",
      "  Total evaluations: 100\n",
      "\n",
      "Cross-Encoder Models:\n",
      "  stsb-roberta-large: RoBERTa-Large trained on STS-B (semantic similarity) (355M)\n",
      "  stsb-roberta-base: RoBERTa-Base trained on STS-B (faster) (125M)\n",
      "  ms-marco-minilm: MiniLM trained on MS MARCO passage ranking (22M)\n",
      "  quora-distilroberta: DistilRoBERTa trained on Quora question pairs (82M)\n"
     ]
    }
   ],
   "source": [
    "# OCEAN dimension definitions\n",
    "OCEAN_DEFINITIONS = {\n",
    "    'openness': \"This person is imaginative, creative, curious about new experiences, and open to new ideas. They appreciate art, emotion, adventure, unusual ideas, and variety of experience.\",\n",
    "    \n",
    "    'conscientiousness': \"This person is organized, responsible, hardworking, reliable, and goal-oriented. They show self-discipline, act dutifully, and aim for achievement against measures or outside expectations.\",\n",
    "    \n",
    "    'extraversion': \"This person is outgoing, energetic, talkative, sociable, and enjoys being around others. They seek stimulation in the company of others and are assertive and enthusiastic.\",\n",
    "    \n",
    "    'agreeableness': \"This person is friendly, cooperative, compassionate, trusting, and considerate of others. They are generally well-tempered, kind, and value getting along with others.\",\n",
    "    \n",
    "    'neuroticism': \"This person tends to experience negative emotions such as anxiety, anger, or depression. They are emotionally unstable, prone to worry, and have difficulty coping with stress.\"\n",
    "}\n",
    "\n",
    "# LLM configurations\n",
    "LLM_CONFIGS = {\n",
    "    'llama': {\n",
    "        'name': 'Llama-3.1-8B',\n",
    "        'ocean_file': '../ocean_ground_truth/llama_3.1_8b_ocean_500.csv'\n",
    "    },\n",
    "    'gpt': {\n",
    "        'name': 'GPT-OSS-120B',\n",
    "        'ocean_file': '../ocean_ground_truth/gpt_oss_120b_ocean_500.csv'\n",
    "    },\n",
    "    'gemma': {\n",
    "        'name': 'Gemma-2-9B',\n",
    "        'ocean_file': '../ocean_ground_truth/gemma_2_9b_ocean_500.csv'\n",
    "    },\n",
    "    'deepseek': {\n",
    "        'name': 'DeepSeek-V3.1',\n",
    "        'ocean_file': '../ocean_ground_truth/deepseek_v3.1_ocean_500.csv'\n",
    "    },\n",
    "    'qwen': {\n",
    "        'name': 'Qwen-2.5-72B',\n",
    "        'ocean_file': '../ocean_ground_truth/qwen_2.5_72b_ocean_500.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cross-Encoder models to evaluate\n",
    "CROSSENCODER_MODELS = {\n",
    "    'stsb-roberta-large': {\n",
    "        'model_name': 'cross-encoder/stsb-roberta-large',\n",
    "        'description': 'RoBERTa-Large trained on STS-B (semantic similarity)',\n",
    "        'params': '355M',\n",
    "        'expected_range': [0, 5]  # STS-B score range\n",
    "    },\n",
    "    'stsb-roberta-base': {\n",
    "        'model_name': 'cross-encoder/stsb-roberta-base',\n",
    "        'description': 'RoBERTa-Base trained on STS-B (faster)',\n",
    "        'params': '125M',\n",
    "        'expected_range': [0, 5]\n",
    "    },\n",
    "    'ms-marco-minilm': {\n",
    "        'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        'description': 'MiniLM trained on MS MARCO passage ranking',\n",
    "        'params': '22M',\n",
    "        'expected_range': [-10, 10]  # Relevance score\n",
    "    },\n",
    "    'quora-distilroberta': {\n",
    "        'model_name': 'cross-encoder/quora-distilroberta-base',\n",
    "        'description': 'DistilRoBERTa trained on Quora question pairs',\n",
    "        'params': '82M',\n",
    "        'expected_range': [0, 1]  # Binary similarity\n",
    "    }\n",
    "}\n",
    "\n",
    "OCEAN_DIMS = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  LLM models: {len(LLM_CONFIGS)}\")\n",
    "print(f\"  Cross-Encoder models: {len(CROSSENCODER_MODELS)}\")\n",
    "print(f\"  OCEAN dimensions: {len(OCEAN_DIMS)}\")\n",
    "print(f\"  Total evaluations: {len(LLM_CONFIGS) * len(CROSSENCODER_MODELS) * len(OCEAN_DIMS)}\")\n",
    "\n",
    "print(\"\\nCross-Encoder Models:\")\n",
    "for key, config in CROSSENCODER_MODELS.items():\n",
    "    print(f\"  {key}: {config['description']} ({config['params']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation function defined (using HF Inference API)\n",
      "  - Simplified API calls using client.sentence_similarity()\n",
      "  - Batch processing with rate limiting\n",
      "  - Automatic retry logic for failed requests\n"
     ]
    }
   ],
   "source": [
    "def call_crossencoder_api(client, model_name, sentence1, sentence2, max_retries=3):\n",
    "    \"\"\"\n",
    "    Call HF Inference API for cross-encoder similarity scoring.\n",
    "    \n",
    "    Args:\n",
    "        client: HuggingFace InferenceClient\n",
    "        model_name: Model name on HF Hub\n",
    "        sentence1: First sentence (OCEAN definition)\n",
    "        sentence2: Second sentence (loan description)\n",
    "        max_retries: Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        float: Similarity score\n",
    "    \"\"\"\n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            # Use sentence-similarity task for cross-encoder models\n",
    "            response = client.sentence_similarity(\n",
    "                sentence=sentence1,\n",
    "                other_sentences=[sentence2],\n",
    "                model=model_name\n",
    "            )\n",
    "            \n",
    "            # Extract score from response\n",
    "            if isinstance(response, list):\n",
    "                return float(response[0])\n",
    "            elif isinstance(response, (int, float)):\n",
    "                return float(response)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected response format: {type(response)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if retry < max_retries - 1:\n",
    "                print(f\"        API error (retry {retry+1}/{max_retries}): {str(e)[:100]}\")\n",
    "                time.sleep(API_CONFIG['retry_delay'])\n",
    "            else:\n",
    "                raise Exception(f\"API failed after {max_retries} retries: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_crossencoder_model(model_key, model_config, llm_key, llm_data, loan_descs, ocean_dims):\n",
    "    \"\"\"\n",
    "    Evaluate a Cross-Encoder model on OCEAN prediction for one LLM using HF Inference API.\n",
    "    \n",
    "    Args:\n",
    "        model_key: Cross-Encoder model identifier\n",
    "        model_config: Model configuration dict\n",
    "        llm_key: LLM identifier\n",
    "        llm_data: Ground truth data for this LLM\n",
    "        loan_descs: Loan descriptions\n",
    "        ocean_dims: OCEAN dimensions to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation results with metrics for each dimension\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Using model: {model_config['model_name']} via HF Inference API...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_name = model_config['model_name']\n",
    "    n_samples = len(llm_data['data'])\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dim in ocean_dims:\n",
    "        print(f\"\\n    [{dim}] Generating predictions via API...\")\n",
    "        \n",
    "        # Get OCEAN definition and loan samples\n",
    "        ocean_def = OCEAN_DEFINITIONS[dim]\n",
    "        loan_samples = loan_descs[:n_samples]\n",
    "        \n",
    "        # Collect predictions with batching\n",
    "        raw_scores = []\n",
    "        pred_start = time.time()\n",
    "        \n",
    "        batch_size = API_CONFIG['batch_size']\n",
    "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "        \n",
    "        with tqdm(total=n_samples, desc=f\"      API calls\", leave=False) as pbar:\n",
    "            for batch_idx in range(n_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min((batch_idx + 1) * batch_size, n_samples)\n",
    "                batch_descs = loan_samples[start_idx:end_idx]\n",
    "                \n",
    "                # Process each description in the batch\n",
    "                for desc in batch_descs:\n",
    "                    score = call_crossencoder_api(client, model_name, ocean_def, desc)\n",
    "                    raw_scores.append(score)\n",
    "                    pbar.update(1)\n",
    "                \n",
    "                # Delay between batches to respect rate limits\n",
    "                if batch_idx < n_batches - 1:\n",
    "                    time.sleep(API_CONFIG['delay_between_batches'])\n",
    "        \n",
    "        pred_time = time.time() - pred_start\n",
    "        raw_scores = np.array(raw_scores)\n",
    "        \n",
    "        # Normalize scores to [0, 1] range\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        predictions = scaler.fit_transform(raw_scores.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Get ground truth\n",
    "        y_true = llm_data['data'][dim].values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_true, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, predictions))\n",
    "        mae = mean_absolute_error(y_true, predictions)\n",
    "        \n",
    "        results[dim] = {\n",
    "            'r2': float(r2),\n",
    "            'rmse': float(rmse),\n",
    "            'mae': float(mae),\n",
    "            'raw_score_mean': float(np.mean(raw_scores)),\n",
    "            'raw_score_std': float(np.std(raw_scores)),\n",
    "            'raw_score_min': float(np.min(raw_scores)),\n",
    "            'raw_score_max': float(np.max(raw_scores)),\n",
    "            'pred_mean': float(np.mean(predictions)),\n",
    "            'pred_std': float(np.std(predictions)),\n",
    "            'true_mean': float(np.mean(y_true)),\n",
    "            'true_std': float(np.std(y_true)),\n",
    "            'inference_time_sec': float(pred_time),\n",
    "            'inference_time_per_sample_ms': float(pred_time * 1000 / n_samples),\n",
    "            'api_calls': int(n_samples)\n",
    "        }\n",
    "        \n",
    "        print(f\"      R²={r2:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
    "        print(f\"      Raw scores: [{raw_scores.min():.2f}, {raw_scores.max():.2f}], μ={raw_scores.mean():.2f}\")\n",
    "        print(f\"      API time: {pred_time:.1f}s ({pred_time*1000/n_samples:.1f}ms/sample, {n_samples} calls)\")\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    r2_scores = [results[dim]['r2'] for dim in ocean_dims]\n",
    "    rmse_scores = [results[dim]['rmse'] for dim in ocean_dims]\n",
    "    mae_scores = [results[dim]['mae'] for dim in ocean_dims]\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    summary = {\n",
    "        'avg_r2': float(np.mean(r2_scores)),\n",
    "        'avg_rmse': float(np.mean(rmse_scores)),\n",
    "        'avg_mae': float(np.mean(mae_scores)),\n",
    "        'min_r2': float(np.min(r2_scores)),\n",
    "        'max_r2': float(np.max(r2_scores)),\n",
    "        'std_r2': float(np.std(r2_scores)),\n",
    "        'total_api_calls': int(n_samples * len(ocean_dims)),\n",
    "        'total_time_sec': float(total_time)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'model_key': model_key,\n",
    "        'model_name': model_config['model_name'],\n",
    "        'llm_key': llm_key,\n",
    "        'llm_name': llm_data['name'],\n",
    "        'n_samples': llm_data['n_samples'],\n",
    "        'results': results,\n",
    "        'summary': summary\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation function defined (using HF Inference API)\")\n",
    "print(\"  - Simplified API calls using client.sentence_similarity()\")\n",
    "print(\"  - Batch processing with rate limiting\")\n",
    "print(\"  - Automatic retry logic for failed requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation function defined\n",
      "  - Uses sentence-transformers CrossEncoder\n",
      "  - Batch processing for efficiency\n",
      "  - Automatic device selection (CPU/GPU)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_crossencoder_model(model_key, model_config, llm_key, llm_data, loan_descs, ocean_dims):\n",
    "    \"\"\"\n",
    "    Evaluate a Cross-Encoder model on OCEAN prediction for one LLM.\n",
    "    \n",
    "    Args:\n",
    "        model_key: Cross-Encoder model identifier\n",
    "        model_config: Model configuration dict\n",
    "        llm_key: LLM identifier\n",
    "        llm_data: Ground truth data for this LLM\n",
    "        loan_descs: Loan descriptions\n",
    "        ocean_dims: OCEAN dimensions to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation results with metrics for each dimension\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Loading model: {model_config['model_name']}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load Cross-Encoder model\n",
    "    model = CrossEncoder(model_config['model_name'], device=device)\n",
    "    model_name = model_config['model_name']\n",
    "    n_samples = len(llm_data['data'])\n",
    "    \n",
    "    print(f\"  Model loaded successfully (device: {device})\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dim in ocean_dims:\n",
    "        print(f\"\\n    [{dim}] Generating predictions...\")\n",
    "        \n",
    "        # Get OCEAN definition and loan samples\n",
    "        ocean_def = OCEAN_DEFINITIONS[dim]\n",
    "        loan_samples = loan_descs[:n_samples]\n",
    "        \n",
    "        # Create sentence pairs for cross-encoder\n",
    "        sentence_pairs = [[ocean_def, desc] for desc in loan_samples]\n",
    "        \n",
    "        # Get predictions with batch processing\n",
    "        pred_start = time.time()\n",
    "        batch_size = 32  # Process 32 pairs at a time\n",
    "        \n",
    "        raw_scores = []\n",
    "        for i in tqdm(range(0, len(sentence_pairs), batch_size), desc=f\"      Batches\", leave=False):\n",
    "            batch = sentence_pairs[i:i+batch_size]\n",
    "            scores = model.predict(batch, show_progress_bar=False)\n",
    "            raw_scores.extend(scores)\n",
    "        \n",
    "        pred_time = time.time() - pred_start\n",
    "        raw_scores = np.array(raw_scores)\n",
    "        \n",
    "        # Normalize scores to [0, 1] range\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        predictions = scaler.fit_transform(raw_scores.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Get ground truth\n",
    "        y_true = llm_data['data'][dim].values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_true, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, predictions))\n",
    "        mae = mean_absolute_error(y_true, predictions)\n",
    "        \n",
    "        results[dim] = {\n",
    "            'r2': float(r2),\n",
    "            'rmse': float(rmse),\n",
    "            'mae': float(mae),\n",
    "            'raw_score_mean': float(np.mean(raw_scores)),\n",
    "            'raw_score_std': float(np.std(raw_scores)),\n",
    "            'raw_score_min': float(np.min(raw_scores)),\n",
    "            'raw_score_max': float(np.max(raw_scores)),\n",
    "            'pred_mean': float(np.mean(predictions)),\n",
    "            'pred_std': float(np.std(predictions)),\n",
    "            'true_mean': float(np.mean(y_true)),\n",
    "            'true_std': float(np.std(y_true)),\n",
    "            'inference_time_sec': float(pred_time),\n",
    "            'inference_time_per_sample_ms': float(pred_time * 1000 / n_samples)\n",
    "        }\n",
    "        \n",
    "        print(f\"      R²={r2:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
    "        print(f\"      Raw scores: [{raw_scores.min():.2f}, {raw_scores.max():.2f}], μ={raw_scores.mean():.2f}\")\n",
    "        print(f\"      Inference time: {pred_time:.1f}s ({pred_time*1000/n_samples:.1f}ms/sample)\")\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    r2_scores = [results[dim]['r2'] for dim in ocean_dims]\n",
    "    rmse_scores = [results[dim]['rmse'] for dim in ocean_dims]\n",
    "    mae_scores = [results[dim]['mae'] for dim in ocean_dims]\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    summary = {\n",
    "        'avg_r2': float(np.mean(r2_scores)),\n",
    "        'avg_rmse': float(np.mean(rmse_scores)),\n",
    "        'avg_mae': float(np.mean(mae_scores)),\n",
    "        'min_r2': float(np.min(r2_scores)),\n",
    "        'max_r2': float(np.max(r2_scores)),\n",
    "        'std_r2': float(np.std(r2_scores)),\n",
    "        'total_time_sec': float(total_time)\n",
    "    }\n",
    "    \n",
    "    # Clean up model to free memory\n",
    "    del model\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'model_key': model_key,\n",
    "        'model_name': model_config['model_name'],\n",
    "        'llm_key': llm_key,\n",
    "        'llm_name': llm_data['name'],\n",
    "        'n_samples': llm_data['n_samples'],\n",
    "        'results': results,\n",
    "        'summary': summary\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation function defined\")\n",
    "print(\"  - Uses sentence-transformers CrossEncoder\")\n",
    "print(\"  - Batch processing for efficiency\")\n",
    "print(\"  - Automatic device selection (CPU/GPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading loan descriptions...\n",
      "✓ Loaded 34529 loan descriptions\n",
      "\n",
      "Loading OCEAN ground truth data...\n",
      "  ✓ Llama-3.1-8B: 500 samples\n",
      "  ✓ GPT-OSS-120B: 500 samples\n",
      "  ✓ Gemma-2-9B: 500 samples\n",
      "  ✓ DeepSeek-V3.1: 500 samples\n",
      "  ✓ Qwen-2.5-72B: 500 samples\n",
      "\n",
      "✓ Data loading complete\n",
      "  Loan descriptions: 34529\n",
      "  LLM datasets: 5\n",
      "  Ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Load loan descriptions\n",
    "print(\"Loading loan descriptions...\")\n",
    "loan_df = pd.read_csv('../loan_final_desc50plus_with_ocean_bge.csv')\n",
    "loan_descriptions = loan_df['desc'].tolist()\n",
    "print(f\"✓ Loaded {len(loan_descriptions)} loan descriptions\")\n",
    "\n",
    "# Load OCEAN ground truth for each LLM\n",
    "llm_ocean_data = {}\n",
    "\n",
    "print(\"\\nLoading OCEAN ground truth data...\")\n",
    "for llm_key, config in LLM_CONFIGS.items():\n",
    "    ocean_file = config['ocean_file']\n",
    "    \n",
    "    if not os.path.exists(ocean_file):\n",
    "        print(f\"  ⚠️ Warning: {ocean_file} not found, skipping {llm_key}\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(ocean_file)\n",
    "    \n",
    "    # Verify OCEAN columns exist\n",
    "    missing_cols = [col for col in OCEAN_DIMS if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"  ⚠️ Warning: {llm_key} missing columns {missing_cols}, skipping\")\n",
    "        continue\n",
    "    \n",
    "    llm_ocean_data[llm_key] = {\n",
    "        'name': config['name'],\n",
    "        'data': df,\n",
    "        'n_samples': len(df)\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ {config['name']}: {len(df)} samples\")\n",
    "\n",
    "print(f\"\\n✓ Data loading complete\")\n",
    "print(f\"  Loan descriptions: {len(loan_descriptions)}\")\n",
    "print(f\"  LLM datasets: {len(llm_ocean_data)}\")\n",
    "print(f\"  Ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting Cross-Encoder Zero-Shot Evaluation\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Model: stsb-roberta-large (RoBERTa-Large trained on STS-B (semantic similarity))\n",
      "================================================================================\n",
      "\n",
      "[1/20] Evaluating Llama-3.1-8B...\n",
      "\n",
      "  Loading model: cross-encoder/stsb-roberta-large...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a95cf3bdaa645a38f77762780cf07b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cf4488264143b3b1f570a7e98a3434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467c5438588043a7a3aa9da7cad06ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdb3863d796403f83205dee4dd3c2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4793c65eea4097bac66a4abc2adb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0601a7c79ec24d23bb66a31a7ee82b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6253150b352a4ee1b082e1583f5158b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model loaded successfully (device: cpu)\n",
      "\n",
      "    [openness] Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ❌ Error evaluating stsb-roberta-large + llama: Input contains NaN.\n",
      "\n",
      "[2/20] Evaluating GPT-OSS-120B...\n",
      "\n",
      "  Loading model: cross-encoder/stsb-roberta-large...\n",
      "  Model loaded successfully (device: cpu)\n",
      "\n",
      "    [openness] Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      R²=-5.7267, RMSE=0.3230, MAE=0.2660\n",
      "      Raw scores: [0.01, 0.69], μ=0.14\n",
      "      Inference time: 147.9s (295.8ms/sample)\n",
      "\n",
      "    [conscientiousness] Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      R²=-28.8090, RMSE=0.5858, MAE=0.5343\n",
      "      Raw scores: [0.01, 0.72], μ=0.17\n",
      "      Inference time: 144.9s (289.7ms/sample)\n",
      "\n",
      "    [extraversion] Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      R²=-9.5760, RMSE=0.2906, MAE=0.2206\n",
      "      Raw scores: [0.01, 0.69], μ=0.13\n",
      "      Inference time: 147.7s (295.4ms/sample)\n",
      "\n",
      "    [agreeableness] Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_evals\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Evaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_crossencoder_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloan_descs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloan_descriptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mocean_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOCEAN_DIMS\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Print summary\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 45\u001b[0m, in \u001b[0;36mevaluate_crossencoder_model\u001b[0;34m(model_key, model_config, llm_key, llm_data, loan_descs, ocean_dims)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentence_pairs), batch_size), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m      Batches\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     batch \u001b[38;5;241m=\u001b[39m sentence_pairs[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 45\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     raw_scores\u001b[38;5;241m.\u001b[39mextend(scores)\n\u001b[1;32m     48\u001b[0m pred_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m pred_start\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/util.py:68\u001b[0m, in \u001b[0;36mcross_encoder_predict_rank_args_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mpop(deprecated_arg)\n\u001b[1;32m     64\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe CrossEncoder.predict `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` argument is deprecated and has no effect. It will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         )\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:465\u001b[0m, in \u001b[0;36mCrossEncoder.predict\u001b[0;34m(self, sentences, batch_size, show_progress_bar, activation_fn, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[0m\n\u001b[1;32m    458\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    459\u001b[0m     batch,\n\u001b[1;32m    460\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    461\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    462\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    463\u001b[0m )\n\u001b[1;32m    464\u001b[0m features\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 465\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(model_predictions\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apply_softmax \u001b[38;5;129;01mand\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1188\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1188\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1199\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1200\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:862\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    860\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 862\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    876\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:606\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    602\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    604\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:543\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    540\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    541\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pytorch_utils.py:257\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:551\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:465\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 465\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run evaluation for all combinations\n",
    "all_results = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting Cross-Encoder Zero-Shot Evaluation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_evals = len(CROSSENCODER_MODELS) * len(llm_ocean_data)\n",
    "eval_count = 0\n",
    "\n",
    "for model_key, model_config in CROSSENCODER_MODELS.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Model: {model_key} ({model_config['description']})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for llm_key, llm_data in llm_ocean_data.items():\n",
    "        eval_count += 1\n",
    "        print(f\"\\n[{eval_count}/{total_evals}] Evaluating {llm_data['name']}...\")\n",
    "        \n",
    "        try:\n",
    "            result = evaluate_crossencoder_model(\n",
    "                model_key=model_key,\n",
    "                model_config=model_config,\n",
    "                llm_key=llm_key,\n",
    "                llm_data=llm_data,\n",
    "                loan_descs=loan_descriptions,\n",
    "                ocean_dims=OCEAN_DIMS\n",
    "            )\n",
    "            \n",
    "            all_results.append(result)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\n  Summary for {model_key} + {llm_key}:\")\n",
    "            print(f\"    Avg R²: {result['summary']['avg_r2']:.4f}\")\n",
    "            print(f\"    Avg RMSE: {result['summary']['avg_rmse']:.4f}\")\n",
    "            print(f\"    Total time: {result['summary']['total_time_sec']:.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ❌ Error evaluating {model_key} + {llm_key}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Evaluation Complete\")\n",
    "print(f\"  Total evaluations: {len(all_results)}/{total_evals}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Evaluation\n",
    "\n",
    "**Note**: This will download and run models locally. Estimated time: 10-20 minutes for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for all combinations\n",
    "all_results = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting Cross-Encoder Zero-Shot Evaluation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_evals = len(CROSSENCODER_MODELS) * len(llm_ocean_data)\n",
    "eval_count = 0\n",
    "\n",
    "for model_key, model_config in CROSSENCODER_MODELS.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Model: {model_key} ({model_config['description']})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for llm_key, llm_data in llm_ocean_data.items():\n",
    "        eval_count += 1\n",
    "        print(f\"\\n[{eval_count}/{total_evals}] Evaluating {llm_data['name']}...\")\n",
    "        \n",
    "        try:\n",
    "            result = evaluate_crossencoder_model(\n",
    "                model_key=model_key,\n",
    "                model_config=model_config,\n",
    "                llm_key=llm_key,\n",
    "                llm_data=llm_data,\n",
    "                loan_descs=loan_descriptions,\n",
    "                ocean_dims=OCEAN_DIMS\n",
    "            )\n",
    "            \n",
    "            all_results.append(result)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\n  Summary for {model_key} + {llm_key}:\")\n",
    "            print(f\"    Avg R²: {result['summary']['avg_r2']:.4f}\")\n",
    "            print(f\"    Avg RMSE: {result['summary']['avg_rmse']:.4f}\")\n",
    "            print(f\"    Total time: {result['summary']['total_time_sec']:.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ❌ Error evaluating {model_key} + {llm_key}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Evaluation Complete\")\n",
    "print(f\"  Total evaluations: {len(all_results)}/{total_evals}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to JSON\n",
    "output_file = '../05f_crossencoder_zeroshot_results.json'\n",
    "\n",
    "output_data = {\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'n_models': len(CROSSENCODER_MODELS),\n",
    "        'n_llms': len(llm_ocean_data),\n",
    "        'n_dimensions': len(OCEAN_DIMS),\n",
    "        'total_evaluations': len(all_results)\n",
    "    },\n",
    "    'models': CROSSENCODER_MODELS,\n",
    "    'llms': {k: {'name': v['name'], 'n_samples': v['n_samples']} for k, v in llm_ocean_data.items()},\n",
    "    'results': all_results\n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_file}\")\n",
    "print(f\"  File size: {os.path.getsize(output_file) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "\n",
    "for result in all_results:\n",
    "    # Check if total_api_calls exists (from API version) or not (from local version)\n",
    "    total_api_calls = result['summary'].get('total_api_calls', 'N/A')\n",
    "    \n",
    "    summary_data.append({\n",
    "        'model': result['model_key'],\n",
    "        'llm': result['llm_key'],\n",
    "        'avg_r2': result['summary']['avg_r2'],\n",
    "        'avg_rmse': result['summary']['avg_rmse'],\n",
    "        'avg_mae': result['summary']['avg_mae'],\n",
    "        'min_r2': result['summary']['min_r2'],\n",
    "        'max_r2': result['summary']['max_r2'],\n",
    "        'std_r2': result['summary']['std_r2'],\n",
    "        'total_time_sec': result['summary']['total_time_sec']\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_csv = '../05f_crossencoder_zeroshot_comparison.csv'\n",
    "df_summary.to_csv(summary_csv, index=False)\n",
    "print(f\"✓ Summary saved to: {summary_csv}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# Find best models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODELS BY R²\")\n",
    "print(\"=\"*80)\n",
    "best_results = df_summary.nlargest(5, 'avg_r2')\n",
    "for idx, row in best_results.iterrows():\n",
    "    print(f\"{row['model']} + {row['llm']}: R²={row['avg_r2']:.4f}, RMSE={row['avg_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. R² by Model\n",
    "ax = axes[0, 0]\n",
    "model_r2 = df_summary.groupby('model')['avg_r2'].mean().sort_values(ascending=False)\n",
    "model_r2.plot(kind='bar', ax=ax, color='steelblue')\n",
    "ax.set_title('Average R² by Cross-Encoder Model', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Average R²')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# 2. R² by LLM\n",
    "ax = axes[0, 1]\n",
    "llm_r2 = df_summary.groupby('llm')['avg_r2'].mean().sort_values(ascending=False)\n",
    "llm_r2.plot(kind='bar', ax=ax, color='coral')\n",
    "ax.set_title('Average R² by LLM Ground Truth', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('LLM')\n",
    "ax.set_ylabel('Average R²')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# 3. Heatmap of R² (Model x LLM)\n",
    "ax = axes[1, 0]\n",
    "pivot_r2 = df_summary.pivot(index='model', columns='llm', values='avg_r2')\n",
    "sns.heatmap(pivot_r2, annot=True, fmt='.3f', cmap='RdYlGn', center=0, ax=ax, cbar_kws={'label': 'R²'})\n",
    "ax.set_title('R² Heatmap: Model × LLM', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('LLM Ground Truth')\n",
    "ax.set_ylabel('Cross-Encoder Model')\n",
    "\n",
    "# 4. RMSE vs R²\n",
    "ax = axes[1, 1]\n",
    "for model in df_summary['model'].unique():\n",
    "    model_data = df_summary[df_summary['model'] == model]\n",
    "    ax.scatter(model_data['avg_r2'], model_data['avg_rmse'], label=model, s=100, alpha=0.7)\n",
    "ax.set_title('RMSE vs R² by Model', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Average R²')\n",
    "ax.set_ylabel('Average RMSE')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "output_png = '../05f_crossencoder_comparison.png'\n",
    "plt.savefig(output_png, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Visualization saved to: {output_png}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "overall_avg_r2 = df_summary['avg_r2'].mean()\n",
    "overall_std_r2 = df_summary['avg_r2'].std()\n",
    "best_model = df_summary.loc[df_summary['avg_r2'].idxmax()]\n",
    "worst_model = df_summary.loc[df_summary['avg_r2'].idxmin()]\n",
    "\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  Average R² across all combinations: {overall_avg_r2:.4f} ± {overall_std_r2:.4f}\")\n",
    "print(f\"  Best: {best_model['model']} + {best_model['llm']} (R²={best_model['avg_r2']:.4f})\")\n",
    "print(f\"  Worst: {worst_model['model']} + {worst_model['llm']} (R²={worst_model['avg_r2']:.4f})\")\n",
    "\n",
    "# Comparison with baseline\n",
    "baseline_r2 = 0.24  # BGE + Elastic Net baseline\n",
    "better_than_baseline = df_summary[df_summary['avg_r2'] > baseline_r2]\n",
    "\n",
    "print(f\"\\nComparison with BGE Baseline (R² = {baseline_r2:.2f}):\")\n",
    "print(f\"  Models better than baseline: {len(better_than_baseline)}/{len(df_summary)}\")\n",
    "\n",
    "if len(better_than_baseline) > 0:\n",
    "    print(f\"\\n  Top performers vs baseline:\")\n",
    "    for idx, row in better_than_baseline.nlargest(3, 'avg_r2').iterrows():\n",
    "        improvement = ((row['avg_r2'] - baseline_r2) / baseline_r2) * 100\n",
    "        print(f\"    {row['model']} + {row['llm']}: R²={row['avg_r2']:.4f} (+{improvement:.1f}%)\")\n",
    "else:\n",
    "    print(\"  ⚠️ No models outperformed the BGE baseline\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nRecommendations:\")\n",
    "if overall_avg_r2 > baseline_r2:\n",
    "    print(f\"  ✓ Cross-Encoder models show promise (avg R² = {overall_avg_r2:.4f})\")\n",
    "    print(f\"  ✓ Consider fine-tuning the top-performing models with LoRA\")\n",
    "    print(f\"  ✓ Best model: {best_model['model']} with {best_model['llm']} ground truth\")\n",
    "else:\n",
    "    print(f\"  ⚠️ Cross-Encoder zero-shot performance below baseline (avg R² = {overall_avg_r2:.4f})\")\n",
    "    print(f\"  → Consider LoRA fine-tuning to improve performance\")\n",
    "    print(f\"  → Alternatively, stick with BGE + Elastic Net approach\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Evaluation complete! Results saved to:\")\n",
    "print(f\"  - JSON: ../05f_crossencoder_zeroshot_results.json\")\n",
    "print(f\"  - CSV: ../05f_crossencoder_zeroshot_comparison.csv\")\n",
    "print(f\"  - PNG: ../05f_crossencoder_comparison.png\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
