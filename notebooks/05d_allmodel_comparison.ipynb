{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# OCEAN Ground Truth Model Comparison\n\nThis notebook compares the quality of OCEAN personality scores generated by 5 different LLM models:\n- Llama-3.1-8B\n- GPT-OSS-120B\n- Qwen-2.5-72B\n- Gemma-2-9B\n- DeepSeek-V3.1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup and Data Loading"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette('Set2')\n\nprint('Libraries imported successfully')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model configurations\nmodels = {\n    'llama_3.1_8b': 'Llama-3.1-8B',\n    'gpt_oss_120b': 'GPT-OSS-120B',\n    'qwen_2.5_72b': 'Qwen-2.5-72B',\n    'gemma_2_9b': 'Gemma-2-9B',\n    'deepseek_v3.1': 'DeepSeek-V3.1'\n}\n\n# Cost and time data (for 500 samples)\n# Note: Llama tokens/cost are N/A due to multiple retries\nmodel_metrics = {\n    'llama_3.1_8b': {'tokens': None, 'cost': None, 'time_min': 30.0},\n    'gpt_oss_120b': {'tokens': 817, 'cost': 0.10, 'time_min': 42.1},\n    'qwen_2.5_72b': {'tokens': 703, 'cost': 0.11, 'time_min': 70.2},\n    'gemma_2_9b': {'tokens': 1030, 'cost': 0.03, 'time_min': 19.0},\n    'deepseek_v3.1': {'tokens': 822, 'cost': 0.18, 'time_min': 32.5}\n}\n\nocean_dims = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n\nprint(f'Total models: {len(models)}')\nprint(f'OCEAN dimensions: {len(ocean_dims)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load all OCEAN ground truth files\ndata = {}\n\nfor model_id, model_name in models.items():\n    file_path = f'../ocean_ground_truth/{model_id}_ocean_500.csv'\n    try:\n        df = pd.read_csv(file_path)\n        data[model_id] = {\n            'name': model_name,\n            'df': df\n        }\n        print(f'Loaded: {model_name} ({len(df)} samples)')\n    except Exception as e:\n        print(f'Error loading {model_name}: {str(e)}')\n        data[model_id] = None\n\nprint(f'\\nSuccessfully loaded {sum(1 for v in data.values() if v is not None)}/{len(models)} models')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Quality Metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate quality metrics for each model\nquality_metrics = []\n\nfor model_id, model_data in data.items():\n    if model_data is None:\n        continue\n    \n    df = model_data['df']\n    model_name = model_data['name']\n    \n    # Basic statistics\n    total_samples = len(df)\n    valid_samples = df[ocean_dims].notna().all(axis=1).sum()\n    valid_pct = (valid_samples / total_samples) * 100\n    \n    # Count samples with all dimensions = 0.5\n    count_all_05 = 0\n    for idx, row in df.iterrows():\n        if all(abs(row[dim] - 0.5) < 0.01 for dim in ocean_dims if pd.notna(row[dim])):\n            count_all_05 += 1\n    \n    all_05_pct = (count_all_05 / total_samples) * 100\n    \n    # Calculate average standard deviation\n    avg_std = np.mean([df[dim].std() for dim in ocean_dims])\n    \n    # Get time metrics\n    time_min = model_metrics[model_id]['time_min']\n    time_per_sample_sec = (time_min * 60) / total_samples if time_min else None\n    \n    quality_metrics.append({\n        'Model': model_name,\n        'Total Samples': total_samples,\n        'Valid Samples': valid_samples,\n        'Valid Percentage': valid_pct,\n        'All=0.5 Count': count_all_05,\n        'All=0.5 Percentage': all_05_pct,\n        'Average Std Dev': avg_std,\n        'Processing Time (min)': time_min,\n        'Time per Sample (sec)': time_per_sample_sec\n    })\n\ndf_quality = pd.DataFrame(quality_metrics)\nprint('Data Quality Summary:')\nprint(df_quality.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "source": "# Calculate cost metrics for each model\ncost_metrics = []\n\nfor model_id, model_data in data.items():\n    if model_data is None:\n        continue\n    \n    model_name = model_data['name']\n    total_samples = len(model_data['df'])\n    \n    # Get cost data\n    tokens = model_metrics[model_id]['tokens']\n    cost = model_metrics[model_id]['cost']\n    \n    # Calculate per-sample metrics\n    if tokens is not None and cost is not None:\n        avg_tokens = tokens / total_samples\n        avg_cost = cost / total_samples\n        tokens_str = f'{tokens:,}'\n        cost_str = f'${cost:.4f}'\n        avg_tokens_str = f'{avg_tokens:.2f}'\n        avg_cost_str = f'${avg_cost:.6f}'\n    else:\n        tokens_str = 'N/A*'\n        cost_str = 'N/A*'\n        avg_tokens_str = 'N/A*'\n        avg_cost_str = 'N/A*'\n    \n    cost_metrics.append({\n        'Model': model_name,\n        'Total Tokens': tokens_str,\n        'Total Cost': cost_str,\n        'Avg Tokens/Sample': avg_tokens_str,\n        'Avg Cost/Sample': avg_cost_str\n    })\n\ndf_cost = pd.DataFrame(cost_metrics)\nprint('\\nCost Metrics Summary:')\nprint(df_cost.to_string(index=False))\nprint('\\n* Llama-3.1-8B: Total metrics N/A due to multiple retries during generation')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Statistical Analysis Per Dimension"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate statistics for each OCEAN dimension per model\nfor model_id, model_data in data.items():\n    if model_data is None:\n        continue\n    \n    df = model_data['df']\n    model_name = model_data['name']\n    \n    print(f'\\n{\"=\" * 80}')\n    print(f'{model_name}')\n    print(f'{\"=\" * 80}')\n    \n    stats_data = []\n    for dim in ocean_dims:\n        valid_values = df[dim].dropna()\n        if len(valid_values) > 0:\n            stats_data.append({\n                'Dimension': dim.capitalize(),\n                'Mean': f'{valid_values.mean():.3f}',\n                'Std': f'{valid_values.std():.3f}',\n                'Min': f'{valid_values.min():.3f}',\n                'Max': f'{valid_values.max():.3f}',\n                'Count=0.5': (abs(valid_values - 0.5) < 0.01).sum()\n            })\n    \n    df_stats = pd.DataFrame(stats_data)\n    print(df_stats.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Visualizations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1 Valid Sample Percentage Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\nmodels_list = df_quality['Model'].tolist()\nvalid_pcts = df_quality['Valid Percentage'].tolist()\n\ncolors = ['#e74c3c' if pct < 90 else '#f39c12' if pct < 95 else '#27ae60' for pct in valid_pcts]\n\nbars = ax.bar(models_list, valid_pcts, color=colors, edgecolor='black', linewidth=1.2)\n\nax.set_xlabel('Model', fontsize=12, fontweight='bold')\nax.set_ylabel('Valid Sample Percentage (%)', fontsize=12, fontweight='bold')\nax.set_title('Valid Sample Coverage Across Models', fontsize=14, fontweight='bold', pad=20)\nax.set_ylim([0, 105])\nax.axhline(y=90, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='90% threshold')\nax.axhline(y=95, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='95% threshold')\nax.legend()\n\nfor bar, pct in zip(bars, valid_pcts):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n            f'{pct:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Default Value (0.5) Percentage Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\ndefault_pcts = df_quality['All=0.5 Percentage'].tolist()\n\ncolors = ['#27ae60' if pct < 5 else '#f39c12' if pct < 20 else '#e74c3c' for pct in default_pcts]\n\nbars = ax.bar(models_list, default_pcts, color=colors, edgecolor='black', linewidth=1.2)\n\nax.set_xlabel('Model', fontsize=12, fontweight='bold')\nax.set_ylabel('All Dimensions = 0.5 Percentage (%)', fontsize=12, fontweight='bold')\nax.set_title('Default Value Prevalence Across Models (Lower is Better)', fontsize=14, fontweight='bold', pad=20)\nax.axhline(y=5, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='5% threshold')\nax.axhline(y=20, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='20% threshold')\nax.legend()\n\nfor bar, pct in zip(bars, default_pcts):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n            f'{pct:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 Distribution of OCEAN Dimensions by Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.flatten()\n\nfor idx, dim in enumerate(ocean_dims):\n    ax = axes[idx]\n    \n    box_data = []\n    labels = []\n    \n    for model_id, model_data in data.items():\n        if model_data is None:\n            continue\n        df = model_data['df']\n        valid_values = df[dim].dropna()\n        if len(valid_values) > 0:\n            box_data.append(valid_values)\n            labels.append(model_data['name'])\n    \n    bp = ax.boxplot(box_data, labels=labels, patch_artist=True, \n                     boxprops=dict(facecolor='lightblue', edgecolor='black'),\n                     medianprops=dict(color='red', linewidth=2),\n                     whiskerprops=dict(color='black'),\n                     capprops=dict(color='black'))\n    \n    ax.set_title(f'{dim.capitalize()}', fontsize=12, fontweight='bold')\n    ax.set_ylabel('Score', fontsize=10)\n    ax.set_ylim([0, 1])\n    ax.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.3)\n    ax.tick_params(axis='x', rotation=45, labelsize=8)\n    ax.grid(axis='y', alpha=0.3)\n\n# Remove extra subplot\nfig.delaxes(axes[5])\n\nplt.suptitle('OCEAN Dimension Distributions Across Models', fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.4 Heatmap: Mean Scores Across Models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create mean scores matrix\nmean_scores = []\nmodel_names = []\n\nfor model_id, model_data in data.items():\n    if model_data is None:\n        continue\n    df = model_data['df']\n    model_names.append(model_data['name'])\n    means = [df[dim].mean() for dim in ocean_dims]\n    mean_scores.append(means)\n\nmean_scores_df = pd.DataFrame(mean_scores, \n                               index=model_names,\n                               columns=[d.capitalize() for d in ocean_dims])\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.heatmap(mean_scores_df, annot=True, fmt='.3f', cmap='YlOrRd', \n            linewidths=0.5, linecolor='black', cbar_kws={'label': 'Mean Score'},\n            vmin=0, vmax=1, ax=ax)\n\nax.set_title('Mean OCEAN Scores Heatmap', fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('OCEAN Dimension', fontsize=12, fontweight='bold')\nax.set_ylabel('Model', fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.5 Standard Deviation Comparison"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.6 Processing Time Comparison",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\n# Get processing times\nprocessing_times = df_quality['Processing Time (min)'].tolist()\nmodels_list = df_quality['Model'].tolist()\n\n# Color code: faster is better (green), slower is worse (red)\ncolors = ['#27ae60' if t < 30 else '#f39c12' if t < 50 else '#e74c3c' for t in processing_times]\n\nbars = ax.barh(models_list, processing_times, color=colors, edgecolor='black', linewidth=1.2)\n\nax.set_xlabel('Processing Time (minutes)', fontsize=12, fontweight='bold')\nax.set_ylabel('Model', fontsize=12, fontweight='bold')\nax.set_title('Processing Time Comparison (500 Samples)', fontsize=14, fontweight='bold', pad=20)\nax.axvline(x=30, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='30 min')\nax.axvline(x=50, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='50 min')\nax.legend()\n\n# Add value labels\nfor bar, time_val in zip(bars, processing_times):\n    width = bar.get_width()\n    ax.text(width + 1, bar.get_y() + bar.get_height()/2., \n            f'{time_val:.1f} min', ha='left', va='center', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Show time efficiency\nprint('\\nTime Efficiency (samples per minute):')\nfor model, time_min in zip(models_list, processing_times):\n    samples_per_min = 500 / time_min\n    print(f'{model:20s}: {samples_per_min:.2f} samples/min')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.7 Average Cost per Sample Comparison",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Prepare cost data for visualization (including Llama with estimated avg cost)\ncost_viz_data = []\n\nfor model_id in models.keys():\n    model_name = models[model_id]\n    tokens = model_metrics[model_id]['tokens']\n    cost = model_metrics[model_id]['cost']\n    \n    if tokens is not None and cost is not None:\n        avg_cost = cost / 500  # 500 samples\n        cost_viz_data.append({'Model': model_name, 'Avg Cost': avg_cost, 'has_data': True})\n\n# Sort by cost\ncost_viz_data = sorted(cost_viz_data, key=lambda x: x['Avg Cost'])\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nmodels_for_cost = [d['Model'] for d in cost_viz_data]\navg_costs = [d['Avg Cost'] for d in cost_viz_data]\n\n# Color code: cheaper is better (green), expensive is worse (red)\ncolors = ['#27ae60' if c < 0.0002 else '#f39c12' if c < 0.0004 else '#e74c3c' for c in avg_costs]\n\nbars = ax.barh(models_for_cost, avg_costs, color=colors, edgecolor='black', linewidth=1.2)\n\nax.set_xlabel('Average Cost per Sample ($)', fontsize=12, fontweight='bold')\nax.set_ylabel('Model', fontsize=12, fontweight='bold')\nax.set_title('Cost Efficiency Comparison (Lower is Better)', fontsize=14, fontweight='bold', pad=20)\n\n# Add value labels\nfor bar, cost_val in zip(bars, avg_costs):\n    width = bar.get_width()\n    ax.text(width + width*0.05, bar.get_y() + bar.get_height()/2., \n            f'${cost_val:.6f}', ha='left', va='center', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Print cost summary\nprint('\\nCost Summary:')\nprint('=' * 60)\nfor item in cost_viz_data:\n    print(f'{item[\"Model\"]:20s}: ${item[\"Avg Cost\"]:.6f} per sample')\nprint('\\n* Note: Llama-3.1-8B excluded due to unreliable total cost data')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.8 Time vs Quality Trade-off Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "fig, ax = plt.subplots(figsize=(10, 8))\n\n# Prepare data for scatter plot\ntimes = df_quality['Processing Time (min)'].tolist()\nquality_scores = df_quality['Quality Score'].tolist()\nmodels_scatter = df_quality['Model'].tolist()\n\n# Create scatter plot with different colors for each model\ncolors_scatter = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n\nfor i, (model, time, score) in enumerate(zip(models_scatter, times, quality_scores)):\n    ax.scatter(time, score, s=300, color=colors_scatter[i], alpha=0.6, \n               edgecolors='black', linewidth=2, label=model, zorder=3)\n    # Add model label next to point\n    ax.annotate(model, (time, score), xytext=(5, 5), textcoords='offset points',\n                fontsize=9, fontweight='bold')\n\nax.set_xlabel('Processing Time (minutes)', fontsize=12, fontweight='bold')\nax.set_ylabel('Quality Score', fontsize=12, fontweight='bold')\nax.set_title('Time vs Quality Trade-off\\n(Top-right is ideal: High quality, Low time)', \n             fontsize=14, fontweight='bold', pad=20)\nax.grid(True, alpha=0.3, linestyle='--')\n\n# Add quadrant lines at median values\nmedian_time = np.median(times)\nmedian_quality = np.median(quality_scores)\nax.axvline(x=median_time, color='gray', linestyle='--', linewidth=1, alpha=0.5)\nax.axhline(y=median_quality, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n\n# Add quadrant labels\nax.text(median_time * 0.5, max(quality_scores) * 0.98, 'Fast & High Quality', \n        ha='center', va='top', fontsize=10, style='italic', color='green', fontweight='bold')\nax.text(max(times) * 0.85, max(quality_scores) * 0.98, 'Slow & High Quality', \n        ha='center', va='top', fontsize=10, style='italic', color='orange')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate efficiency metrics\nprint('\\nTime-Quality Efficiency Metrics:')\nprint('=' * 70)\nfor model, time, score in zip(models_scatter, times, quality_scores):\n    efficiency = score / time  # Quality per minute\n    print(f'{model:20s}: {efficiency:.2f} quality points per minute')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.9 Overall Value Analysis: Quality per Dollar",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Calculate value metrics (quality per dollar) - excluding Llama\nvalue_data = []\n\nfor model_id in models.keys():\n    model_name = models[model_id]\n    \n    # Get quality score for this model\n    model_quality = df_quality[df_quality['Model'] == model_name]['Quality Score'].values\n    \n    if len(model_quality) > 0:\n        quality_score = model_quality[0]\n        \n        # Get cost data\n        tokens = model_metrics[model_id]['tokens']\n        cost = model_metrics[model_id]['cost']\n        \n        if tokens is not None and cost is not None:\n            value_score = quality_score / cost  # Quality points per dollar\n            value_data.append({\n                'Model': model_name,\n                'Quality Score': quality_score,\n                'Total Cost': cost,\n                'Value Score': value_score\n            })\n\n# Sort by value score\nvalue_data = sorted(value_data, key=lambda x: x['Value Score'], reverse=True)\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nmodels_value = [d['Model'] for d in value_data]\nvalue_scores = [d['Value Score'] for d in value_data]\n\n# Color code: higher value is better\ncolors = ['#27ae60' if v > 1000 else '#f39c12' if v > 500 else '#e74c3c' for v in value_scores]\n\nbars = ax.barh(models_value, value_scores, color=colors, edgecolor='black', linewidth=1.2)\n\nax.set_xlabel('Value Score (Quality Points per Dollar)', fontsize=12, fontweight='bold')\nax.set_ylabel('Model', fontsize=12, fontweight='bold')\nax.set_title('Overall Value: Quality per Dollar Spent (Higher is Better)', \n             fontsize=14, fontweight='bold', pad=20)\n\n# Add value labels\nfor bar, val in zip(bars, value_scores):\n    width = bar.get_width()\n    ax.text(width + width*0.03, bar.get_y() + bar.get_height()/2., \n            f'{val:.1f}', ha='left', va='center', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Print comprehensive value analysis\nprint('\\nComprehensive Value Analysis:')\nprint('=' * 90)\nprint(f'{\"Model\":<20} {\"Quality\":<12} {\"Cost\":<12} {\"Value Score\":<15} {\"Rank\"}')\nprint('=' * 90)\nfor idx, item in enumerate(value_data, 1):\n    print(f'{item[\"Model\"]:<20} {item[\"Quality Score\"]:<12.2f} '\n          f'${item[\"Total Cost\"]:<11.4f} {item[\"Value Score\"]:<15.1f} #{idx}')\nprint('=' * 90)\nprint('\\n* Note: Llama-3.1-8B excluded due to unreliable cost data')\nprint('* Value Score = Quality Score / Total Cost (500 samples)')\nprint('* Higher value score means better quality-to-cost ratio')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create standard deviation matrix\nstd_scores = []\n\nfor model_id, model_data in data.items():\n    if model_data is None:\n        continue\n    df = model_data['df']\n    stds = [df[dim].std() for dim in ocean_dims]\n    std_scores.append(stds)\n\nstd_scores_df = pd.DataFrame(std_scores, \n                              index=model_names,\n                              columns=[d.capitalize() for d in ocean_dims])\n\nfig, ax = plt.subplots(figsize=(10, 6))\nstd_scores_df.T.plot(kind='bar', ax=ax, width=0.8)\n\nax.set_title('Standard Deviation by OCEAN Dimension and Model', fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('OCEAN Dimension', fontsize=12, fontweight='bold')\nax.set_ylabel('Standard Deviation', fontsize=12, fontweight='bold')\nax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Comparative Summary and Ranking"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate composite quality score\ndf_quality['Quality Score'] = (\n    df_quality['Valid Percentage'] * 0.3 + \n    (100 - df_quality['All=0.5 Percentage']) * 0.4 + \n    df_quality['Average Std Dev'] * 100 * 0.3\n)\n\n# Sort by quality score\ndf_quality_sorted = df_quality.sort_values('Quality Score', ascending=False).reset_index(drop=True)\n\nprint('\\n' + '='*100)\nprint('MODEL RANKINGS ACROSS MULTIPLE DIMENSIONS')\nprint('='*100)\n\n# 1. Quality Ranking\nprint('\\n1. QUALITY RANKING (Data Quality Only):')\nprint('=' * 100)\nfor idx, row in df_quality_sorted.iterrows():\n    print(f\"Rank {idx+1}: {row['Model']}\")\n    print(f\"  Valid Rate: {row['Valid Percentage']:.1f}%\")\n    print(f\"  Default Rate: {row['All=0.5 Percentage']:.1f}%\")\n    print(f\"  Avg Std Dev: {row['Average Std Dev']:.3f}\")\n    print(f\"  Quality Score: {row['Quality Score']:.2f}\")\n    print()\n\n# 2. Speed Ranking (Time Efficiency)\nprint('\\n2. SPEED RANKING (Fastest Processing):')\nprint('=' * 100)\ndf_time_sorted = df_quality.sort_values('Processing Time (min)', ascending=True).reset_index(drop=True)\nfor idx, row in df_time_sorted.iterrows():\n    print(f\"Rank {idx+1}: {row['Model']:<20} - {row['Processing Time (min)']:.1f} min \"\n          f\"({500/row['Processing Time (min)']:.2f} samples/min)\")\n\n# 3. Cost Efficiency Ranking\nprint('\\n\\n3. COST EFFICIENCY RANKING (Lowest Cost):')\nprint('=' * 100)\n# Build cost ranking data\ncost_ranking = []\nfor model_id in models.keys():\n    model_name = models[model_id]\n    cost = model_metrics[model_id]['cost']\n    if cost is not None:\n        model_row = df_quality[df_quality['Model'] == model_name].iloc[0]\n        cost_ranking.append({\n            'Model': model_name,\n            'Total Cost': cost,\n            'Avg Cost per Sample': cost / 500,\n            'Quality Score': model_row['Quality Score']\n        })\n\ncost_ranking_df = pd.DataFrame(cost_ranking).sort_values('Total Cost', ascending=True)\nfor idx, row in cost_ranking_df.iterrows():\n    print(f\"Rank {list(cost_ranking_df.index).index(idx)+1}: {row['Model']:<20} - \"\n          f\"Total: ${row['Total Cost']:.4f}, Per Sample: ${row['Avg Cost per Sample']:.6f}\")\nprint(\"\\n* Note: Llama-3.1-8B excluded due to unreliable cost data\")\n\n# 4. Value Ranking (Quality per Dollar)\nprint('\\n\\n4. VALUE RANKING (Best Quality-to-Cost Ratio):')\nprint('=' * 100)\nif len(value_data) > 0:\n    for idx, item in enumerate(value_data, 1):\n        print(f\"Rank {idx}: {item['Model']:<20} - Value Score: {item['Value Score']:.1f} \"\n              f\"(Quality: {item['Quality Score']:.2f} / Cost: ${item['Total Cost']:.4f})\")\nprint(\"\\n* Note: Llama-3.1-8B excluded due to unreliable cost data\")\n\n# 5. Time Efficiency Ranking (Quality per Minute)\nprint('\\n\\n5. TIME EFFICIENCY RANKING (Quality per Minute):')\nprint('=' * 100)\ndf_quality['Time Efficiency'] = df_quality['Quality Score'] / df_quality['Processing Time (min)']\ndf_time_eff_sorted = df_quality.sort_values('Time Efficiency', ascending=False).reset_index(drop=True)\nfor idx, row in df_time_eff_sorted.iterrows():\n    print(f\"Rank {idx+1}: {row['Model']:<20} - {row['Time Efficiency']:.2f} quality pts/min \"\n          f\"({row['Quality Score']:.2f} / {row['Processing Time (min)']:.1f} min)\")\n\nprint('\\n' + '='*100)\nprint('SUMMARY: Choose based on your priorities')\nprint('='*100)\nprint(f\"  Best Quality:        {df_quality_sorted.iloc[0]['Model']}\")\nprint(f\"  Fastest Processing:  {df_time_sorted.iloc[0]['Model']}\")\nif len(cost_ranking_df) > 0:\n    print(f\"  Lowest Cost:         {cost_ranking_df.iloc[0]['Model']}\")\nif len(value_data) > 0:\n    print(f\"  Best Value:          {value_data[0]['Model']}\")\nprint(f\"  Most Time-Efficient: {df_time_eff_sorted.iloc[0]['Model']}\")\nprint('='*100)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Recommendations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('\\n' + '='*100)\nprint('MODEL SELECTION RECOMMENDATIONS')\nprint('='*100)\n\n# Get the top models from different rankings\nbest_quality_model = df_quality_sorted.iloc[0]\nfastest_model = df_time_sorted.iloc[0]\nbest_time_eff_model = df_time_eff_sorted.iloc[0]\n\nprint('\\n1. RECOMMENDED FOR QUALITY (Best Data Quality):')\nprint('='*100)\nprint(f'MODEL: {best_quality_model[\"Model\"]}')\nprint(f'  - Quality Score: {best_quality_model[\"Quality Score\"]:.2f}')\nprint(f'  - Valid Rate: {best_quality_model[\"Valid Percentage\"]:.1f}%')\nprint(f'  - Non-default Scores: {100 - best_quality_model[\"All=0.5 Percentage\"]:.1f}%')\nprint(f'  - Average Std Dev: {best_quality_model[\"Average Std Dev\"]:.3f}')\nprint(f'  - Processing Time: {best_quality_model[\"Processing Time (min)\"]:.1f} minutes')\nprint()\nprint('Best for:')\nprint('  • Research and analysis where data quality is paramount')\nprint('  • Creating ground truth datasets for model training')\nprint('  • When you need the most reliable OCEAN personality assessments')\n\nprint('\\n\\n2. RECOMMENDED FOR SPEED (Fastest Processing):')\nprint('='*100)\nprint(f'MODEL: {fastest_model[\"Model\"]}')\nprint(f'  - Processing Time: {fastest_model[\"Processing Time (min)\"]:.1f} minutes')\nprint(f'  - Throughput: {500/fastest_model[\"Processing Time (min)\"]:.2f} samples/min')\nprint(f'  - Quality Score: {fastest_model[\"Quality Score\"]:.2f}')\nprint()\nprint('Best for:')\nprint('  • Real-time or near-real-time applications')\nprint('  • Processing large volumes of data quickly')\nprint('  • Time-sensitive production environments')\n\nif len(value_data) > 0:\n    best_value_model_name = value_data[0]['Model']\n    best_value_quality = value_data[0]['Quality Score']\n    best_value_cost = value_data[0]['Total Cost']\n    best_value_score = value_data[0]['Value Score']\n    \n    print('\\n\\n3. RECOMMENDED FOR VALUE (Best Quality-to-Cost Ratio):')\n    print('='*100)\n    print(f'MODEL: {best_value_model_name}')\n    print(f'  - Value Score: {best_value_score:.1f} quality pts/$')\n    print(f'  - Quality Score: {best_value_quality:.2f}')\n    print(f'  - Total Cost (500 samples): ${best_value_cost:.4f}')\n    print(f'  - Cost per Sample: ${best_value_cost/500:.6f}')\n    print()\n    print('Best for:')\n    print('  • Budget-conscious projects')\n    print('  • Large-scale deployments where cost adds up')\n    print('  • Balancing quality with budget constraints')\n\nprint('\\n\\n4. RECOMMENDED FOR EFFICIENCY (Best Time-Quality Balance):')\nprint('='*100)\nprint(f'MODEL: {best_time_eff_model[\"Model\"]}')\nprint(f'  - Time Efficiency: {best_time_eff_model[\"Time Efficiency\"]:.2f} quality pts/min')\nprint(f'  - Quality Score: {best_time_eff_model[\"Quality Score\"]:.2f}')\nprint(f'  - Processing Time: {best_time_eff_model[\"Processing Time (min)\"]:.1f} minutes')\nprint()\nprint('Best for:')\nprint('  • Production systems needing good quality without long waits')\nprint('  • Balanced approach between quality and speed')\nprint('  • Most practical applications with moderate requirements')\n\nprint('\\n\\n5. SCENARIO-BASED RECOMMENDATIONS:')\nprint('='*100)\nprint('\\nScenario A: Academic Research / Ground Truth Generation')\nprint(f'  → Recommendation: {best_quality_model[\"Model\"]}')\nprint('  → Reason: Highest data quality is critical for reliable research')\nprint()\nprint('\\nScenario B: Production API with High Volume')\nif len(value_data) > 0:\n    print(f'  → Recommendation: {best_value_model_name}')\n    print('  → Reason: Best balance of cost and quality for scale')\nprint()\nprint('\\nScenario C: Real-time User-Facing Application')\nprint(f'  → Recommendation: {fastest_model[\"Model\"]}')\nprint('  → Reason: Speed is critical for user experience')\nprint()\nprint('\\nScenario D: Prototype/MVP Development')\nif len(cost_ranking_df) > 0:\n    cheapest_model = cost_ranking_df.iloc[0]['Model']\n    print(f'  → Recommendation: {cheapest_model}')\n    print('  → Reason: Minimize costs while testing viability')\n\nprint('\\n\\n' + '='*100)\nprint('NEXT STEPS:')\nprint('='*100)\nprint('1. Select model based on your specific requirements (quality/speed/cost)')\nprint('2. Use selected model to generate OCEAN features for full dataset')\nprint('3. Train XGBoost model with OCEAN personality features')\nprint('4. Evaluate impact on loan default prediction performance')\nprint('5. Consider A/B testing if choosing between top 2-3 models')\nprint('='*100)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Export Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive export dataframe with all metrics\nexport_data = []\n\nfor model_id in models.keys():\n    model_name = models[model_id]\n    \n    # Get quality metrics\n    model_quality_row = df_quality[df_quality['Model'] == model_name]\n    if len(model_quality_row) == 0:\n        continue\n    \n    model_quality_row = model_quality_row.iloc[0]\n    \n    # Get cost metrics\n    tokens = model_metrics[model_id]['tokens']\n    cost = model_metrics[model_id]['cost']\n    time_min = model_metrics[model_id]['time_min']\n    \n    # Calculate derived metrics\n    if tokens is not None and cost is not None:\n        avg_tokens = tokens / 500\n        avg_cost = cost / 500\n        value_score = model_quality_row['Quality Score'] / cost\n    else:\n        avg_tokens = None\n        avg_cost = None\n        value_score = None\n    \n    time_efficiency = model_quality_row['Quality Score'] / time_min if time_min else None\n    \n    export_data.append({\n        'Model': model_name,\n        'Quality Score': model_quality_row['Quality Score'],\n        'Valid Percentage': model_quality_row['Valid Percentage'],\n        'Default Value Percentage': model_quality_row['All=0.5 Percentage'],\n        'Average Std Dev': model_quality_row['Average Std Dev'],\n        'Processing Time (min)': time_min,\n        'Time per Sample (sec)': model_quality_row['Time per Sample (sec)'],\n        'Total Tokens': tokens if tokens is not None else 'N/A',\n        'Total Cost ($)': cost if cost is not None else 'N/A',\n        'Avg Tokens per Sample': avg_tokens if avg_tokens is not None else 'N/A',\n        'Avg Cost per Sample ($)': avg_cost if avg_cost is not None else 'N/A',\n        'Value Score (Quality/$)': value_score if value_score is not None else 'N/A',\n        'Time Efficiency (Quality/min)': time_efficiency if time_efficiency is not None else 'N/A'\n    })\n\ndf_export = pd.DataFrame(export_data)\n\n# Sort by quality score for export\ndf_export_sorted = df_export.sort_values('Quality Score', ascending=False)\n\n# Save comprehensive summary\noutput_file = '../ocean_ground_truth/model_comparison_summary.csv'\ndf_export_sorted.to_csv(output_file, index=False)\nprint(f'Comprehensive summary saved to: {output_file}')\nprint()\nprint('Exported columns:')\nfor idx, col in enumerate(df_export_sorted.columns, 1):\n    print(f'  {idx}. {col}')\n\n# Also save rankings\nrankings_file = '../ocean_ground_truth/model_rankings.csv'\nrankings_data = []\n\n# Quality ranking\nfor idx, row in df_quality_sorted.iterrows():\n    rankings_data.append({\n        'Ranking Type': 'Quality',\n        'Rank': idx + 1,\n        'Model': row['Model'],\n        'Score/Metric': row['Quality Score']\n    })\n\n# Speed ranking\nfor idx, row in df_time_sorted.iterrows():\n    rankings_data.append({\n        'Ranking Type': 'Speed',\n        'Rank': idx + 1,\n        'Model': row['Model'],\n        'Score/Metric': row['Processing Time (min)']\n    })\n\n# Time efficiency ranking\nfor idx, row in df_time_eff_sorted.iterrows():\n    rankings_data.append({\n        'Ranking Type': 'Time Efficiency',\n        'Rank': idx + 1,\n        'Model': row['Model'],\n        'Score/Metric': row['Time Efficiency']\n    })\n\n# Cost ranking\nif len(cost_ranking_df) > 0:\n    for idx, row in cost_ranking_df.iterrows():\n        rankings_data.append({\n            'Ranking Type': 'Cost',\n            'Rank': list(cost_ranking_df.index).index(idx) + 1,\n            'Model': row['Model'],\n            'Score/Metric': row['Total Cost']\n        })\n\n# Value ranking\nif len(value_data) > 0:\n    for idx, item in enumerate(value_data, 1):\n        rankings_data.append({\n            'Ranking Type': 'Value',\n            'Rank': idx,\n            'Model': item['Model'],\n            'Score/Metric': item['Value Score']\n        })\n\ndf_rankings = pd.DataFrame(rankings_data)\ndf_rankings.to_csv(rankings_file, index=False)\nprint(f'\\nRankings saved to: {rankings_file}')\n\nprint()\nprint('='*80)\nprint('ANALYSIS COMPLETE!')\nprint('='*80)\nprint('\\nFiles generated:')\nprint(f'  1. {output_file}')\nprint(f'     - Comprehensive model comparison with all metrics')\nprint(f'  2. {rankings_file}')\nprint(f'     - Model rankings across different dimensions')\nprint()\nprint('You can now:')\nprint('  • Review the exported CSVs for detailed analysis')\nprint('  • Share results with your team')\nprint('  • Use rankings to make informed model selection decisions')\nprint('='*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}