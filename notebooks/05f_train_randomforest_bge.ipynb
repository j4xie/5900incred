{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 05f - Train Random Forest Models (All LLMs, BGE Embeddings)\n",
    "\n",
    "**Purpose**: Train Random Forest regression models for all 5 LLMs to learn non-linear mapping from BGE embeddings to OCEAN scores\n",
    "\n",
    "**Why Random Forest over Elastic Net?**\n",
    "- Elastic Net (Linear): Avg Test R² = 0.127\n",
    "- Problem: Embeddings → OCEAN relationship may be non-linear\n",
    "- Random Forest advantages:\n",
    "  - Captures non-linear patterns\n",
    "  - Handles high-dimensional data well\n",
    "  - Built-in feature importance\n",
    "  - Less prone to overfitting (with proper tuning)\n",
    "\n",
    "**Input Files**:\n",
    "- bge_embeddings_500.npy - BGE embeddings (500x1024)\n",
    "- ocean_ground_truth/[llm]_ocean_500.csv - OCEAN ground truth for each LLM\n",
    "\n",
    "**Output Files** (per LLM):\n",
    "- randomforest_models_bge_[llm].pkl - 5 Random Forest models + Scaler\n",
    "- 05f_randomforest_training_report_[llm].json - Training report with feature importance\n",
    "\n",
    "**Summary Output**:\n",
    "- 05f_randomforest_comparison.png - Performance comparison across LLMs\n",
    "- 05f_elasticnet_vs_randomforest.csv - Elastic Net vs Random Forest comparison\n",
    "\n",
    "**Estimated Time**: Approximately 20-30 minutes (5 LLMs x GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM configurations\n",
    "LLM_CONFIGS = {\n",
    "    'llama': {\n",
    "        'name': 'Llama-3.1-8B',\n",
    "        'ocean_file': '../ocean_ground_truth/llama_3.1_8b_ocean_500.csv',\n",
    "        'elasticnet_report': '../05f_elasticnet_training_report_llama.json'\n",
    "    },\n",
    "    'gpt': {\n",
    "        'name': 'GPT-OSS-120B',\n",
    "        'ocean_file': '../ocean_ground_truth/gpt_oss_120b_ocean_500.csv',\n",
    "        'elasticnet_report': '../05f_elasticnet_training_report_gpt.json'\n",
    "    },\n",
    "    'gemma': {\n",
    "        'name': 'Gemma-2-9B',\n",
    "        'ocean_file': '../ocean_ground_truth/gemma_2_9b_ocean_500.csv',\n",
    "        'elasticnet_report': '../05f_elasticnet_training_report_gemma.json'\n",
    "    },\n",
    "    'deepseek': {\n",
    "        'name': 'DeepSeek-V3.1',\n",
    "        'ocean_file': '../ocean_ground_truth/deepseek_v3.1_ocean_500.csv',\n",
    "        'elasticnet_report': '../05f_elasticnet_training_report_deepseek.json'\n",
    "    },\n",
    "    'qwen': {\n",
    "        'name': 'Qwen-2.5-72B',\n",
    "        'ocean_file': '../ocean_ground_truth/qwen_2.5_72b_ocean_500.csv',\n",
    "        'elasticnet_report': '../05f_elasticnet_training_report_qwen.json'\n",
    "    }\n",
    "}\n",
    "\n",
    "# OCEAN dimensions\n",
    "OCEAN_DIMS = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "\n",
    "# Random Forest hyperparameters (conservative to prevent overfitting)\n",
    "PARAM_GRID = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [10, 20, 30],\n",
    "    'min_samples_leaf': [5, 10, 15],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  LLM models: {len(LLM_CONFIGS)}\")\n",
    "print(f\"  OCEAN dimensions: {len(OCEAN_DIMS)}\")\n",
    "print(f\"  GridSearch parameters:\")\n",
    "for param, values in PARAM_GRID.items():\n",
    "    print(f\"    - {param}: {values}\")\n",
    "print(f\"  Total combinations: {np.prod([len(v) for v in PARAM_GRID.values()])} per dimension\")\n",
    "print(f\"  CV folds: {CV_FOLDS}\")\n",
    "print(f\"\\n  Note: Conservative parameters to prevent overfitting with 400 training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Load BGE Embeddings (Shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading BGE Embeddings\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "embedding_file = '../bge_embeddings_500.npy'\n",
    "print(f\"\\nLoading: {embedding_file}\")\n",
    "X_full = np.load(embedding_file)\n",
    "print(f\"Embeddings shape: {X_full.shape}\")\n",
    "print(f\"  Data type: {X_full.dtype}\")\n",
    "print(f\"  Memory usage: {X_full.nbytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  Value range: [{X_full.min():.4f}, {X_full.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Train Random Forest Models for Each LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for all results\n",
    "all_results = {}\n",
    "elasticnet_comparison = {}\n",
    "\n",
    "for llm_key, llm_config in LLM_CONFIGS.items():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Training Random Forest Models: {llm_config['name']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load OCEAN targets\n",
    "    print(f\"\\n[1/7] Loading OCEAN targets...\")\n",
    "    ocean_file = llm_config['ocean_file']\n",
    "    y_df = pd.read_csv(ocean_file)\n",
    "    print(f\"  Shape: {y_df.shape}\")\n",
    "    print(f\"  Columns: {y_df.columns.tolist()}\")\n",
    "    \n",
    "    # Check and handle NaN values\n",
    "    nan_count_total = y_df.isnull().sum().sum()\n",
    "    if nan_count_total > 0:\n",
    "        print(f\"  Warning: Found {nan_count_total} NaN values\")\n",
    "        nan_indices = y_df[y_df.isnull().any(axis=1)].index\n",
    "        y_df = y_df.dropna()\n",
    "        X = np.delete(X_full, nan_indices, axis=0)\n",
    "        print(f\"  After dropping NaN: {len(y_df)} samples\")\n",
    "    else:\n",
    "        X = X_full.copy()\n",
    "    \n",
    "    # Verify consistency\n",
    "    if len(X) != len(y_df):\n",
    "        raise ValueError(f\"Data inconsistency: X={len(X)}, y={len(y_df)}\")\n",
    "    \n",
    "    # Train/test split\n",
    "    print(f\"\\n[2/7] Splitting data (80/20)...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_df,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    print(f\"  Training: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "    print(f\"  Feature-to-sample ratio: {X_train.shape[1] / X_train.shape[0]:.2f}:1\")\n",
    "    \n",
    "    # Standardize (Random Forest doesn't require but helps)\n",
    "    print(f\"\\n[3/7] Standardizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(f\"  Train mean={X_train_scaled.mean():.6f}, std={X_train_scaled.std():.6f}\")\n",
    "    print(f\"  Test mean={X_test_scaled.mean():.6f}, std={X_test_scaled.std():.6f}\")\n",
    "    \n",
    "    # Train models\n",
    "    print(f\"\\n[4/7] Training Random Forest models (5 dimensions)...\")\n",
    "    print(f\"  Using GridSearchCV with {len(PARAM_GRID['n_estimators']) * len(PARAM_GRID['max_depth']) * len(PARAM_GRID['min_samples_split']) * len(PARAM_GRID['min_samples_leaf']) * len(PARAM_GRID['max_features'])} combinations\")\n",
    "    print(f\"  This may take 20-30 minutes...\")\n",
    "    \n",
    "    rf_models = {}\n",
    "    training_results = {}\n",
    "    \n",
    "    for i, dim in enumerate(OCEAN_DIMS):\n",
    "        print(f\"\\n  [{i+1}/5] Training {dim}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get target\n",
    "        y_train_dim = y_train[dim].values\n",
    "        y_test_dim = y_test[dim].values\n",
    "        \n",
    "        # Create base model\n",
    "        rf_base = RandomForestRegressor(\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            rf_base,\n",
    "            PARAM_GRID,\n",
    "            cv=CV_FOLDS,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit\n",
    "        grid_search.fit(X_train_scaled, y_train_dim)\n",
    "        model = grid_search.best_estimator_\n",
    "        \n",
    "        # Predict\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Metrics\n",
    "        train_r2 = r2_score(y_train_dim, y_train_pred)\n",
    "        test_r2 = r2_score(y_test_dim, y_test_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_dim, y_train_pred))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test_dim, y_test_pred))\n",
    "        train_mae = mean_absolute_error(y_train_dim, y_train_pred)\n",
    "        test_mae = mean_absolute_error(y_test_dim, y_test_pred)\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importances = model.feature_importances_\n",
    "        top_indices = np.argsort(feature_importances)[-20:][::-1]\n",
    "        top_features = [\n",
    "            {'index': int(idx), 'importance': float(feature_importances[idx])}\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        # Save model and results\n",
    "        rf_models[dim] = model\n",
    "        training_results[dim] = {\n",
    "            'train_r2': float(train_r2),\n",
    "            'test_r2': float(test_r2),\n",
    "            'train_rmse': float(train_rmse),\n",
    "            'test_rmse': float(test_rmse),\n",
    "            'train_mae': float(train_mae),\n",
    "            'test_mae': float(test_mae),\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'cv_best_score': float(grid_search.best_score_),\n",
    "            'top_20_features': top_features,\n",
    "            'training_time_seconds': float(time.time() - start_time)\n",
    "        }\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"      Best params: {grid_search.best_params_}\")\n",
    "        print(f\"      CV score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"      Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "        print(f\"      Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"      Time: {elapsed:.1f}s\")\n",
    "        \n",
    "        # Check overfitting\n",
    "        if train_r2 - test_r2 > 0.3:\n",
    "            print(f\"      ⚠️  Warning: Possible overfitting (train-test gap: {train_r2 - test_r2:.3f})\")\n",
    "    \n",
    "    # Save models\n",
    "    print(f\"\\n[5/7] Saving models...\")\n",
    "    model_data = {\n",
    "        'models': rf_models,\n",
    "        'scaler': scaler,\n",
    "        'ocean_dims': OCEAN_DIMS,\n",
    "        'training_results': training_results,\n",
    "        'training_timestamp': datetime.now().isoformat(),\n",
    "        'llm_model': llm_config['name'],\n",
    "        'hyperparameters': PARAM_GRID\n",
    "    }\n",
    "    \n",
    "    model_file = f'../randomforest_models_bge_{llm_key}.pkl'\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    print(f\"  Saved: {model_file} ({os.path.getsize(model_file) / 1024:.1f} KB)\")\n",
    "    \n",
    "    # Load ElasticNet results for comparison\n",
    "    print(f\"\\n[6/7] Loading ElasticNet results for comparison...\")\n",
    "    try:\n",
    "        with open(llm_config['elasticnet_report'], 'r') as f:\n",
    "            elasticnet_data = json.load(f)\n",
    "        elasticnet_comparison[llm_key] = elasticnet_data['training_results']\n",
    "        print(f\"  ✓ ElasticNet report loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Could not load ElasticNet report: {e}\")\n",
    "        elasticnet_comparison[llm_key] = None\n",
    "    \n",
    "    # Generate report\n",
    "    print(f\"\\n[7/7] Generating training report...\")\n",
    "    report = {\n",
    "        'phase': f'05f - Train Random Forest Models ({llm_config[\"name\"]}, BGE Embeddings)',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'llm_model': llm_config['name'],\n",
    "        'embedding_model': 'BAAI/bge-large-en-v1.5',\n",
    "        'embedding_dimension': 1024,\n",
    "        'training_samples': int(X_train.shape[0]),\n",
    "        'test_samples': int(X_test.shape[0]),\n",
    "        'model_type': 'Random Forest Regressor',\n",
    "        'hyperparameter_grid': PARAM_GRID,\n",
    "        'ocean_dimensions': OCEAN_DIMS,\n",
    "        'model_file': model_file,\n",
    "        'training_results': training_results\n",
    "    }\n",
    "    \n",
    "    # Summary metrics\n",
    "    test_r2_scores = [training_results[dim]['test_r2'] for dim in OCEAN_DIMS]\n",
    "    test_rmse_scores = [training_results[dim]['test_rmse'] for dim in OCEAN_DIMS]\n",
    "    test_mae_scores = [training_results[dim]['test_mae'] for dim in OCEAN_DIMS]\n",
    "    train_r2_scores = [training_results[dim]['train_r2'] for dim in OCEAN_DIMS]\n",
    "    total_training_time = sum([training_results[dim]['training_time_seconds'] for dim in OCEAN_DIMS])\n",
    "    \n",
    "    report['summary_metrics'] = {\n",
    "        'avg_test_r2': float(np.mean(test_r2_scores)),\n",
    "        'avg_train_r2': float(np.mean(train_r2_scores)),\n",
    "        'avg_test_rmse': float(np.mean(test_rmse_scores)),\n",
    "        'avg_test_mae': float(np.mean(test_mae_scores)),\n",
    "        'min_test_r2': float(np.min(test_r2_scores)),\n",
    "        'max_test_r2': float(np.max(test_r2_scores)),\n",
    "        'avg_overfitting_gap': float(np.mean(train_r2_scores) - np.mean(test_r2_scores)),\n",
    "        'total_training_time_seconds': float(total_training_time)\n",
    "    }\n",
    "    \n",
    "    report_file = f'../05f_randomforest_training_report_{llm_key}.json'\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"  Report saved: {report_file}\")\n",
    "    \n",
    "    # Store for final comparison\n",
    "    all_results[llm_key] = {\n",
    "        'name': llm_config['name'],\n",
    "        'training_results': training_results,\n",
    "        'summary': report['summary_metrics']\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n  Summary for {llm_config['name']}:\")\n",
    "    print(f\"    Avg Test R²: {report['summary_metrics']['avg_test_r2']:.4f}\")\n",
    "    print(f\"    Avg Train R²: {report['summary_metrics']['avg_train_r2']:.4f}\")\n",
    "    print(f\"    Overfitting gap: {report['summary_metrics']['avg_overfitting_gap']:.4f}\")\n",
    "    print(f\"    Test R² range: [{report['summary_metrics']['min_test_r2']:.4f}, {report['summary_metrics']['max_test_r2']:.4f}]\")\n",
    "    print(f\"    Total training time: {total_training_time/60:.1f} minutes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All Random Forest models trained successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Generate Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generating Comparison Visualizations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for llm_key, results in all_results.items():\n",
    "    for dim in OCEAN_DIMS:\n",
    "        rf_r2 = results['training_results'][dim]['test_r2']\n",
    "        rf_train_r2 = results['training_results'][dim]['train_r2']\n",
    "        rf_rmse = results['training_results'][dim]['test_rmse']\n",
    "        \n",
    "        # Get ElasticNet results if available\n",
    "        if elasticnet_comparison.get(llm_key):\n",
    "            en_r2 = elasticnet_comparison[llm_key][dim]['test_r2']\n",
    "            en_rmse = elasticnet_comparison[llm_key][dim]['test_rmse']\n",
    "        else:\n",
    "            en_r2 = None\n",
    "            en_rmse = None\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'LLM': results['name'],\n",
    "            'llm_key': llm_key,\n",
    "            'Dimension': dim,\n",
    "            'RandomForest_R2': rf_r2,\n",
    "            'RandomForest_Train_R2': rf_train_r2,\n",
    "            'ElasticNet_R2': en_r2,\n",
    "            'R2_Improvement': rf_r2 - en_r2 if en_r2 else None,\n",
    "            'RandomForest_RMSE': rf_rmse,\n",
    "            'ElasticNet_RMSE': en_rmse,\n",
    "            'Overfitting_Gap': rf_train_r2 - rf_r2\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_file = '../05f_elasticnet_vs_randomforest.csv'\n",
    "comparison_df.to_csv(comparison_file, index=False)\n",
    "print(f\"\\nComparison table saved: {comparison_file}\")\n",
    "print(f\"\\nPreview:\")\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Random Forest vs ElasticNet Performance Comparison (All LLMs)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Test R² Comparison\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x_pos - width/2, comparison_df['RandomForest_R2'], width, label='Random Forest', color='#9b59b6', alpha=0.8)\n",
    "if comparison_df['ElasticNet_R2'].notna().any():\n",
    "    ax1.bar(x_pos + width/2, comparison_df['ElasticNet_R2'], width, label='ElasticNet', color='#2ecc71', alpha=0.8)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_xlabel('Model-Dimension', fontsize=10)\n",
    "ax1.set_ylabel('Test R² Score', fontsize=10)\n",
    "ax1.set_title('Test R² Comparison (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=90, labelsize=7)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f\"{row['llm_key'][:3]}-{row['Dimension'][:3]}\" for _, row in comparison_df.iterrows()])\n",
    "\n",
    "# 2. Average R² by LLM\n",
    "ax2 = axes[0, 1]\n",
    "avg_by_llm = comparison_df.groupby('LLM')[['RandomForest_R2', 'ElasticNet_R2']].mean()\n",
    "avg_by_llm.plot(kind='bar', ax=ax2, color=['#9b59b6', '#2ecc71'], alpha=0.8)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_xlabel('LLM Model', fontsize=10)\n",
    "ax2.set_ylabel('Average Test R²', fontsize=10)\n",
    "ax2.set_title('Average Test R² by LLM', fontsize=12, fontweight='bold')\n",
    "ax2.legend(['Random Forest', 'ElasticNet'])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. R² Improvement (RF - ElasticNet)\n",
    "ax3 = axes[1, 0]\n",
    "if comparison_df['R2_Improvement'].notna().any():\n",
    "    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in comparison_df['R2_Improvement']]\n",
    "    ax3.bar(x_pos, comparison_df['R2_Improvement'], color=colors, alpha=0.8)\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax3.set_xlabel('Model-Dimension', fontsize=10)\n",
    "    ax3.set_ylabel('R² Improvement', fontsize=10)\n",
    "    ax3.set_title('R² Improvement (RF - ElasticNet)', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.tick_params(axis='x', rotation=90, labelsize=7)\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels([f\"{row['llm_key'][:3]}-{row['Dimension'][:3]}\" for _, row in comparison_df.iterrows()])\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'ElasticNet data not available', ha='center', va='center', transform=ax3.transAxes)\n",
    "\n",
    "# 4. Overfitting Analysis\n",
    "ax4 = axes[1, 1]\n",
    "overfitting_by_llm = comparison_df.groupby('LLM')['Overfitting_Gap'].mean().sort_values(ascending=False)\n",
    "colors_overfitting = ['#e74c3c' if x > 0.3 else '#f39c12' if x > 0.15 else '#2ecc71' for x in overfitting_by_llm]\n",
    "overfitting_by_llm.plot(kind='barh', ax=ax4, color=colors_overfitting, alpha=0.8)\n",
    "ax4.axvline(x=0.15, color='orange', linestyle='--', linewidth=1, alpha=0.7, label='Warning (0.15)')\n",
    "ax4.axvline(x=0.3, color='red', linestyle='--', linewidth=1, alpha=0.7, label='Severe (0.30)')\n",
    "ax4.set_xlabel('Train-Test R² Gap', fontsize=10)\n",
    "ax4.set_ylabel('LLM Model', fontsize=10)\n",
    "ax4.set_title('Overfitting Analysis (Train - Test R²)', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "viz_file = '../05f_randomforest_comparison.png'\n",
    "plt.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nVisualization saved: {viz_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 6: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY (Random Forest)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Overall Performance (Random Forest):\")\n",
    "print(f\"   Average Test R² across all models: {comparison_df['RandomForest_R2'].mean():.4f}\")\n",
    "print(f\"   Best Test R²: {comparison_df['RandomForest_R2'].max():.4f}\")\n",
    "print(f\"   Worst Test R²: {comparison_df['RandomForest_R2'].min():.4f}\")\n",
    "print(f\"   Std Dev: {comparison_df['RandomForest_R2'].std():.4f}\")\n",
    "\n",
    "if comparison_df['ElasticNet_R2'].notna().any():\n",
    "    print(\"\\n2. Comparison with ElasticNet:\")\n",
    "    print(f\"   Average ElasticNet Test R²: {comparison_df['ElasticNet_R2'].mean():.4f}\")\n",
    "    print(f\"   Average Improvement: {comparison_df['R2_Improvement'].mean():.4f}\")\n",
    "    improved_count = (comparison_df['R2_Improvement'] > 0).sum()\n",
    "    print(f\"   Models improved: {improved_count}/{len(comparison_df)}\")\n",
    "    if improved_count > 0:\n",
    "        print(f\"   Best improvement: {comparison_df['R2_Improvement'].max():.4f}\")\n",
    "    if improved_count < len(comparison_df):\n",
    "        print(f\"   Worst change: {comparison_df['R2_Improvement'].min():.4f}\")\n",
    "\n",
    "print(\"\\n3. Overfitting Analysis:\")\n",
    "print(f\"   Average overfitting gap: {comparison_df['Overfitting_Gap'].mean():.4f}\")\n",
    "print(f\"   Max overfitting gap: {comparison_df['Overfitting_Gap'].max():.4f}\")\n",
    "severe_overfitting = (comparison_df['Overfitting_Gap'] > 0.3).sum()\n",
    "moderate_overfitting = ((comparison_df['Overfitting_Gap'] > 0.15) & (comparison_df['Overfitting_Gap'] <= 0.3)).sum()\n",
    "print(f\"   Severe overfitting (>0.3): {severe_overfitting}/{len(comparison_df)} models\")\n",
    "print(f\"   Moderate overfitting (0.15-0.3): {moderate_overfitting}/{len(comparison_df)} models\")\n",
    "\n",
    "print(\"\\n4. Best Performing LLM:\")\n",
    "best_llm = comparison_df.groupby('LLM')['RandomForest_R2'].mean().idxmax()\n",
    "best_r2 = comparison_df.groupby('LLM')['RandomForest_R2'].mean().max()\n",
    "print(f\"   {best_llm}: {best_r2:.4f}\")\n",
    "\n",
    "print(\"\\n5. Best Performing Dimension:\")\n",
    "best_dim = comparison_df.groupby('Dimension')['RandomForest_R2'].mean().idxmax()\n",
    "best_dim_r2 = comparison_df.groupby('Dimension')['RandomForest_R2'].mean().max()\n",
    "print(f\"   {best_dim}: {best_dim_r2:.4f}\")\n",
    "\n",
    "print(\"\\n6. Model Recommendation:\")\n",
    "rf_avg = comparison_df['RandomForest_R2'].mean()\n",
    "if comparison_df['ElasticNet_R2'].notna().any():\n",
    "    en_avg = comparison_df['ElasticNet_R2'].mean()\n",
    "    improvement = rf_avg - en_avg\n",
    "    avg_overfitting = comparison_df['Overfitting_Gap'].mean()\n",
    "    \n",
    "    if improvement > 0.05 and avg_overfitting < 0.2:\n",
    "        print(f\"   ✅ RECOMMEND: Random Forest (Improvement: +{improvement:.4f}, Low overfitting)\")\n",
    "    elif improvement > 0.02:\n",
    "        print(f\"   ⚠️  CAUTIOUS: Random Forest slightly better (+{improvement:.4f}) but check overfitting\")\n",
    "    else:\n",
    "        print(f\"   ❌ RECOMMEND: ElasticNet (RF improvement too small: +{improvement:.4f})\")\n",
    "else:\n",
    "    print(f\"   Average R²: {rf_avg:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Output Files Generated:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Models:\")\n",
    "for llm_key in LLM_CONFIGS.keys():\n",
    "    print(f\"  - randomforest_models_bge_{llm_key}.pkl\")\n",
    "print(\"\\nReports:\")\n",
    "for llm_key in LLM_CONFIGS.keys():\n",
    "    print(f\"  - 05f_randomforest_training_report_{llm_key}.json\")\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  - 05f_elasticnet_vs_randomforest.csv\")\n",
    "print(f\"  - 05f_randomforest_comparison.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"05f Random Forest Training Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "**Key Findings to Look For**:\n",
    "\n",
    "1. **Non-linear vs Linear**:\n",
    "   - Random Forest can capture non-linear patterns\n",
    "   - If RF >> ElasticNet: suggests non-linear relationships exist\n",
    "   - If RF ≈ ElasticNet: relationship is mostly linear\n",
    "\n",
    "2. **Overfitting Detection**:\n",
    "   - Train-Test R² gap < 0.15: Good\n",
    "   - Gap 0.15-0.30: Moderate overfitting\n",
    "   - Gap > 0.30: Severe overfitting (need more data or simpler model)\n",
    "\n",
    "3. **Feature Importance**:\n",
    "   - Check which embedding dimensions are most important\n",
    "   - Compare with ElasticNet's selected features\n",
    "\n",
    "4. **Next Steps**:\n",
    "   - If RF R² > 0.25: Random Forest is valuable\n",
    "   - If RF R² < 0.15: Need more training data\n",
    "   - If severe overfitting: Consider ensemble methods or increase min_samples_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
