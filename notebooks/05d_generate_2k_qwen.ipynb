{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05d Extended - Generate 2,000 OCEAN Ground Truth (Qwen-2.5-72B)\n",
    "\n",
    "## Purpose\n",
    "Generate high-quality OCEAN personality scores for 2,000 loan application samples using Qwen-2.5-72B.\n",
    "\n",
    "## Model Selection\n",
    "- **Selected Model**: Qwen-2.5-72B (Qwen/Qwen2.5-72B-Instruct)\n",
    "- **Provider**: nebius\n",
    "\n",
    "## Expected Metrics\n",
    "- Total samples: 2,000\n",
    "- Estimated time: ~281 minutes\n",
    "- Expected cost: ~$0.44\n",
    "\n",
    "## Output Files\n",
    "- `ocean_targets_2000_qwen.csv`: OCEAN scores for 2,000 samples\n",
    "- `samples_2000_with_desc_qwen.csv`: Sample descriptions\n",
    "- `samples_2000_metadata_qwen.csv`: Sample metadata\n",
    "- `.checkpoint_2k_ocean_qwen.json`: Resume capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print('Libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF token\n",
    "def load_env():\n",
    "    env_dict = {}\n",
    "    try:\n",
    "        with open('../.env', 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip() and not line.startswith('#'):\n",
    "                    key, value = line.strip().split('=', 1)\n",
    "                    env_dict[key] = value\n",
    "    except:\n",
    "        print('Warning: Unable to read .env file')\n",
    "    return env_dict\n",
    "\n",
    "env_vars = load_env()\n",
    "HF_TOKEN = env_vars.get('HF_TOKEN', '')\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError('HF_TOKEN not found in .env file!')\n",
    "\n",
    "print(f'HF token loaded: {HF_TOKEN[:10]}...{HF_TOKEN[-5:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-72B-Instruct'\n",
    "PROVIDER = 'nebius'\n",
    "DISPLAY_NAME = 'Qwen-2.5-72B'\n",
    "\n",
    "# File paths\n",
    "DATA_FILE = '../loan_final_desc50plus_with_ocean_bge.csv'\n",
    "OUTPUT_OCEAN = '../ocean_targets_2000_qwen.csv'\n",
    "OUTPUT_METADATA = '../samples_2000_metadata_qwen.csv'\n",
    "CHECKPOINT_FILE = '../.checkpoint_2k_ocean_qwen.json'\n",
    "\n",
    "# Parameters\n",
    "SAMPLE_SIZE = 2000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f'Model: {DISPLAY_NAME}')\n",
    "print(f'Provider: {PROVIDER}')\n",
    "print(f'Target samples: {SAMPLE_SIZE:,}')\n",
    "print(f'Random seed: {RANDOM_STATE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Sample 2000 rows randomly\nprint(f'\\nSampling {SAMPLE_SIZE:,} samples (random seed: {RANDOM_STATE})...')\n\nnp.random.seed(RANDOM_STATE)\ndf_samples = df_full.sample(n=SAMPLE_SIZE, random_state=RANDOM_STATE).reset_index(drop=True)\n\nprint(f'Sampled: {len(df_samples):,} samples')\nprint(f'\\nDescription statistics:')\ndf_samples['desc_length'] = df_samples['desc'].str.len()\nprint(f'  Min length: {df_samples[\"desc_length\"].min()}')\nprint(f'  Mean length: {df_samples[\"desc_length\"].mean():.1f}')\nprint(f'  Max length: {df_samples[\"desc_length\"].max()}')\n\n# Save full sampled data (including desc) for 05g to use\nSAMPLES_WITH_DESC_FILE = '../samples_2000_with_desc.csv'\ndf_samples_to_save = df_samples[['desc']].copy()\ndf_samples_to_save.insert(0, 'sample_id', range(len(df_samples_to_save)))\ndf_samples_to_save.to_csv(SAMPLES_WITH_DESC_FILE, index=False)\nprint(f'\\nSaved samples with desc to {SAMPLES_WITH_DESC_FILE}')\n\n# Save metadata\nmetadata_cols = ['desc_length']\nif 'loan_amnt' in df_samples.columns:\n    metadata_cols.append('loan_amnt')\nif 'grade' in df_samples.columns:\n    metadata_cols.append('grade')\n\ndf_metadata = df_samples[metadata_cols].copy()\ndf_metadata.insert(0, 'sample_id', range(len(df_metadata)))\ndf_metadata.to_csv(OUTPUT_METADATA, index=False)\n\nprint(f'Saved metadata to {OUTPUT_METADATA}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCEAN_PROMPT_TEMPLATE = '''You are a psychologist specialized in the Big Five (OCEAN) personality assessment for credit behavior research.\n",
    "\n",
    "Analyze the loan applicant's text and provide personality scores for each of the Big Five traits. Base your assessment on ANY available linguistic cues, writing style, word choice, and expressed intentions in the text.\n",
    "\n",
    "Trait definitions and scoring guidelines:\n",
    "- Openness (0.0-1.0): curiosity, imagination, preference for novelty and new ideas\n",
    "  * High (0.7-1.0): words like \"learn,\" \"try new,\" \"explore,\" \"creative,\" \"open-minded,\" \"different,\" \"unique\"\n",
    "  * Medium (0.4-0.6): neutral or mixed signals\n",
    "  * Low (0.0-0.3): focus on routine, traditional, familiar, conservative language\n",
    "  \n",
    "- Conscientiousness (0.0-1.0): organization, discipline, reliability, planning, self-control\n",
    "  * High (0.7-1.0): \"planning,\" \"saving,\" \"on time,\" \"responsibility,\" \"organized,\" \"careful\"\n",
    "  * Medium (0.4-0.6): neutral or mixed signals\n",
    "  * Low (0.0-0.3): impulsive, unplanned, casual language\n",
    "  \n",
    "- Extraversion (0.0-1.0): sociability, assertiveness, energy, enthusiasm\n",
    "  * High (0.7-1.0): \"team,\" \"connect,\" \"talk,\" \"outgoing,\" \"social,\" \"people,\" \"friends\"\n",
    "  * Medium (0.4-0.6): neutral or mixed signals\n",
    "  * Low (0.0-0.3): solitary, quiet, reserved language\n",
    "  \n",
    "- Agreeableness (0.0-1.0): cooperation, empathy, kindness, trust\n",
    "  * High (0.7-1.0): \"help,\" \"care,\" \"family,\" \"support,\" \"honest,\" \"kind,\" \"together\"\n",
    "  * Medium (0.4-0.6): neutral or mixed signals\n",
    "  * Low (0.0-0.3): competitive, critical, confrontational language\n",
    "  \n",
    "- Neuroticism (0.0-1.0): emotional instability, anxiety, sensitivity to stress\n",
    "  * High (0.7-1.0): \"worry,\" \"stress,\" \"pressure,\" \"concern,\" \"can't sleep,\" \"anxious,\" \"difficult\"\n",
    "  * Medium (0.4-0.6): neutral or mixed signals\n",
    "  * Low (0.0-0.3): calm, stable, confident language\n",
    "\n",
    "IMPORTANT: You MUST provide a score between 0.0 and 1.0 for each trait based on the available text. Do NOT default to 0.5 unless you genuinely find perfectly neutral/balanced evidence. Use the full range of scores (0.0-1.0) to reflect varying degrees of each trait.\n",
    "\n",
    "Loan description:\n",
    "{description_text}\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{{\n",
    "  \"openness\": 0.X,\n",
    "  \"conscientiousness\": 0.X,\n",
    "  \"extraversion\": 0.X,\n",
    "  \"agreeableness\": 0.X,\n",
    "  \"neuroticism\": 0.X\n",
    "}}'''\n",
    "\n",
    "print('OCEAN prompt template defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define API Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_for_ocean_scores(description_text, model_name, provider, api_token, max_retries=3):\n",
    "    \"\"\"\n",
    "    Call HuggingFace Router API to generate OCEAN scores.\n",
    "    \n",
    "    Returns:\n",
    "        dict: OCEAN scores or None if failed\n",
    "    \"\"\"\n",
    "    prompt = OCEAN_PROMPT_TEMPLATE.format(description_text=description_text)\n",
    "    \n",
    "    api_url = 'https://router.huggingface.co/v1/chat/completions'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        'messages': [{'role': 'user', 'content': prompt}],\n",
    "        'model': f'{model_name}:{provider}',\n",
    "        'stream': False,\n",
    "        'max_tokens': 200,\n",
    "        'temperature': 0.7\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                if 'choices' in result and len(result['choices']) > 0:\n",
    "                    text_output = result['choices'][0].get('message', {}).get('content', '')\n",
    "                    \n",
    "                    try:\n",
    "                        # Extract JSON from response\n",
    "                        json_start = text_output.find('{')\n",
    "                        if json_start != -1:\n",
    "                            json_string = text_output[json_start:]\n",
    "                            json_end = json_string.find('}') + 1\n",
    "                            json_string = json_string[:json_end]\n",
    "                            score_dict = json.loads(json_string)\n",
    "                            \n",
    "                            # Validate all OCEAN dimensions present\n",
    "                            return_value = {}\n",
    "                            for key in ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:\n",
    "                                if key in score_dict:\n",
    "                                    return_value[key] = float(score_dict[key])\n",
    "                            \n",
    "                            if len(return_value) == 5:\n",
    "                                return return_value\n",
    "                    except Exception as parse_error:\n",
    "                        pass\n",
    "                        \n",
    "            elif response.status_code == 429 and attempt < max_retries - 1:\n",
    "                # Rate limit, wait and retry\n",
    "                time.sleep(2 * (attempt + 1))\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return None\n",
    "\n",
    "print('API function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading data from {DATA_FILE}...')\n",
    "df_full = pd.read_csv(DATA_FILE)\n",
    "\n",
    "print(f'Total samples in dataset: {len(df_full):,}')\n",
    "print(f'Columns: {df_full.shape[1]}')\n",
    "\n",
    "# Check for required column\n",
    "if 'desc' not in df_full.columns:\n",
    "    raise ValueError('Column \"desc\" not found in dataset!')\n",
    "\n",
    "print('Column \"desc\" found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 2000 rows randomly\n",
    "print(f'\\nSampling {SAMPLE_SIZE:,} samples (random seed: {RANDOM_STATE})...')\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "df_samples = df_full.sample(n=SAMPLE_SIZE, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "print(f'Sampled: {len(df_samples):,} samples')\n",
    "print(f'\\nDescription statistics:')\n",
    "df_samples['desc_length'] = df_samples['desc'].str.len()\n",
    "print(f'  Min length: {df_samples[\"desc_length\"].min()}')\n",
    "print(f'  Mean length: {df_samples[\"desc_length\"].mean():.1f}')\n",
    "print(f'  Max length: {df_samples[\"desc_length\"].max()}')\n",
    "\n",
    "# Save metadata\n",
    "metadata_cols = ['desc_length']\n",
    "if 'loan_amnt' in df_samples.columns:\n",
    "    metadata_cols.append('loan_amnt')\n",
    "if 'grade' in df_samples.columns:\n",
    "    metadata_cols.append('grade')\n",
    "\n",
    "df_metadata = df_samples[metadata_cols].copy()\n",
    "df_metadata.insert(0, 'sample_id', range(len(df_metadata)))\n",
    "df_metadata.to_csv(OUTPUT_METADATA, index=False)\n",
    "\n",
    "print(f'\\nSaved metadata to {OUTPUT_METADATA}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Checkpoint (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    \n",
    "    print(f'Checkpoint loaded: {checkpoint[\"processed_count\"]}/{checkpoint[\"total_count\"]}')\n",
    "    ocean_scores = checkpoint['ocean_scores']\n",
    "    start_idx = checkpoint['processed_count']\n",
    "    success_count = checkpoint['success_count']\n",
    "    failure_count = checkpoint['failure_count']\n",
    "    \n",
    "else:\n",
    "    print('No checkpoint found, starting from scratch')\n",
    "    ocean_scores = []\n",
    "    start_idx = 0\n",
    "    success_count = 0\n",
    "    failure_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate OCEAN Scores\n",
    "\n",
    "**This will take approximately 76 minutes (19 min per 500 samples \u00d7 4)**\n",
    "\n",
    "Progress is saved every 50 samples, so you can resume if interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print(f'Processing {DISPLAY_NAME} for {SAMPLE_SIZE:,} samples')\n",
    "print('=' * 80)\n",
    "print(f'Total samples: {len(df_samples):,}')\n",
    "print(f'Starting from: {start_idx}')\n",
    "print(f'Estimated time: ~{(SAMPLE_SIZE - start_idx) / 500 * 70:.1f} minutes')\n",
    "print('=' * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for idx in range(start_idx, len(df_samples)):\n",
    "    row = df_samples.iloc[idx]\n",
    "    description = row.get('desc', '')\n",
    "    \n",
    "    # Skip very short descriptions\n",
    "    if len(description) < 10:\n",
    "        ocean_scores.append(None)\n",
    "        failure_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Call LLM\n",
    "    ocean_score = call_llm_for_ocean_scores(\n",
    "        description, \n",
    "        MODEL_NAME, \n",
    "        PROVIDER, \n",
    "        HF_TOKEN, \n",
    "        max_retries=3\n",
    "    )\n",
    "    \n",
    "    if ocean_score:\n",
    "        ocean_scores.append(ocean_score)\n",
    "        success_count += 1\n",
    "    else:\n",
    "        ocean_scores.append(None)\n",
    "        failure_count += 1\n",
    "    \n",
    "    # Progress reporting and checkpointing every 50 samples\n",
    "    if (idx + 1) % 50 == 0 or (idx + 1) == len(df_samples):\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (idx + 1 - start_idx) / elapsed if elapsed > 0 else 0\n",
    "        eta = (len(df_samples) - (idx + 1)) / rate / 60 if rate > 0 else 0\n",
    "        \n",
    "        print(f'{idx + 1}/{len(df_samples)} ({(idx+1)/len(df_samples)*100:.1f}%) | '\n",
    "              f'Success: {success_count} ({success_count/(idx+1)*100:.1f}%) | '\n",
    "              f'Failed: {failure_count} | '\n",
    "              f'Rate: {rate:.2f} samples/sec | '\n",
    "              f'ETA: {eta:.1f} min')\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'model_name': MODEL_NAME,\n",
    "            'provider': PROVIDER,\n",
    "            'display_name': DISPLAY_NAME,\n",
    "            'total_count': len(df_samples),\n",
    "            'processed_count': idx + 1,\n",
    "            'success_count': success_count,\n",
    "            'failure_count': failure_count,\n",
    "            'ocean_scores': ocean_scores,\n",
    "            'last_update': datetime.now().isoformat()\n",
    "        }\n",
    "        with open(CHECKPOINT_FILE, 'w') as f:\n",
    "            json.dump(checkpoint, f, indent=2)\n",
    "    \n",
    "    # Rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nCOMPLETE: {total_time/60:.1f} minutes')\n",
    "print(f'Success: {success_count}/{len(df_samples)} ({success_count/len(df_samples)*100:.1f}%)')\n",
    "print(f'Failed: {failure_count}/{len(df_samples)} ({failure_count/len(df_samples)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with OCEAN scores\n",
    "data_list = []\n",
    "for idx, score in enumerate(ocean_scores):\n",
    "    if score:\n",
    "        data_list.append({'sample_id': idx, **score})\n",
    "    else:\n",
    "        data_list.append({\n",
    "            'sample_id': idx,\n",
    "            'openness': None,\n",
    "            'conscientiousness': None,\n",
    "            'extraversion': None,\n",
    "            'agreeableness': None,\n",
    "            'neuroticism': None\n",
    "        })\n",
    "\n",
    "df_ocean = pd.DataFrame(data_list)\n",
    "\n",
    "# Save OCEAN targets\n",
    "df_ocean.to_csv(OUTPUT_OCEAN, index=False)\n",
    "print(f'Results saved: {OUTPUT_OCEAN}')\n",
    "print(f'  Total rows: {len(df_ocean)}')\n",
    "print(f'  Valid rows: {df_ocean[\"openness\"].notna().sum()}')\n",
    "\n",
    "# Clean up checkpoint\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "    print(f'\\nCheckpoint file removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Display Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('OCEAN GROUND TRUTH STATISTICS (2,000 samples)')\n",
    "print('=' * 80)\n",
    "\n",
    "ocean_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "print(df_ocean[ocean_cols].describe())\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'Model: {DISPLAY_NAME}')\n",
    "print(f'Total samples: {len(df_ocean):,}')\n",
    "print(f'Valid samples: {df_ocean[\"openness\"].notna().sum():,} ({df_ocean[\"openness\"].notna().sum()/len(df_ocean)*100:.1f}%)')\n",
    "print(f'Processing time: {total_time/60:.1f} minutes')\n",
    "print(f'Rate: {len(df_ocean)/(total_time/60):.1f} samples/minute')\n",
    "print('=' * 80)\n",
    "\n",
    "print('\\nALL DONE! Ready for 05g model training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Verify the generated OCEAN scores look reasonable\n",
    "2. Use `ocean_targets_2000.csv` in the 05g training notebook\n",
    "3. Extract BGE embeddings for these 2,000 samples\n",
    "4. Train ElasticNet, Random Forest, and Gradient Boosting models\n",
    "5. Compare performance with 500-sample baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}