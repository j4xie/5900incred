{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phase 05g: Quick Validation with 2,000 Ground Truth Samples\n\n## Purpose\nValidate if increasing sample size from 500 to 2,000 improves model performance:\n- **Current (500 samples)**: ElasticNet R² = 0.127, Random Forest R² = 0.103\n- **Expected (2000 samples)**: ElasticNet R² = 0.15-0.18, Random Forest R² = 0.18-0.22\n\n## Key Questions\n1.  Does more data significantly improve R²?\n2.  Does Random Forest outperform ElasticNet with more data?\n3.  Is overfitting gap reduced?\n4.  Should we continue to 10k or full 34k samples?\n\n## Data Source\n- **OCEAN Ground Truth**: Generated from 05d using Gemma-2-9B\n- **Samples**: 2,000 samples with high-quality OCEAN labels\n- **Embeddings**: BGE-Large (1024 dimensions) to be extracted\n- **Dimension ratio**: 1:5 (much better than 1:2 with 500 samples)\n\n## Expected Training Time\n- BGE embedding extraction: ~15-20 minutes (HF API)\n- ElasticNet training: ~30 seconds\n- Random Forest training: ~5 minutes\n- Gradient Boosting training: ~4 minutes\n- **Total**: ~25-30 minutes\n\n## Prerequisites\nYou must run 05d_generate_2k_ocean_ground_truth.ipynb first to generate:\n- ocean_targets_2000.csv\n- samples_2000_with_desc.csv\n- samples_2000_metadata.csv"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# HuggingFace API\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file!\")\n",
    "\n",
    "# Initialize HF client\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(f\" Setup complete\")\n",
    "print(f\"   HF API Token: {HF_TOKEN[:10]}...{HF_TOKEN[-5:]}\")\n",
    "print(f\"   Random seed: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data configuration - Using 05d generated ground truth\nOCEAN_GROUND_TRUTH_FILE = '../ocean_targets_2000.csv'\nSAMPLES_WITH_DESC_FILE = '../samples_2000_with_desc.csv'\nSAMPLES_METADATA_FILE = '../samples_2000_metadata.csv'\n\n# BGE embedding configuration\nBGE_MODEL = 'BAAI/bge-large-en-v1.5'\nEMBEDDING_DIM = 1024\n\n# Training configuration\nSAMPLE_SIZE = 2000\nTEST_SIZE = 0.2\n\n# OCEAN dimensions\nOCEAN_DIMS = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n\n# Output files\nEMBEDDING_FILE = '../bge_embeddings_2k_ground_truth.npy'\nREPORT_FILE = '../05g_validation_2k_report.json'\nCOMPARISON_CSV = '../05g_500_vs_2000_comparison.csv'\nPLOT_FILE = '../05g_500_vs_2000_comparison.png'\n\nprint(f\" Configuration:\")\nprint(f\"   OCEAN ground truth: {OCEAN_GROUND_TRUTH_FILE}\")\nprint(f\"   Samples file: {SAMPLES_WITH_DESC_FILE}\")\nprint(f\"   Sample size: {SAMPLE_SIZE:,}\")\nprint(f\"   Train/Test split: {int((1-TEST_SIZE)*100)}/{int(TEST_SIZE*100)}\")\nprint(f\"   BGE model: {BGE_MODEL}\")\nprint(f\"   Embedding dim: {EMBEDDING_DIM}\")\nprint(f\"   Dimension ratio: {SAMPLE_SIZE}:{EMBEDDING_DIM} ≈ {SAMPLE_SIZE/EMBEDDING_DIM:.1f}:1\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\" Loading OCEAN ground truth from 05d...\")\n\n# Load OCEAN ground truth\nif not os.path.exists(OCEAN_GROUND_TRUTH_FILE):\n    raise FileNotFoundError(\n        f\"OCEAN ground truth file not found: {OCEAN_GROUND_TRUTH_FILE}\\n\"\n        f\"Please run 05d_generate_2k_ocean_ground_truth.ipynb first!\"\n    )\n\ndf_ocean = pd.read_csv(OCEAN_GROUND_TRUTH_FILE)\nprint(f\"   Loaded OCEAN ground truth: {len(df_ocean):,} samples\")\n\n# Load sample descriptions\nif not os.path.exists(SAMPLES_WITH_DESC_FILE):\n    raise FileNotFoundError(\n        f\"Samples file not found: {SAMPLES_WITH_DESC_FILE}\\n\"\n        f\"Please run 05d_generate_2k_ocean_ground_truth.ipynb first!\"\n    )\n\ndf_samples = pd.read_csv(SAMPLES_WITH_DESC_FILE)\nprint(f\"   Loaded sample descriptions: {len(df_samples):,} samples\")\n\n# Verify data consistency\nassert len(df_ocean) == len(df_samples), \"Mismatch between OCEAN targets and samples!\"\nassert len(df_ocean) == SAMPLE_SIZE, f\"Expected {SAMPLE_SIZE} samples, got {len(df_ocean)}\"\n\nprint(f\"\\n OCEAN statistics (ground truth):\")\nprint(df_ocean[OCEAN_DIMS].describe())\n\n# Check description lengths\ndf_samples['desc_length'] = df_samples['desc'].str.len()\nprint(f\"\\n Description length statistics:\")\nprint(f\"   Min: {df_samples['desc_length'].min()}\")\nprint(f\"   Mean: {df_samples['desc_length'].mean():.1f}\")\nprint(f\"   Max: {df_samples['desc_length'].max()}\")\n\nprint(f\"\\n Data loading complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract BGE Embeddings\n",
    "\n",
    "**Note**: This will take ~15-20 minutes using HuggingFace API.\n",
    "- Processing ~2,000 descriptions\n",
    "- Using BAAI/bge-large-en-v1.5 (1024 dimensions)\n",
    "- Progress bar will show estimated time remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bge_embedding(text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Extract BGE embedding for a single text using HuggingFace API.\n",
    "    Includes retry logic for API failures.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = client.feature_extraction(\n",
    "                text=text,\n",
    "                model=BGE_MODEL\n",
    "            )\n",
    "            \n",
    "            # Convert to numpy array and compute mean pooling\n",
    "            embedding = np.array(result)\n",
    "            if embedding.ndim == 2:  # Token-level embeddings\n",
    "                mean_embedding = embedding.mean(axis=0)\n",
    "            else:\n",
    "                mean_embedding = embedding\n",
    "            \n",
    "            # Verify dimension\n",
    "            if len(mean_embedding) == EMBEDDING_DIM:\n",
    "                return mean_embedding\n",
    "            else:\n",
    "                print(f\"     Unexpected dimension: {len(mean_embedding)}, expected {EMBEDDING_DIM}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"    Failed after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(f\" Extracting BGE embeddings for {len(df_sample):,} samples...\")\n",
    "print(f\"   Model: {BGE_MODEL}\")\n",
    "print(f\"   Expected time: ~15-20 minutes\")\n",
    "print(f\"   Progress will be saved every 100 samples\\n\")\n",
    "\n",
    "embeddings = []\n",
    "failed_indices = []\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, desc in enumerate(tqdm(df_sample['desc'], desc=\"Extracting embeddings\")):\n",
    "    embedding = extract_bge_embedding(desc)\n",
    "    \n",
    "    if embedding is not None:\n",
    "        embeddings.append(embedding)\n",
    "    else:\n",
    "        embeddings.append(np.zeros(EMBEDDING_DIM))  # Fallback for failed extractions\n",
    "        failed_indices.append(idx)\n",
    "    \n",
    "    # Save intermediate results every 100 samples\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        temp_embeddings = np.array(embeddings)\n",
    "        np.save(EMBEDDING_FILE + '.temp', temp_embeddings)\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (idx + 1) / elapsed\n",
    "        remaining = (len(df_sample) - idx - 1) / rate\n",
    "        print(f\"   Progress: {idx+1}/{len(df_sample)} ({(idx+1)/len(df_sample)*100:.1f}%) | \"\n",
    "              f\"Rate: {rate:.1f} samples/sec | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n Embedding extraction complete!\")\n",
    "print(f\"   Shape: {embeddings.shape}\")\n",
    "print(f\"   Time: {elapsed_time/60:.1f} minutes\")\n",
    "print(f\"   Rate: {len(df_sample)/elapsed_time:.2f} samples/sec\")\n",
    "print(f\"   Failed: {len(failed_indices)} samples\")\n",
    "\n",
    "if failed_indices:\n",
    "    print(f\"     Failed indices: {failed_indices[:10]}...\" if len(failed_indices) > 10 else f\"     Failed indices: {failed_indices}\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save(EMBEDDING_FILE, embeddings)\n",
    "print(f\"\\n Saved embeddings to {EMBEDDING_FILE}\")\n",
    "\n",
    "# Clean up temp file\n",
    "import os\n",
    "if os.path.exists(EMBEDDING_FILE + '.temp'):\n",
    "    os.remove(EMBEDDING_FILE + '.temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Preparing training data...\")\n",
    "\n",
    "# Load embeddings\n",
    "X_full = np.load(EMBEDDING_FILE)\n",
    "print(f\"   Embeddings shape: {X_full.shape}\")\n",
    "\n",
    "# Verify dimension\n",
    "assert X_full.shape[1] == EMBEDDING_DIM, f\"Expected {EMBEDDING_DIM} dimensions, got {X_full.shape[1]}\"\n",
    "\n",
    "# Prepare OCEAN targets\n",
    "y_full = df_sample[OCEAN_DIMS].values\n",
    "print(f\"   Targets shape: {y_full.shape}\")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\n Data split complete:\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Test samples: {len(X_test):,}\")\n",
    "print(f\"   Feature dimensions: {X_train.shape[1]:,}\")\n",
    "print(f\"   Dimension ratio (train): {len(X_train)}:{X_train.shape[1]} ≈ {len(X_train)/X_train.shape[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "print(f\" Preparing training data...\")\n\n# Load embeddings\nX_full = np.load(EMBEDDING_FILE)\nprint(f\"   Embeddings shape: {X_full.shape}\")\n\n# Verify dimension\nassert X_full.shape[1] == EMBEDDING_DIM, f\"Expected {EMBEDDING_DIM} dimensions, got {X_full.shape[1]}\"\n\n# Prepare OCEAN targets from ground truth\ny_full = df_ocean[OCEAN_DIMS].values\nprint(f\"   Targets shape: {y_full.shape}\")\n\n# Check for any missing values\nmissing_count = np.isnan(y_full).sum()\nif missing_count > 0:\n    print(f\"   WARNING: {missing_count} missing OCEAN values detected\")\n    print(f\"   Filling missing values with column means...\")\n    for i, dim in enumerate(OCEAN_DIMS):\n        col_mean = np.nanmean(y_full[:, i])\n        y_full[np.isnan(y_full[:, i]), i] = col_mean\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_full, y_full, \n    test_size=TEST_SIZE, \n    random_state=RANDOM_STATE\n)\n\nprint(f\"\\n Data split complete:\")\nprint(f\"   Training samples: {len(X_train):,}\")\nprint(f\"   Test samples: {len(X_test):,}\")\nprint(f\"   Feature dimensions: {X_train.shape[1]:,}\")\nprint(f\"   Dimension ratio (train): {len(X_train)}:{X_train.shape[1]} ≈ {len(X_train)/X_train.shape[1]:.1f}:1\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Training ElasticNet models...\\n\")\n",
    "\n",
    "elasticnet_results = {}\n",
    "elasticnet_models = {}\n",
    "\n",
    "# ElasticNet hyperparameters\n",
    "ALPHAS = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "L1_RATIOS = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, dim in enumerate(OCEAN_DIMS):\n",
    "    print(f\"Training {dim}...\", end=\" \")\n",
    "    \n",
    "    # Train ElasticNet with cross-validation\n",
    "    model = ElasticNetCV(\n",
    "        alphas=ALPHAS,\n",
    "        l1_ratio=L1_RATIOS,\n",
    "        cv=5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=10000,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train[:, i])\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train[:, i], y_train_pred)\n",
    "    test_r2 = r2_score(y_test[:, i], y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train[:, i], y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test[:, i], y_test_pred))\n",
    "    \n",
    "    # Sparsity\n",
    "    non_zero = np.sum(model.coef_ != 0)\n",
    "    sparsity = (1 - non_zero / len(model.coef_)) * 100\n",
    "    \n",
    "    elasticnet_results[dim] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'overfitting_gap': train_r2 - test_r2,\n",
    "        'best_alpha': model.alpha_,\n",
    "        'best_l1_ratio': model.l1_ratio_,\n",
    "        'non_zero_features': int(non_zero),\n",
    "        'sparsity_percent': sparsity\n",
    "    }\n",
    "    \n",
    "    elasticnet_models[dim] = model\n",
    "    \n",
    "    print(f\"R²={test_r2:.3f}, Gap={train_r2-test_r2:.3f}, Sparsity={sparsity:.1f}%\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n ElasticNet training complete ({elapsed:.1f} seconds)\")\n",
    "\n",
    "# Average metrics\n",
    "avg_test_r2 = np.mean([r['test_r2'] for r in elasticnet_results.values()])\n",
    "avg_gap = np.mean([r['overfitting_gap'] for r in elasticnet_results.values()])\n",
    "print(f\"   Average Test R²: {avg_test_r2:.3f}\")\n",
    "print(f\"   Average Overfitting Gap: {avg_gap:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Random Forest (Optimized for 2k samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Training Random Forest models...\\n\")\n",
    "\n",
    "randomforest_results = {}\n",
    "randomforest_models = {}\n",
    "\n",
    "# Random Forest hyperparameters (optimized for 2k samples with 1024 features)\n",
    "RF_PARAM_GRID = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'min_samples_split': [20, 30],\n",
    "    'min_samples_leaf': [10, 15],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(f\"Hyperparameter grid: {sum([len(v) for v in RF_PARAM_GRID.values()])} combinations per dimension\")\n",
    "print(f\"Expected time: ~5 minutes\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, dim in enumerate(OCEAN_DIMS):\n",
    "    dim_start = time.time()\n",
    "    print(f\"Training {dim}...\", end=\" \")\n",
    "    \n",
    "    # GridSearch with cross-validation\n",
    "    rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    grid_search = GridSearchCV(\n",
    "        rf,\n",
    "        RF_PARAM_GRID,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train[:, i])\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train[:, i], y_train_pred)\n",
    "    test_r2 = r2_score(y_test[:, i], y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train[:, i], y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test[:, i], y_test_pred))\n",
    "    \n",
    "    randomforest_results[dim] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'overfitting_gap': train_r2 - test_r2,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_best_score': grid_search.best_score_,\n",
    "        'training_time': time.time() - dim_start\n",
    "    }\n",
    "    \n",
    "    randomforest_models[dim] = best_model\n",
    "    \n",
    "    print(f\"R²={test_r2:.3f}, Gap={train_r2-test_r2:.3f} ({time.time()-dim_start:.1f}s)\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n Random Forest training complete ({elapsed/60:.1f} minutes)\")\n",
    "\n",
    "# Average metrics\n",
    "avg_test_r2 = np.mean([r['test_r2'] for r in randomforest_results.values()])\n",
    "avg_gap = np.mean([r['overfitting_gap'] for r in randomforest_results.values()])\n",
    "print(f\"   Average Test R²: {avg_test_r2:.3f}\")\n",
    "print(f\"   Average Overfitting Gap: {avg_gap:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Training Gradient Boosting models...\\n\")\n",
    "\n",
    "gb_results = {}\n",
    "gb_models = {}\n",
    "\n",
    "# Gradient Boosting hyperparameters\n",
    "GB_PARAM_GRID = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'min_samples_leaf': [10, 20]\n",
    "}\n",
    "\n",
    "print(f\"Hyperparameter grid: {sum([len(v) for v in GB_PARAM_GRID.values()])} combinations per dimension\")\n",
    "print(f\"Expected time: ~4 minutes\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, dim in enumerate(OCEAN_DIMS):\n",
    "    dim_start = time.time()\n",
    "    print(f\"Training {dim}...\", end=\" \")\n",
    "    \n",
    "    # GridSearch with cross-validation\n",
    "    gb = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "    grid_search = GridSearchCV(\n",
    "        gb,\n",
    "        GB_PARAM_GRID,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train[:, i])\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train[:, i], y_train_pred)\n",
    "    test_r2 = r2_score(y_test[:, i], y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train[:, i], y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test[:, i], y_test_pred))\n",
    "    \n",
    "    gb_results[dim] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'overfitting_gap': train_r2 - test_r2,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_best_score': grid_search.best_score_,\n",
    "        'training_time': time.time() - dim_start\n",
    "    }\n",
    "    \n",
    "    gb_models[dim] = best_model\n",
    "    \n",
    "    print(f\"R²={test_r2:.3f}, Gap={train_r2-test_r2:.3f} ({time.time()-dim_start:.1f}s)\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n Gradient Boosting training complete ({elapsed/60:.1f} minutes)\")\n",
    "\n",
    "# Average metrics\n",
    "avg_test_r2 = np.mean([r['test_r2'] for r in gb_results.values()])\n",
    "avg_gap = np.mean([r['overfitting_gap'] for r in gb_results.values()])\n",
    "print(f\"   Average Test R²: {avg_test_r2:.3f}\")\n",
    "print(f\"   Average Overfitting Gap: {avg_gap:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare with 500-Sample Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Comparing 500-sample vs 2000-sample results...\\n\")\n",
    "\n",
    "# Baseline results from 500-sample experiments\n",
    "baseline_500 = {\n",
    "    'ElasticNet': {\n",
    "        'avg_test_r2': 0.127,\n",
    "        'avg_overfitting_gap': 0.17\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'avg_test_r2': 0.103,\n",
    "        'avg_overfitting_gap': 0.45\n",
    "    }\n",
    "}\n",
    "\n",
    "# Current 2000-sample results\n",
    "results_2k = {\n",
    "    'ElasticNet': {\n",
    "        'avg_test_r2': np.mean([r['test_r2'] for r in elasticnet_results.values()]),\n",
    "        'avg_overfitting_gap': np.mean([r['overfitting_gap'] for r in elasticnet_results.values()])\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'avg_test_r2': np.mean([r['test_r2'] for r in randomforest_results.values()]),\n",
    "        'avg_overfitting_gap': np.mean([r['overfitting_gap'] for r in randomforest_results.values()])\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'avg_test_r2': np.mean([r['test_r2'] for r in gb_results.values()]),\n",
    "        'avg_overfitting_gap': np.mean([r['overfitting_gap'] for r in gb_results.values()])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"═\" * 80)\n",
    "print(\"                  500 SAMPLES vs 2000 SAMPLES COMPARISON\")\n",
    "print(\"═\" * 80)\n",
    "print(f\"{'Model':<20} {'500-R²':<10} {'2k-R²':<10} {'Δ R²':<10} {'Gap-500':<10} {'Gap-2k':<10} {'Δ Gap':<10}\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "for model in ['ElasticNet', 'RandomForest']:\n",
    "    r2_500 = baseline_500[model]['avg_test_r2']\n",
    "    r2_2k = results_2k[model]['avg_test_r2']\n",
    "    gap_500 = baseline_500[model]['avg_overfitting_gap']\n",
    "    gap_2k = results_2k[model]['avg_overfitting_gap']\n",
    "    \n",
    "    r2_delta = r2_2k - r2_500\n",
    "    gap_delta = gap_2k - gap_500\n",
    "    \n",
    "    print(f\"{model:<20} {r2_500:<10.3f} {r2_2k:<10.3f} {r2_delta:+10.3f} {gap_500:<10.3f} {gap_2k:<10.3f} {gap_delta:+10.3f}\")\n",
    "\n",
    "# New model (no baseline)\n",
    "print(f\"{'GradientBoosting':<20} {'N/A':<10} {results_2k['GradientBoosting']['avg_test_r2']:<10.3f} {'NEW':<10} \"\n",
    "      f\"{'N/A':<10} {results_2k['GradientBoosting']['avg_overfitting_gap']:<10.3f} {'NEW':<10}\")\n",
    "\n",
    "print(\"═\" * 80)\n",
    "\n",
    "# Determine best model for 2k samples\n",
    "best_model_2k = max(results_2k.items(), key=lambda x: x[1]['avg_test_r2'])\n",
    "print(f\"\\n Best model (2k samples): {best_model_2k[0]} with R² = {best_model_2k[1]['avg_test_r2']:.3f}\")\n",
    "\n",
    "# Calculate improvement magnitude\n",
    "elasticnet_improvement = results_2k['ElasticNet']['avg_test_r2'] - baseline_500['ElasticNet']['avg_test_r2']\n",
    "rf_improvement = results_2k['RandomForest']['avg_test_r2'] - baseline_500['RandomForest']['avg_test_r2']\n",
    "\n",
    "print(f\"\\n R² Improvements:\")\n",
    "print(f\"   ElasticNet: {elasticnet_improvement:+.3f} ({elasticnet_improvement/baseline_500['ElasticNet']['avg_test_r2']*100:+.1f}%)\")\n",
    "print(f\"   Random Forest: {rf_improvement:+.3f} ({rf_improvement/baseline_500['RandomForest']['avg_test_r2']*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Decision & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"═\" * 80)\n",
    "print(\"                         DECISION & RECOMMENDATIONS\")\n",
    "print(\"═\" * 80 + \"\\n\")\n",
    "\n",
    "# Determine if improvement is significant\n",
    "max_improvement = max(elasticnet_improvement, rf_improvement)\n",
    "\n",
    "if max_improvement > 0.05:\n",
    "    verdict = \" SIGNIFICANT IMPROVEMENT\"\n",
    "    recommendation = \"Continue to 10,000 samples\"\n",
    "    explanation = (\n",
    "        f\"Increasing from 500 to 2,000 samples improved R² by {max_improvement:.3f} ({max_improvement*100:.1f}%). \"\n",
    "        \"This indicates that more data substantially helps model performance. \"\n",
    "        \"Scaling to 10,000 samples (1:20 feature ratio) should yield R² ≈ 0.22-0.26.\"\n",
    "    )\n",
    "    next_steps = [\n",
    "        \" Create 05h_train_10k_samples.ipynb\",\n",
    "        \" Focus on Random Forest or Gradient Boosting (non-linear models)\",\n",
    "        \" Consider using all 34,529 samples for final production model\",\n",
    "        \" Expected final R² with 34k samples: 0.28-0.35\"\n",
    "    ]\n",
    "    \n",
    "elif max_improvement > 0.02:\n",
    "    verdict = \"  MODERATE IMPROVEMENT\"\n",
    "    recommendation = \"Try 10,000 samples with feature engineering\"\n",
    "    explanation = (\n",
    "        f\"R² improved by {max_improvement:.3f} ({max_improvement*100:.1f}%), showing modest gains. \"\n",
    "        \"More data helps, but may not be sufficient alone. \"\n",
    "        \"Consider combining more samples with feature engineering.\"\n",
    "    )\n",
    "    next_steps = [\n",
    "        \"  Try 10,000 samples but don't expect dramatic improvement\",\n",
    "        \"  Add feature engineering:\",\n",
    "        \"   - TF-IDF features from descriptions\",\n",
    "        \"   - Loan metadata (amount, grade, purpose, etc.)\",\n",
    "        \"   - Ensemble of 5 LLM labels (not just Llama)\",\n",
    "        \"  Consider dimensionality reduction (PCA, feature selection)\"\n",
    "    ]\n",
    "    \n",
    "else:\n",
    "    verdict = \" MINIMAL IMPROVEMENT\"\n",
    "    recommendation = \"Sample size is NOT the bottleneck\"\n",
    "    explanation = (\n",
    "        f\"R² only improved by {max_improvement:.3f} ({max_improvement*100:.1f}%), indicating that \"\n",
    "        \"the problem is not lack of data, but rather: \"\n",
    "        \"(1) BGE embeddings don't capture personality well, or \"\n",
    "        \"(2) predicting OCEAN from loan descriptions is inherently difficult.\"\n",
    "    )\n",
    "    next_steps = [\n",
    "        \" Do NOT scale to 10k or 34k samples - it won't help significantly\",\n",
    "        \" Root causes to investigate:\",\n",
    "        \"   - Try different embeddings (sentence-transformers specialized for personality)\",\n",
    "        \"   - Add non-text features (loan amount, income, employment, etc.)\",\n",
    "        \"   - Ensemble 5 LLM labels instead of using just Llama\",\n",
    "        \"   - Consider this may be the performance ceiling for this task\",\n",
    "        \" Alternative: Multi-modal model (text + structured features)\"\n",
    "    ]\n",
    "\n",
    "print(f\"VERDICT: {verdict}\")\n",
    "print(f\"\\n Recommendation: {recommendation}\")\n",
    "print(f\"\\n Explanation:\\n{explanation}\")\n",
    "print(f\"\\n Next Steps:\")\n",
    "for step in next_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(\"\\n\" + \"═\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comprehensive report\n",
    "report = {\n",
    "    'phase': '05g - Quick Validation with 2,000 Samples',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'configuration': {\n",
    "        'sample_size': SAMPLE_SIZE,\n",
    "        'test_size': TEST_SIZE,\n",
    "        'llm_model': LLM_MODEL,\n",
    "        'embedding_model': BGE_MODEL,\n",
    "        'embedding_dimension': EMBEDDING_DIM,\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test)\n",
    "    },\n",
    "    'baseline_500_samples': baseline_500,\n",
    "    'results_2k_samples': {\n",
    "        'elasticnet': {dim: elasticnet_results[dim] for dim in OCEAN_DIMS},\n",
    "        'randomforest': {dim: randomforest_results[dim] for dim in OCEAN_DIMS},\n",
    "        'gradient_boosting': {dim: gb_results[dim] for dim in OCEAN_DIMS}\n",
    "    },\n",
    "    'summary': {\n",
    "        'elasticnet_avg_test_r2': results_2k['ElasticNet']['avg_test_r2'],\n",
    "        'randomforest_avg_test_r2': results_2k['RandomForest']['avg_test_r2'],\n",
    "        'gb_avg_test_r2': results_2k['GradientBoosting']['avg_test_r2'],\n",
    "        'elasticnet_improvement': elasticnet_improvement,\n",
    "        'rf_improvement': rf_improvement,\n",
    "        'best_model': best_model_2k[0],\n",
    "        'best_r2': best_model_2k[1]['avg_test_r2']\n",
    "    },\n",
    "    'decision': {\n",
    "        'verdict': verdict,\n",
    "        'recommendation': recommendation,\n",
    "        'explanation': explanation,\n",
    "        'next_steps': next_steps\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON report\n",
    "with open(REPORT_FILE, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\" Saved detailed report to {REPORT_FILE}\")\n",
    "\n",
    "# Create comparison CSV\n",
    "comparison_data = []\n",
    "for model_name in ['ElasticNet', 'RandomForest']:\n",
    "    for dim in OCEAN_DIMS:\n",
    "        if model_name == 'ElasticNet':\n",
    "            results_dict = elasticnet_results\n",
    "        else:\n",
    "            results_dict = randomforest_results\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Dimension': dim,\n",
    "            'Samples_500_R2': baseline_500[model_name]['avg_test_r2'],  # Using average for simplicity\n",
    "            'Samples_2000_R2': results_dict[dim]['test_r2'],\n",
    "            'R2_Improvement': results_dict[dim]['test_r2'] - baseline_500[model_name]['avg_test_r2'],\n",
    "            'Samples_500_Gap': baseline_500[model_name]['avg_overfitting_gap'],\n",
    "            'Samples_2000_Gap': results_dict[dim]['overfitting_gap'],\n",
    "            'Gap_Improvement': results_dict[dim]['overfitting_gap'] - baseline_500[model_name]['avg_overfitting_gap']\n",
    "        })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison.to_csv(COMPARISON_CSV, index=False)\n",
    "print(f\" Saved comparison CSV to {COMPARISON_CSV}\")\n",
    "\n",
    "print(f\"\\n Phase 05g complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Prepare comprehensive report\nreport = {\n    'phase': '05g - Validation with 2,000 Ground Truth Samples',\n    'timestamp': datetime.now().isoformat(),\n    'configuration': {\n        'sample_size': SAMPLE_SIZE,\n        'test_size': TEST_SIZE,\n        'ocean_source': '05d generated ground truth (Gemma-2-9B)',\n        'embedding_model': BGE_MODEL,\n        'embedding_dimension': EMBEDDING_DIM,\n        'training_samples': len(X_train),\n        'test_samples': len(X_test)\n    },\n    'baseline_500_samples': baseline_500,\n    'results_2k_samples': {\n        'elasticnet': {dim: elasticnet_results[dim] for dim in OCEAN_DIMS},\n        'randomforest': {dim: randomforest_results[dim] for dim in OCEAN_DIMS},\n        'gradient_boosting': {dim: gb_results[dim] for dim in OCEAN_DIMS}\n    },\n    'summary': {\n        'elasticnet_avg_test_r2': results_2k['ElasticNet']['avg_test_r2'],\n        'randomforest_avg_test_r2': results_2k['RandomForest']['avg_test_r2'],\n        'gb_avg_test_r2': results_2k['GradientBoosting']['avg_test_r2'],\n        'elasticnet_improvement': elasticnet_improvement,\n        'rf_improvement': rf_improvement,\n        'best_model': best_model_2k[0],\n        'best_r2': best_model_2k[1]['avg_test_r2']\n    },\n    'decision': {\n        'verdict': verdict,\n        'recommendation': recommendation,\n        'explanation': explanation,\n        'next_steps': next_steps\n    }\n}\n\n# Save JSON report\nwith open(REPORT_FILE, 'w') as f:\n    json.dump(report, f, indent=2)\n\nprint(f\" Saved detailed report to {REPORT_FILE}\")\n\n# Create comparison CSV\ncomparison_data = []\nfor model_name in ['ElasticNet', 'RandomForest']:\n    for dim in OCEAN_DIMS:\n        if model_name == 'ElasticNet':\n            results_dict = elasticnet_results\n        else:\n            results_dict = randomforest_results\n        \n        comparison_data.append({\n            'Model': model_name,\n            'Dimension': dim,\n            'Samples_500_R2': baseline_500[model_name]['avg_test_r2'],  # Using average for simplicity\n            'Samples_2000_R2': results_dict[dim]['test_r2'],\n            'R2_Improvement': results_dict[dim]['test_r2'] - baseline_500[model_name]['avg_test_r2'],\n            'Samples_500_Gap': baseline_500[model_name]['avg_overfitting_gap'],\n            'Samples_2000_Gap': results_dict[dim]['overfitting_gap'],\n            'Gap_Improvement': results_dict[dim]['overfitting_gap'] - baseline_500[model_name]['avg_overfitting_gap']\n        })\n\ndf_comparison = pd.DataFrame(comparison_data)\ndf_comparison.to_csv(COMPARISON_CSV, index=False)\nprint(f\" Saved comparison CSV to {COMPARISON_CSV}\")\n\nprint(f\"\\n Phase 05g complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('500 Samples vs 2,000 Samples Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: R² comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = ['ElasticNet', 'RandomForest', 'GradientBoosting']\n",
    "r2_500 = [baseline_500['ElasticNet']['avg_test_r2'], \n",
    "          baseline_500['RandomForest']['avg_test_r2'],\n",
    "          0]  # No baseline for GB\n",
    "r2_2k = [results_2k['ElasticNet']['avg_test_r2'],\n",
    "         results_2k['RandomForest']['avg_test_r2'],\n",
    "         results_2k['GradientBoosting']['avg_test_r2']]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, r2_500, width, label='500 samples', alpha=0.8)\n",
    "ax1.bar(x + width/2, r2_2k, width, label='2,000 samples', alpha=0.8)\n",
    "ax1.set_ylabel('Test R²', fontsize=12)\n",
    "ax1.set_title('Average Test R² Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Overfitting Gap comparison\n",
    "ax2 = axes[0, 1]\n",
    "gap_500 = [baseline_500['ElasticNet']['avg_overfitting_gap'],\n",
    "           baseline_500['RandomForest']['avg_overfitting_gap'],\n",
    "           0]\n",
    "gap_2k = [results_2k['ElasticNet']['avg_overfitting_gap'],\n",
    "          results_2k['RandomForest']['avg_overfitting_gap'],\n",
    "          results_2k['GradientBoosting']['avg_overfitting_gap']]\n",
    "\n",
    "ax2.bar(x - width/2, gap_500, width, label='500 samples', alpha=0.8, color='orange')\n",
    "ax2.bar(x + width/2, gap_2k, width, label='2,000 samples', alpha=0.8, color='green')\n",
    "ax2.set_ylabel('Overfitting Gap (Train R² - Test R²)', fontsize=12)\n",
    "ax2.set_title('Overfitting Gap Comparison (Lower is Better)', fontsize=13, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Per-dimension R² for best model\n",
    "ax3 = axes[1, 0]\n",
    "best_results = randomforest_results if results_2k['RandomForest']['avg_test_r2'] > results_2k['ElasticNet']['avg_test_r2'] else elasticnet_results\n",
    "best_name = 'Random Forest' if results_2k['RandomForest']['avg_test_r2'] > results_2k['ElasticNet']['avg_test_r2'] else 'ElasticNet'\n",
    "r2_per_dim = [best_results[dim]['test_r2'] for dim in OCEAN_DIMS]\n",
    "\n",
    "ax3.barh(OCEAN_DIMS, r2_per_dim, color='steelblue', alpha=0.8)\n",
    "ax3.set_xlabel('Test R²', fontsize=12)\n",
    "ax3.set_title(f'Per-Dimension Performance ({best_name}, 2k samples)', fontsize=13, fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add R² values on bars\n",
    "for i, v in enumerate(r2_per_dim):\n",
    "    ax3.text(v + 0.005, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# Plot 4: Improvement summary\n",
    "ax4 = axes[1, 1]\n",
    "improvements = [\n",
    "    ('ElasticNet\\nR² Δ', elasticnet_improvement),\n",
    "    ('RandomForest\\nR² Δ', rf_improvement),\n",
    "    ('ElasticNet\\nGap Δ', results_2k['ElasticNet']['avg_overfitting_gap'] - baseline_500['ElasticNet']['avg_overfitting_gap']),\n",
    "    ('RandomForest\\nGap Δ', results_2k['RandomForest']['avg_overfitting_gap'] - baseline_500['RandomForest']['avg_overfitting_gap'])\n",
    "]\n",
    "\n",
    "labels, values = zip(*improvements)\n",
    "colors = ['green' if v > 0 else 'red' for v in values[:2]] + ['red' if v < 0 else 'green' for v in values[2:]]\n",
    "ax4.bar(range(len(labels)), values, color=colors, alpha=0.7)\n",
    "ax4.set_xticks(range(len(labels)))\n",
    "ax4.set_xticklabels(labels, fontsize=10)\n",
    "ax4.set_ylabel('Change (500 → 2000 samples)', fontsize=12)\n",
    "ax4.set_title('Improvement Analysis', fontsize=13, fontweight='bold')\n",
    "ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(values):\n",
    "    ax4.text(i, v + 0.002 if v > 0 else v - 0.002, f'{v:+.3f}', \n",
    "             ha='center', va='bottom' if v > 0 else 'top', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_FILE, dpi=300, bbox_inches='tight')\n",
    "print(f\" Saved comparison plot to {PLOT_FILE}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}