{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05g - Sample 2K and Generate OCEAN Features for All 5 LLMs\n",
    "\n",
    "**Purpose**: Sample 2000 samples from 34K dataset, extract BGE embeddings, and generate OCEAN predictions using all 5 LLM-trained ElasticNet models\n",
    "\n",
    "## Workflow:\n",
    "1. Load 34K dataset\n",
    "2. Stratified sampling (2000 samples, balanced Fully Paid/Charged Off)\n",
    "3. Extract BGE embeddings (2000 samples, ~17 minutes)\n",
    "4. Load 5 ElasticNet models (Llama, GPT, Qwen, Gemma, DeepSeek)\n",
    "5. Predict OCEAN for each model (5 LLMs x 5 dimensions = 25 OCEAN columns)\n",
    "6. Save combined dataset\n",
    "\n",
    "## Input Files:\n",
    "- loan_final_desc50plus_with_ocean_bge.csv (34,530 rows)\n",
    "- elasticnet_models_llama.pkl\n",
    "- elasticnet_models_gpt.pkl\n",
    "- elasticnet_models_qwen.pkl\n",
    "- elasticnet_models_gemma.pkl\n",
    "- elasticnet_models_deepseek.pkl\n",
    "\n",
    "## Output Files:\n",
    "- loan_2k_with_all_ocean.csv (2000 rows x 62 columns)\n",
    "  - 36 base features\n",
    "  - 25 OCEAN features (5 LLMs x 5 dimensions)\n",
    "  - 1 target column\n",
    "- bge_embeddings_2k.npy (2000 x 1024)\n",
    "- 05g_generation_report.json\n",
    "\n",
    "**Estimated Time**: 20-25 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For BGE embeddings\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load environment variables from .env file\ndef load_env_file(filepath='../.env'):\n    \"\"\"Load environment variables from .env file\"\"\"\n    env_vars = {}\n    try:\n        with open(filepath, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line and not line.startswith('#') and '=' in line:\n                    key, value = line.split('=', 1)\n                    env_vars[key.strip()] = value.strip()\n                    os.environ[key.strip()] = value.strip()\n        return env_vars\n    except FileNotFoundError:\n        print(f\"Warning: .env file not found at {filepath}\")\n        return {}\n\n# Load .env\nenv_vars = load_env_file('../.env')\n\n# Configuration\nCONFIG = {\n    # Input files\n    'input_data': '../loan_final_desc50plus_with_ocean_bge.csv',\n    \n    # ElasticNet models (5 LLMs)\n    'elasticnet_models': {\n        'llama': '../elasticnet_models_llama.pkl',\n        'gpt': '../elasticnet_models_gpt.pkl',\n        'qwen': '../elasticnet_models_qwen.pkl',\n        'gemma': '../elasticnet_models_gemma.pkl',\n        'deepseek': '../elasticnet_models_deepseek.pkl'\n    },\n    \n    # Output files\n    'output_data': '../loan_2k_with_all_ocean.csv',\n    'output_embeddings': '../bge_embeddings_2k.npy',\n    'output_report': '../05g_generation_report.json',\n    \n    # Sampling parameters\n    'sample_size': 2000,\n    'random_state': 42,\n    \n    # BGE model configuration\n    'bge_model': 'BAAI/bge-large-en-v1.5',\n    'embedding_dim': 1024,\n    \n    # API configuration (adjusted to reduce 500 errors)\n    'batch_size': 10,\n    'delay_between_batches': 0.6,  # Increased from 0.5 to 0.6\n    'max_retries': 5,  # Increased from 3 to 5\n    \n    # OCEAN dimensions\n    'ocean_dims': ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n}\n\n# Load HuggingFace API token\nHF_TOKEN = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_API_KEY')\nif not HF_TOKEN:\n    raise ValueError(\"HuggingFace API token not found! Please set HF_TOKEN in .env file\")\n    \nprint(f\"HuggingFace API token loaded: {HF_TOKEN[:10]}...\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Configuration\")\nprint(\"=\"*80)\nprint(f\"  Sample size: {CONFIG['sample_size']}\")\nprint(f\"  LLM models: {list(CONFIG['elasticnet_models'].keys())}\")\nprint(f\"  OCEAN dimensions: {CONFIG['ocean_dims']}\")\nprint(f\"  Total OCEAN columns: {len(CONFIG['elasticnet_models'])} LLMs x {len(CONFIG['ocean_dims'])} dims = {len(CONFIG['elasticnet_models']) * len(CONFIG['ocean_dims'])}\")\nprint(f\"\\n  API Settings:\")\nprint(f\"    Batch size: {CONFIG['batch_size']}\")\nprint(f\"    Delay: {CONFIG['delay_between_batches']}s\")\nprint(f\"    Max retries: {CONFIG['max_retries']}\")\nprint(f\"    Checkpoint: Every 50 samples\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load 34K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading 34K dataset...\")\n",
    "df_full = pd.read_csv(CONFIG['input_data'], low_memory=False)\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Rows: {len(df_full):,}\")\n",
    "print(f\"  Columns: {len(df_full.columns)}\")\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['desc', 'target']\n",
    "missing_cols = [col for col in required_cols if col not in df_full.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "# Remove rows with missing desc\n",
    "df_valid = df_full[df_full['desc'].notna()].copy()\n",
    "print(f\"\\nRows with valid descriptions: {len(df_valid):,}\")\n",
    "\n",
    "# Target distribution\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_valid['target'].value_counts())\n",
    "print(f\"Default rate: {df_valid['target'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stratified Sampling (2000 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Performing stratified sampling ({CONFIG['sample_size']} samples)...\")\n",
    "\n",
    "# Stratified sampling to maintain class balance\n",
    "df_sample, _ = train_test_split(\n",
    "    df_valid,\n",
    "    train_size=CONFIG['sample_size'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=df_valid['target']\n",
    ")\n",
    "\n",
    "print(f\"\\nSampled data:\")\n",
    "print(f\"  Rows: {len(df_sample):,}\")\n",
    "print(f\"  Target distribution:\")\n",
    "print(df_sample['target'].value_counts())\n",
    "print(f\"  Default rate: {df_sample['target'].mean()*100:.2f}%\")\n",
    "\n",
    "# Reset index\n",
    "df_sample = df_sample.reset_index(drop=True)\n",
    "\n",
    "# Extract descriptions for embedding\n",
    "descriptions = df_sample['desc'].tolist()\n",
    "print(f\"\\nExtracted {len(descriptions)} descriptions for BGE embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract BGE Embeddings (2000 samples)\n",
    "\n",
    "This step extracts BGE embeddings using HuggingFace Inference API.\n",
    "\n",
    "Estimated time: 2000 samples x 0.5s = ~17 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_session_with_retries():\n    \"\"\"Create requests session with automatic retries.\"\"\"\n    session = requests.Session()\n    retry_strategy = Retry(\n        total=CONFIG['max_retries'],\n        backoff_factor=1,\n        status_forcelist=[429, 500, 502, 503, 504]\n    )\n    adapter = HTTPAdapter(max_retries=retry_strategy)\n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n    return session\n\n\ndef extract_bge_embedding(text, session, hf_token, max_retries=5, base_delay=3):\n    \"\"\"\n    Extract BGE embedding for a single text using HF Inference API with enhanced retry logic.\n    \n    Args:\n        text: Input text\n        session: Requests session with retries\n        hf_token: HuggingFace API token\n        max_retries: Maximum retry attempts\n        base_delay: Base delay in seconds for exponential backoff\n    \n    Returns:\n        numpy array: 1024-dim embedding vector\n    \"\"\"\n    api_url = f\"https://api-inference.huggingface.co/models/{CONFIG['bge_model']}\"\n    headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n    \n    for attempt in range(max_retries):\n        try:\n            response = session.post(\n                api_url,\n                headers=headers,\n                json={\"inputs\": text, \"options\": {\"wait_for_model\": True}},\n                timeout=60\n            )\n            \n            if response.status_code == 200:\n                embedding = np.array(response.json())\n                if embedding.shape == (1024,):\n                    return embedding\n                elif embedding.shape == (1, 1024):\n                    return embedding[0]\n                else:\n                    raise ValueError(f\"Unexpected embedding shape: {embedding.shape}\")\n            \n            elif response.status_code == 500:\n                # Internal server error - exponential backoff\n                if attempt < max_retries - 1:\n                    delay = base_delay * (2 ** attempt)\n                    print(f\"      [500 error] Retry {attempt+1}/{max_retries}, waiting {delay}s...\")\n                    time.sleep(delay)\n                    continue\n                else:\n                    raise Exception(f\"API Error 500 after {max_retries} retries\")\n            \n            elif response.status_code == 503:\n                # Model loading\n                if attempt < max_retries - 1:\n                    delay = base_delay * 2\n                    print(f\"      [Model loading] Retry {attempt+1}/{max_retries}, waiting {delay}s...\")\n                    time.sleep(delay)\n                    continue\n                else:\n                    raise Exception(f\"API Error 503 after {max_retries} retries\")\n            \n            else:\n                raise Exception(f\"API error {response.status_code}: {response.text}\")\n                \n        except Exception as e:\n            if attempt < max_retries - 1:\n                delay = base_delay * (attempt + 1)\n                print(f\"      [Error] {str(e)[:50]}, retry {attempt+1}/{max_retries}, waiting {delay}s...\")\n                time.sleep(delay)\n                continue\n            else:\n                raise Exception(f\"Error extracting embedding: {e}\")\n\n\ndef extract_bge_embeddings_batch(texts, session, hf_token, batch_size=10, delay=0.5, \n                                  resume_from=0, existing_embeddings=None):\n    \"\"\"\n    Extract BGE embeddings for multiple texts in batches with resume support.\n    \n    Args:\n        texts: List of texts\n        session: Requests session\n        hf_token: HuggingFace API token\n        batch_size: Batch size\n        delay: Delay between batches\n        resume_from: Index to resume from (for checkpoint recovery)\n        existing_embeddings: Existing embeddings array to continue from\n    \n    Returns:\n        numpy array: (n_texts, 1024) embeddings\n    \"\"\"\n    # Initialize embeddings array\n    if existing_embeddings is not None and len(existing_embeddings) == len(texts):\n        embeddings = existing_embeddings.copy()\n        print(f\"✓ Loaded existing embeddings, resuming from index {resume_from}\")\n    else:\n        embeddings = np.zeros((len(texts), 1024))\n        resume_from = 0\n        print(f\"✓ Starting fresh extraction\")\n    \n    # Calculate remaining work\n    remaining = len(texts) - resume_from\n    if remaining == 0:\n        print(\"✓ All embeddings already extracted!\")\n        return embeddings\n    \n    n_batches_total = (len(texts) + batch_size - 1) // batch_size\n    n_batches_remaining = (remaining + batch_size - 1) // batch_size\n    \n    print(f\"\\nExtracting BGE embeddings for {len(texts)} texts...\")\n    print(f\"  Batch size: {batch_size}\")\n    print(f\"  Resume from: {resume_from} / {len(texts)} ({resume_from/len(texts)*100:.1f}%)\")\n    print(f\"  Remaining: {remaining} samples\")\n    print(f\"  Batches remaining: {n_batches_remaining} / {n_batches_total}\")\n    print(f\"  Estimated time: {n_batches_remaining * delay / 60:.1f} minutes\\n\")\n    \n    start_time = time.time()\n    \n    # Temp file for checkpointing\n    temp_file = CONFIG['output_embeddings'].replace('.npy', '.temp.npy')\n    \n    with tqdm(total=len(texts), initial=resume_from, desc=\"Extracting embeddings\") as pbar:\n        for i in range(resume_from, len(texts)):\n            try:\n                text = texts[i]\n                embedding = extract_bge_embedding(text, session, hf_token)\n                embeddings[i] = embedding\n                pbar.update(1)\n                \n                # Save checkpoint every 50 samples\n                if (i + 1) % 50 == 0:\n                    np.save(temp_file, embeddings)\n                    pbar.set_postfix({'saved': f'checkpoint@{i+1}'})\n                \n                # Delay between requests\n                if i < len(texts) - 1:\n                    time.sleep(delay)\n                    \n            except KeyboardInterrupt:\n                print(f\"\\n\\n⚠️  Interrupted by user at index {i}\")\n                print(f\"   Saving checkpoint to {temp_file}...\")\n                np.save(temp_file, embeddings)\n                print(f\"   ✓ Checkpoint saved! {i}/{len(texts)} completed ({i/len(texts)*100:.1f}%)\")\n                print(f\"   To resume, run the cell again (it will auto-resume from {i})\")\n                raise\n            \n            except Exception as e:\n                print(f\"\\n⚠️  Error at index {i}: {e}\")\n                print(f\"   Saving checkpoint to {temp_file}...\")\n                np.save(temp_file, embeddings)\n                print(f\"   ✓ Checkpoint saved! To resume, run the cell again\")\n                raise\n    \n    elapsed_time = time.time() - start_time\n    \n    # Save final checkpoint\n    np.save(temp_file, embeddings)\n    \n    print(f\"\\n✓ Embedding extraction complete!\")\n    print(f\"  Total time: {elapsed_time / 60:.1f} minutes\")\n    print(f\"  Average time per sample: {elapsed_time / remaining:.2f} seconds\")\n    print(f\"  Checkpoint saved: {temp_file}\")\n    \n    return embeddings\n\nprint(\"✓ Embedding extraction functions defined (with checkpoint support)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for existing checkpoint\ntemp_file = CONFIG['output_embeddings'].replace('.npy', '.temp.npy')\nexisting_embeddings = None\nresume_from = 0\n\nif os.path.exists(temp_file):\n    print(\"=\"*80)\n    print(\"✓ Found checkpoint file!\")\n    print(\"=\"*80)\n    \n    try:\n        existing_embeddings = np.load(temp_file)\n        print(f\"  File: {temp_file}\")\n        print(f\"  Shape: {existing_embeddings.shape}\")\n        print(f\"  Size: {os.path.getsize(temp_file) / 1024 / 1024:.2f} MB\")\n        \n        # Verify shape matches\n        if existing_embeddings.shape[0] != len(descriptions):\n            print(f\"\\n⚠️  Warning: Checkpoint size mismatch!\")\n            print(f\"    Checkpoint: {existing_embeddings.shape[0]} samples\")\n            print(f\"    Current: {len(descriptions)} samples\")\n            print(f\"    Starting fresh extraction...\")\n            existing_embeddings = None\n            resume_from = 0\n        else:\n            # Find where to resume (first zero vector)\n            vector_norms = np.linalg.norm(existing_embeddings, axis=1)\n            completed = np.sum(vector_norms > 0.01)\n            resume_from = completed\n            \n            print(f\"\\n  Progress analysis:\")\n            print(f\"    Completed: {completed} / {len(descriptions)} ({completed/len(descriptions)*100:.1f}%)\")\n            print(f\"    Remaining: {len(descriptions) - completed}\")\n            \n            if resume_from >= len(descriptions):\n                print(f\"\\n✓ All embeddings already extracted!\")\n                embeddings = existing_embeddings\n            else:\n                print(f\"\\n  Will resume from index {resume_from}\")\n                \n    except Exception as e:\n        print(f\"\\n⚠️  Error loading checkpoint: {e}\")\n        print(f\"    Starting fresh extraction...\")\n        existing_embeddings = None\n        resume_from = 0\nelse:\n    print(\"=\"*80)\n    print(\"Starting fresh extraction (no checkpoint found)\")\n    print(\"=\"*80)\n\n# Extract embeddings (with resume support)\nif resume_from < len(descriptions):\n    # Create session\n    session = create_session_with_retries()\n    \n    # Extract embeddings\n    embeddings = extract_bge_embeddings_batch(\n        texts=descriptions,\n        session=session,\n        hf_token=HF_TOKEN,\n        batch_size=CONFIG['batch_size'],\n        delay=CONFIG['delay_between_batches'],\n        resume_from=resume_from,\n        existing_embeddings=existing_embeddings\n    )\n    \n    print(f\"\\n\" + \"=\"*80)\n    print(\"Embeddings Extraction Summary\")\n    print(\"=\"*80)\n    print(f\"  Shape: {embeddings.shape}\")\n    print(f\"  Expected: ({len(df_sample)}, {CONFIG['embedding_dim']})\")\n    \n    # Verify all embeddings are valid\n    vector_norms = np.linalg.norm(embeddings, axis=1)\n    valid_count = np.sum(vector_norms > 0.01)\n    print(f\"  Valid embeddings: {valid_count} / {len(embeddings)} ({valid_count/len(embeddings)*100:.1f}%)\")\n    \n    assert embeddings.shape == (len(df_sample), CONFIG['embedding_dim']), \"Embedding shape mismatch!\"\n    \n    # Save final embeddings\n    final_output = CONFIG['output_embeddings'].replace('.temp', '')\n    np.save(final_output, embeddings)\n    print(f\"\\n✓ Final embeddings saved: {final_output}\")\n    print(f\"  File size: {os.path.getsize(final_output) / 1024 / 1024:.1f} MB\")\n    \n    # Keep temp file for safety\n    print(f\"\\n  Temp file kept for safety: {temp_file}\")\n    print(f\"  (You can delete it manually if extraction is successful)\")\nelse:\n    print(\"\\n✓ Using existing complete embeddings\")\n    final_output = CONFIG['output_embeddings']\n    np.save(final_output, embeddings)\n    print(f\"  Saved to: {final_output}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load ElasticNet Models and Predict OCEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading ElasticNet Models and Predicting OCEAN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Storage for all OCEAN predictions\n",
    "all_ocean_predictions = {}\n",
    "\n",
    "for llm_key, model_path in CONFIG['elasticnet_models'].items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing {llm_key.upper()} model\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model: {model_path}\")\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    # Extract models and scaler\n",
    "    models = model_data['models']\n",
    "    scaler = model_data['scaler']\n",
    "    \n",
    "    print(f\"  Models loaded: {list(models.keys())}\")\n",
    "    print(f\"  Scaler: {type(scaler).__name__}\")\n",
    "    \n",
    "    # Standardize embeddings\n",
    "    embeddings_scaled = scaler.transform(embeddings)\n",
    "    print(f\"  Embeddings standardized: mean={embeddings_scaled.mean():.6f}, std={embeddings_scaled.std():.6f}\")\n",
    "    \n",
    "    # Predict each OCEAN dimension\n",
    "    print(f\"\\n  Predicting OCEAN dimensions:\")\n",
    "    for dim in CONFIG['ocean_dims']:\n",
    "        model = models[dim]\n",
    "        predictions = model.predict(embeddings_scaled)\n",
    "        \n",
    "        # Clip to [0, 1] range\n",
    "        predictions = np.clip(predictions, 0, 1)\n",
    "        \n",
    "        # Store with naming convention: {llm}_{dimension}\n",
    "        col_name = f\"{llm_key}_{dim}\"\n",
    "        all_ocean_predictions[col_name] = predictions\n",
    "        \n",
    "        print(f\"    {dim}: mean={predictions.mean():.3f}, std={predictions.std():.3f}, range=[{predictions.min():.3f}, {predictions.max():.3f}]\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"All OCEAN predictions complete!\")\n",
    "print(f\"Total columns generated: {len(all_ocean_predictions)}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Combine Features and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCombining all features...\")\n",
    "\n",
    "# Start with original dataframe\n",
    "df_output = df_sample.copy()\n",
    "\n",
    "# Add all OCEAN predictions\n",
    "for col_name, predictions in all_ocean_predictions.items():\n",
    "    df_output[col_name] = predictions\n",
    "\n",
    "print(f\"\\nOutput dataframe:\")\n",
    "print(f\"  Rows: {len(df_output):,}\")\n",
    "print(f\"  Columns: {len(df_output.columns)}\")\n",
    "\n",
    "# Show OCEAN columns\n",
    "ocean_cols = [col for col in df_output.columns if any(llm in col for llm in CONFIG['elasticnet_models'].keys())]\n",
    "print(f\"\\nOCEAN columns ({len(ocean_cols)}):\")\n",
    "for i, col in enumerate(sorted(ocean_cols), 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Show sample statistics\n",
    "print(f\"\\nSample OCEAN statistics:\")\n",
    "print(df_output[ocean_cols].describe())\n",
    "\n",
    "# Save to CSV\n",
    "print(f\"\\nSaving to CSV...\")\n",
    "df_output.to_csv(CONFIG['output_data'], index=False)\n",
    "print(f\"Saved: {CONFIG['output_data']}\")\n",
    "print(f\"File size: {os.path.getsize(CONFIG['output_data']) / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "report = {\n",
    "    'phase': '05g - Sample 2K and Generate All OCEAN Features',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    \n",
    "    'data': {\n",
    "        'input_file': CONFIG['input_data'],\n",
    "        'input_size': int(len(df_full)),\n",
    "        'sample_size': int(len(df_output)),\n",
    "        'output_file': CONFIG['output_data'],\n",
    "        'n_features': int(len(df_output.columns))\n",
    "    },\n",
    "    \n",
    "    'method': {\n",
    "        'embedding_model': CONFIG['bge_model'],\n",
    "        'embedding_dim': CONFIG['embedding_dim'],\n",
    "        'llm_models': list(CONFIG['elasticnet_models'].keys()),\n",
    "        'n_llm_models': len(CONFIG['elasticnet_models']),\n",
    "        'ocean_dimensions': CONFIG['ocean_dims'],\n",
    "        'n_ocean_dimensions': len(CONFIG['ocean_dims'])\n",
    "    },\n",
    "    \n",
    "    'ocean_features': {},\n",
    "    \n",
    "    'target_distribution': {\n",
    "        'fully_paid': int((df_output['target'] == 0).sum()),\n",
    "        'charged_off': int((df_output['target'] == 1).sum()),\n",
    "        'default_rate': float(df_output['target'].mean())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add statistics for each OCEAN feature\n",
    "for col in ocean_cols:\n",
    "    report['ocean_features'][col] = {\n",
    "        'mean': float(df_output[col].mean()),\n",
    "        'std': float(df_output[col].std()),\n",
    "        'min': float(df_output[col].min()),\n",
    "        'max': float(df_output[col].max()),\n",
    "        'median': float(df_output[col].median())\n",
    "    }\n",
    "\n",
    "# Save report\n",
    "with open(CONFIG['output_report'], 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. Data Processing\")\n",
    "print(f\"   Input: {report['data']['input_file']}\")\n",
    "print(f\"   Input size: {report['data']['input_size']:,}\")\n",
    "print(f\"   Sample size: {report['data']['sample_size']:,}\")\n",
    "print(f\"   Output: {report['data']['output_file']}\")\n",
    "\n",
    "print(f\"\\n2. LLM Models Processed\")\n",
    "for llm in report['method']['llm_models']:\n",
    "    print(f\"   - {llm}\")\n",
    "\n",
    "print(f\"\\n3. OCEAN Features Generated\")\n",
    "print(f\"   Total: {len(ocean_cols)} columns\")\n",
    "print(f\"   ({report['method']['n_llm_models']} LLMs × {report['method']['n_ocean_dimensions']} dimensions)\")\n",
    "\n",
    "print(f\"\\n4. Target Distribution\")\n",
    "print(f\"   Fully Paid: {report['target_distribution']['fully_paid']:,}\")\n",
    "print(f\"   Charged Off: {report['target_distribution']['charged_off']:,}\")\n",
    "print(f\"   Default rate: {report['target_distribution']['default_rate']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n5. Output Files\")\n",
    "print(f\"   - {CONFIG['output_data']}\")\n",
    "print(f\"   - {CONFIG['output_embeddings']}\")\n",
    "print(f\"   - {CONFIG['output_report']}\")\n",
    "\n",
    "print(f\"\\n6. Next Steps\")\n",
    "print(f\"   Run 07_xgboost_all_llm_comparison_2k.ipynb to:\")\n",
    "print(f\"   - Train 6 XGBoost models (1 baseline + 5 LLM OCEAN)\")\n",
    "print(f\"   - Compare performance across all LLMs\")\n",
    "print(f\"   - Identify best LLM ground truth for loan default prediction\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"05g Complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}