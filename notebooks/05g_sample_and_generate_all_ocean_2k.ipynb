{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05g - Sample 2K and Generate OCEAN Features for All 5 LLMs\n",
    "\n",
    "**Purpose**: Sample 2000 samples from 34K dataset, extract BGE embeddings, and generate OCEAN predictions using all 5 LLM-trained ElasticNet models\n",
    "\n",
    "## Workflow:\n",
    "1. Load 34K dataset\n",
    "2. Stratified sampling (2000 samples, balanced Fully Paid/Charged Off)\n",
    "3. Extract BGE embeddings (2000 samples, ~17 minutes)\n",
    "4. Load 5 ElasticNet models (Llama, GPT, Qwen, Gemma, DeepSeek)\n",
    "5. Predict OCEAN for each model (5 LLMs x 5 dimensions = 25 OCEAN columns)\n",
    "6. Save combined dataset\n",
    "\n",
    "## Input Files:\n",
    "- loan_final_desc50plus_with_ocean_bge.csv (34,530 rows)\n",
    "- elasticnet_models_llama.pkl\n",
    "- elasticnet_models_gpt.pkl\n",
    "- elasticnet_models_qwen.pkl\n",
    "- elasticnet_models_gemma.pkl\n",
    "- elasticnet_models_deepseek.pkl\n",
    "\n",
    "## Output Files:\n",
    "- loan_2k_with_all_ocean.csv (2000 rows x 62 columns)\n",
    "  - 36 base features\n",
    "  - 25 OCEAN features (5 LLMs x 5 dimensions)\n",
    "  - 1 target column\n",
    "- bge_embeddings_2k.npy (2000 x 1024)\n",
    "- 05g_generation_report.json\n",
    "\n",
    "**Estimated Time**: 20-25 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n",
      "Timestamp: 2025-10-31 13:11:41.051080\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For BGE embeddings\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace API token loaded: hf_TdTspnR...\n",
      "\n",
      "================================================================================\n",
      "Configuration\n",
      "================================================================================\n",
      "  Sample size: 2000\n",
      "  LLM models: ['llama', 'gpt', 'qwen', 'gemma', 'deepseek']\n",
      "  OCEAN dimensions: ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "  Total OCEAN columns: 5 LLMs x 5 dims = 25\n",
      "\n",
      "  API Settings:\n",
      "    Batch size: 10\n",
      "    Delay: 0.6s\n",
      "    Max retries: 5\n",
      "    Checkpoint: Every 50 samples\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "def load_env_file(filepath='../.env'):\n",
    "    \"\"\"Load environment variables from .env file\"\"\"\n",
    "    env_vars = {}\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '=' in line:\n",
    "                    key, value = line.split('=', 1)\n",
    "                    env_vars[key.strip()] = value.strip()\n",
    "                    os.environ[key.strip()] = value.strip()\n",
    "        return env_vars\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: .env file not found at {filepath}\")\n",
    "        return {}\n",
    "\n",
    "# Load .env\n",
    "env_vars = load_env_file('../.env')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Input files\n",
    "    'input_data': '../loan_final_desc50plus_with_ocean_bge.csv',\n",
    "    \n",
    "    # ElasticNet models (5 LLMs)\n",
    "    'elasticnet_models': {\n",
    "        'llama': '../elasticnet_models_llama.pkl',\n",
    "        'gpt': '../elasticnet_models_gpt.pkl',\n",
    "        'qwen': '../elasticnet_models_qwen.pkl',\n",
    "        'gemma': '../elasticnet_models_gemma.pkl',\n",
    "        'deepseek': '../elasticnet_models_deepseek.pkl'\n",
    "    },\n",
    "    \n",
    "    # Output files\n",
    "    'output_data': '../loan_2k_with_all_ocean.csv',\n",
    "    'output_embeddings': '../bge_embeddings_2k.npy',\n",
    "    'output_report': '../05g_generation_report.json',\n",
    "    \n",
    "    # Sampling parameters\n",
    "    'sample_size': 2000,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # BGE model configuration\n",
    "    'bge_model': 'BAAI/bge-large-en-v1.5',\n",
    "    'embedding_dim': 1024,\n",
    "    \n",
    "    # API configuration (adjusted to reduce 500 errors)\n",
    "    'batch_size': 10,\n",
    "    'delay_between_batches': 0.6,  # Increased from 0.5 to 0.6\n",
    "    'max_retries': 5,  # Increased from 3 to 5\n",
    "    \n",
    "    # OCEAN dimensions\n",
    "    'ocean_dims': ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "}\n",
    "\n",
    "# Load HuggingFace API token\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_API_KEY')\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HuggingFace API token not found! Please set HF_TOKEN in .env file\")\n",
    "    \n",
    "print(f\"HuggingFace API token loaded: {HF_TOKEN[:10]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Configuration\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Sample size: {CONFIG['sample_size']}\")\n",
    "print(f\"  LLM models: {list(CONFIG['elasticnet_models'].keys())}\")\n",
    "print(f\"  OCEAN dimensions: {CONFIG['ocean_dims']}\")\n",
    "print(f\"  Total OCEAN columns: {len(CONFIG['elasticnet_models'])} LLMs x {len(CONFIG['ocean_dims'])} dims = {len(CONFIG['elasticnet_models']) * len(CONFIG['ocean_dims'])}\")\n",
    "print(f\"\\n  API Settings:\")\n",
    "print(f\"    Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"    Delay: {CONFIG['delay_between_batches']}s\")\n",
    "print(f\"    Max retries: {CONFIG['max_retries']}\")\n",
    "print(f\"    Checkpoint: Every 50 samples\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load 34K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 34K dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loaded:\n",
      "  Rows: 34,529\n",
      "  Columns: 38\n",
      "\n",
      "Rows with valid descriptions: 34,529\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    29479\n",
      "1     5050\n",
      "Name: count, dtype: int64\n",
      "Default rate: 14.63%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading 34K dataset...\")\n",
    "df_full = pd.read_csv(CONFIG['input_data'], low_memory=False)\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Rows: {len(df_full):,}\")\n",
    "print(f\"  Columns: {len(df_full.columns)}\")\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['desc', 'target']\n",
    "missing_cols = [col for col in required_cols if col not in df_full.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "# Remove rows with missing desc\n",
    "df_valid = df_full[df_full['desc'].notna()].copy()\n",
    "print(f\"\\nRows with valid descriptions: {len(df_valid):,}\")\n",
    "\n",
    "# Target distribution\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_valid['target'].value_counts())\n",
    "print(f\"Default rate: {df_valid['target'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Stratified Sampling (2000 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stratified sampling (2000 samples)...\n",
      "\n",
      "Sampled data:\n",
      "  Rows: 2,000\n",
      "  Target distribution:\n",
      "target\n",
      "0    1707\n",
      "1     293\n",
      "Name: count, dtype: int64\n",
      "  Default rate: 14.65%\n",
      "\n",
      "Extracted 2000 descriptions for BGE embedding\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Performing stratified sampling ({CONFIG['sample_size']} samples)...\")\n",
    "\n",
    "# Stratified sampling to maintain class balance\n",
    "df_sample, _ = train_test_split(\n",
    "    df_valid,\n",
    "    train_size=CONFIG['sample_size'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=df_valid['target']\n",
    ")\n",
    "\n",
    "print(f\"\\nSampled data:\")\n",
    "print(f\"  Rows: {len(df_sample):,}\")\n",
    "print(f\"  Target distribution:\")\n",
    "print(df_sample['target'].value_counts())\n",
    "print(f\"  Default rate: {df_sample['target'].mean()*100:.2f}%\")\n",
    "\n",
    "# Reset index\n",
    "df_sample = df_sample.reset_index(drop=True)\n",
    "\n",
    "# Extract descriptions for embedding\n",
    "descriptions = df_sample['desc'].tolist()\n",
    "print(f\"\\nExtracted {len(descriptions)} descriptions for BGE embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract BGE Embeddings (2000 samples)\n",
    "\n",
    "This step extracts BGE embeddings using HuggingFace Inference API.\n",
    "\n",
    "Estimated time: 2000 samples x 0.5s = ~17 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embedding extraction functions defined (with checkpoint support)\n"
     ]
    }
   ],
   "source": [
    "def create_session_with_retries():\n",
    "    \"\"\"Create requests session with automatic retries.\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=CONFIG['max_retries'],\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "\n",
    "def extract_bge_embedding(text, session, hf_token, max_retries=5, base_delay=3):\n",
    "    \"\"\"\n",
    "    Extract BGE embedding for a single text using HF Inference API with enhanced retry logic.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        session: Requests session with retries\n",
    "        hf_token: HuggingFace API token\n",
    "        max_retries: Maximum retry attempts\n",
    "        base_delay: Base delay in seconds for exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: 1024-dim embedding vector\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{CONFIG['bge_model']}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = session.post(\n",
    "                api_url,\n",
    "                headers=headers,\n",
    "                json={\"inputs\": text, \"options\": {\"wait_for_model\": True}},\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                embedding = np.array(response.json())\n",
    "                if embedding.shape == (1024,):\n",
    "                    return embedding\n",
    "                elif embedding.shape == (1, 1024):\n",
    "                    return embedding[0]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected embedding shape: {embedding.shape}\")\n",
    "            \n",
    "            elif response.status_code == 500:\n",
    "                # Internal server error - exponential backoff\n",
    "                if attempt < max_retries - 1:\n",
    "                    delay = base_delay * (2 ** attempt)\n",
    "                    print(f\"      [500 error] Retry {attempt+1}/{max_retries}, waiting {delay}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception(f\"API Error 500 after {max_retries} retries\")\n",
    "            \n",
    "            elif response.status_code == 503:\n",
    "                # Model loading\n",
    "                if attempt < max_retries - 1:\n",
    "                    delay = base_delay * 2\n",
    "                    print(f\"      [Model loading] Retry {attempt+1}/{max_retries}, waiting {delay}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception(f\"API Error 503 after {max_retries} retries\")\n",
    "            \n",
    "            else:\n",
    "                raise Exception(f\"API error {response.status_code}: {response.text}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                delay = base_delay * (attempt + 1)\n",
    "                print(f\"      [Error] {str(e)[:50]}, retry {attempt+1}/{max_retries}, waiting {delay}s...\")\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f\"Error extracting embedding: {e}\")\n",
    "\n",
    "\n",
    "def extract_bge_embeddings_batch(texts, session, hf_token, batch_size=10, delay=0.5, \n",
    "                                  resume_from=0, existing_embeddings=None):\n",
    "    \"\"\"\n",
    "    Extract BGE embeddings for multiple texts in batches with resume support.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts\n",
    "        session: Requests session\n",
    "        hf_token: HuggingFace API token\n",
    "        batch_size: Batch size\n",
    "        delay: Delay between batches\n",
    "        resume_from: Index to resume from (for checkpoint recovery)\n",
    "        existing_embeddings: Existing embeddings array to continue from\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: (n_texts, 1024) embeddings\n",
    "    \"\"\"\n",
    "    # Initialize embeddings array\n",
    "    if existing_embeddings is not None and len(existing_embeddings) == len(texts):\n",
    "        embeddings = existing_embeddings.copy()\n",
    "        print(f\"✓ Loaded existing embeddings, resuming from index {resume_from}\")\n",
    "    else:\n",
    "        embeddings = np.zeros((len(texts), 1024))\n",
    "        resume_from = 0\n",
    "        print(f\"✓ Starting fresh extraction\")\n",
    "    \n",
    "    # Calculate remaining work\n",
    "    remaining = len(texts) - resume_from\n",
    "    if remaining == 0:\n",
    "        print(\"✓ All embeddings already extracted!\")\n",
    "        return embeddings\n",
    "    \n",
    "    n_batches_total = (len(texts) + batch_size - 1) // batch_size\n",
    "    n_batches_remaining = (remaining + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\nExtracting BGE embeddings for {len(texts)} texts...\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Resume from: {resume_from} / {len(texts)} ({resume_from/len(texts)*100:.1f}%)\")\n",
    "    print(f\"  Remaining: {remaining} samples\")\n",
    "    print(f\"  Batches remaining: {n_batches_remaining} / {n_batches_total}\")\n",
    "    print(f\"  Estimated time: {n_batches_remaining * delay / 60:.1f} minutes\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Temp file for checkpointing\n",
    "    temp_file = CONFIG['output_embeddings'].replace('.npy', '.temp.npy')\n",
    "    \n",
    "    with tqdm(total=len(texts), initial=resume_from, desc=\"Extracting embeddings\") as pbar:\n",
    "        for i in range(resume_from, len(texts)):\n",
    "            try:\n",
    "                text = texts[i]\n",
    "                embedding = extract_bge_embedding(text, session, hf_token)\n",
    "                embeddings[i] = embedding\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Save checkpoint every 50 samples\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    np.save(temp_file, embeddings)\n",
    "                    pbar.set_postfix({'saved': f'checkpoint@{i+1}'})\n",
    "                \n",
    "                # Delay between requests\n",
    "                if i < len(texts) - 1:\n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(f\"\\n\\n⚠️  Interrupted by user at index {i}\")\n",
    "                print(f\"   Saving checkpoint to {temp_file}...\")\n",
    "                np.save(temp_file, embeddings)\n",
    "                print(f\"   ✓ Checkpoint saved! {i}/{len(texts)} completed ({i/len(texts)*100:.1f}%)\")\n",
    "                print(f\"   To resume, run the cell again (it will auto-resume from {i})\")\n",
    "                raise\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"\\n⚠️  Error at index {i}: {e}\")\n",
    "                print(f\"   Saving checkpoint to {temp_file}...\")\n",
    "                np.save(temp_file, embeddings)\n",
    "                print(f\"   ✓ Checkpoint saved! To resume, run the cell again\")\n",
    "                raise\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    np.save(temp_file, embeddings)\n",
    "    \n",
    "    print(f\"\\n✓ Embedding extraction complete!\")\n",
    "    print(f\"  Total time: {elapsed_time / 60:.1f} minutes\")\n",
    "    print(f\"  Average time per sample: {elapsed_time / remaining:.2f} seconds\")\n",
    "    print(f\"  Checkpoint saved: {temp_file}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "print(\"✓ Embedding extraction functions defined (with checkpoint support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "✓ Found checkpoint file!\n",
      "================================================================================\n",
      "  File: ../bge_embeddings_2k.temp.npy\n",
      "  Shape: (2000, 1024)\n",
      "  Size: 15.63 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Progress analysis:\n",
      "    Completed: 2000 / 2000 (100.0%)\n",
      "    Remaining: 0\n",
      "\n",
      "✓ All embeddings already extracted!\n",
      "\n",
      "✓ Using existing complete embeddings\n",
      "  Saved to: ../bge_embeddings_2k.npy\n"
     ]
    }
   ],
   "source": [
    "# Check for existing checkpoint\n",
    "temp_file = CONFIG['output_embeddings'].replace('.npy', '.temp.npy')\n",
    "existing_embeddings = None\n",
    "resume_from = 0\n",
    "\n",
    "if os.path.exists(temp_file):\n",
    "    print(\"=\"*80)\n",
    "    print(\"✓ Found checkpoint file!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        existing_embeddings = np.load(temp_file)\n",
    "        print(f\"  File: {temp_file}\")\n",
    "        print(f\"  Shape: {existing_embeddings.shape}\")\n",
    "        print(f\"  Size: {os.path.getsize(temp_file) / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # Verify shape matches\n",
    "        if existing_embeddings.shape[0] != len(descriptions):\n",
    "            print(f\"\\n⚠️  Warning: Checkpoint size mismatch!\")\n",
    "            print(f\"    Checkpoint: {existing_embeddings.shape[0]} samples\")\n",
    "            print(f\"    Current: {len(descriptions)} samples\")\n",
    "            print(f\"    Starting fresh extraction...\")\n",
    "            existing_embeddings = None\n",
    "            resume_from = 0\n",
    "        else:\n",
    "            # Find where to resume (first zero vector)\n",
    "            vector_norms = np.linalg.norm(existing_embeddings, axis=1)\n",
    "            completed = np.sum(vector_norms > 0.01)\n",
    "            resume_from = completed\n",
    "            \n",
    "            print(f\"\\n  Progress analysis:\")\n",
    "            print(f\"    Completed: {completed} / {len(descriptions)} ({completed/len(descriptions)*100:.1f}%)\")\n",
    "            print(f\"    Remaining: {len(descriptions) - completed}\")\n",
    "            \n",
    "            if resume_from >= len(descriptions):\n",
    "                print(f\"\\n✓ All embeddings already extracted!\")\n",
    "                embeddings = existing_embeddings\n",
    "            else:\n",
    "                print(f\"\\n  Will resume from index {resume_from}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️  Error loading checkpoint: {e}\")\n",
    "        print(f\"    Starting fresh extraction...\")\n",
    "        existing_embeddings = None\n",
    "        resume_from = 0\n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"Starting fresh extraction (no checkpoint found)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Extract embeddings (with resume support)\n",
    "if resume_from < len(descriptions):\n",
    "    # Create session\n",
    "    session = create_session_with_retries()\n",
    "    \n",
    "    # Extract embeddings\n",
    "    embeddings = extract_bge_embeddings_batch(\n",
    "        texts=descriptions,\n",
    "        session=session,\n",
    "        hf_token=HF_TOKEN,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        delay=CONFIG['delay_between_batches'],\n",
    "        resume_from=resume_from,\n",
    "        existing_embeddings=existing_embeddings\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"Embeddings Extraction Summary\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Shape: {embeddings.shape}\")\n",
    "    print(f\"  Expected: ({len(df_sample)}, {CONFIG['embedding_dim']})\")\n",
    "    \n",
    "    # Verify all embeddings are valid\n",
    "    vector_norms = np.linalg.norm(embeddings, axis=1)\n",
    "    valid_count = np.sum(vector_norms > 0.01)\n",
    "    print(f\"  Valid embeddings: {valid_count} / {len(embeddings)} ({valid_count/len(embeddings)*100:.1f}%)\")\n",
    "    \n",
    "    assert embeddings.shape == (len(df_sample), CONFIG['embedding_dim']), \"Embedding shape mismatch!\"\n",
    "    \n",
    "    # Save final embeddings\n",
    "    final_output = CONFIG['output_embeddings'].replace('.temp', '')\n",
    "    np.save(final_output, embeddings)\n",
    "    print(f\"\\n✓ Final embeddings saved: {final_output}\")\n",
    "    print(f\"  File size: {os.path.getsize(final_output) / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Keep temp file for safety\n",
    "    print(f\"\\n  Temp file kept for safety: {temp_file}\")\n",
    "    print(f\"  (You can delete it manually if extraction is successful)\")\n",
    "else:\n",
    "    print(\"\\n✓ Using existing complete embeddings\")\n",
    "    final_output = CONFIG['output_embeddings']\n",
    "    np.save(final_output, embeddings)\n",
    "    print(f\"  Saved to: {final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load ElasticNet Models and Predict OCEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading ElasticNet Models and Predicting OCEAN\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Processing LLAMA model\n",
      "================================================================================\n",
      "Loading model: ../elasticnet_models_llama.pkl\n",
      "  Models loaded: ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "  Scaler: StandardScaler\n",
      "  Embeddings standardized: mean=-0.000603, std=1.107493\n",
      "\n",
      "  Predicting OCEAN dimensions:\n",
      "    openness: mean=0.343, std=0.020, range=[0.261, 0.407]\n",
      "    conscientiousness: mean=0.582, std=0.000, range=[0.582, 0.582]\n",
      "    extraversion: mean=0.256, std=0.000, range=[0.256, 0.256]\n",
      "    agreeableness: mean=0.605, std=0.000, range=[0.605, 0.605]\n",
      "    neuroticism: mean=0.177, std=0.000, range=[0.177, 0.177]\n",
      "\n",
      "================================================================================\n",
      "Processing GPT model\n",
      "================================================================================\n",
      "Loading model: ../elasticnet_models_gpt.pkl\n",
      "  Models loaded: ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "  Scaler: StandardScaler\n",
      "  Embeddings standardized: mean=-0.000391, std=1.100176\n",
      "\n",
      "  Predicting OCEAN dimensions:\n",
      "    openness: mean=0.261, std=0.000, range=[0.261, 0.261]\n",
      "    conscientiousness: mean=0.727, std=0.000, range=[0.727, 0.727]\n",
      "    extraversion: mean=0.184, std=0.000, range=[0.184, 0.184]\n",
      "    agreeableness: mean=0.415, std=0.000, range=[0.415, 0.415]\n",
      "    neuroticism: mean=0.325, std=0.000, range=[0.324, 0.325]\n",
      "\n",
      "================================================================================\n",
      "Processing QWEN model\n",
      "================================================================================\n",
      "Loading model: ../elasticnet_models_qwen.pkl\n",
      "  Models loaded: ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "  Scaler: StandardScaler\n",
      "  Embeddings standardized: mean=-0.000735, std=1.100584\n",
      "\n",
      "  Predicting OCEAN dimensions:\n",
      "    openness: mean=0.338, std=0.000, range=[0.338, 0.338]\n",
      "    conscientiousness: mean=0.690, std=0.000, range=[0.690, 0.690]\n",
      "    extraversion: mean=0.405, std=0.000, range=[0.405, 0.405]\n",
      "    agreeableness: mean=0.595, std=0.000, range=[0.595, 0.595]\n",
      "    neuroticism: mean=0.473, std=0.000, range=[0.473, 0.473]\n",
      "\n",
      "================================================================================\n",
      "Processing GEMMA model\n",
      "================================================================================\n",
      "Loading model: ../elasticnet_models_gemma.pkl\n",
      "  Models loaded: ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "  Scaler: StandardScaler\n",
      "  Embeddings standardized: mean=-0.000391, std=1.100176\n",
      "\n",
      "  Predicting OCEAN dimensions:\n",
      "    openness: mean=0.283, std=0.000, range=[0.283, 0.283]\n",
      "    conscientiousness: mean=0.595, std=0.000, range=[0.595, 0.595]\n",
      "    extraversion: mean=0.278, std=0.000, range=[0.278, 0.278]\n",
      "    agreeableness: mean=0.507, std=0.000, range=[0.507, 0.507]\n",
      "    neuroticism: mean=0.202, std=0.003, range=[0.189, 0.212]\n",
      "\n",
      "================================================================================\n",
      "Processing DEEPSEEK model\n",
      "================================================================================\n",
      "Loading model: ../elasticnet_models_deepseek.pkl\n",
      "  Models loaded: ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "  Scaler: StandardScaler\n",
      "  Embeddings standardized: mean=-0.000391, std=1.100176\n",
      "\n",
      "  Predicting OCEAN dimensions:\n",
      "    openness: mean=0.293, std=0.000, range=[0.293, 0.293]\n",
      "    conscientiousness: mean=0.763, std=0.000, range=[0.763, 0.763]\n",
      "    extraversion: mean=0.258, std=0.000, range=[0.258, 0.258]\n",
      "    agreeableness: mean=0.487, std=0.000, range=[0.487, 0.487]\n",
      "    neuroticism: mean=0.382, std=0.000, range=[0.382, 0.382]\n",
      "\n",
      "================================================================================\n",
      "All OCEAN predictions complete!\n",
      "Total columns generated: 25\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading ElasticNet Models and Predicting OCEAN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Storage for all OCEAN predictions\n",
    "all_ocean_predictions = {}\n",
    "\n",
    "for llm_key, model_path in CONFIG['elasticnet_models'].items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing {llm_key.upper()} model\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model: {model_path}\")\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    # Extract models and scaler\n",
    "    models = model_data['models']\n",
    "    scaler = model_data['scaler']\n",
    "    \n",
    "    print(f\"  Models loaded: {list(models.keys())}\")\n",
    "    print(f\"  Scaler: {type(scaler).__name__}\")\n",
    "    \n",
    "    # Standardize embeddings\n",
    "    embeddings_scaled = scaler.transform(embeddings)\n",
    "    print(f\"  Embeddings standardized: mean={embeddings_scaled.mean():.6f}, std={embeddings_scaled.std():.6f}\")\n",
    "    \n",
    "    # Predict each OCEAN dimension\n",
    "    print(f\"\\n  Predicting OCEAN dimensions:\")\n",
    "    for dim in CONFIG['ocean_dims']:\n",
    "        model = models[dim]\n",
    "        predictions = model.predict(embeddings_scaled)\n",
    "        \n",
    "        # Clip to [0, 1] range\n",
    "        predictions = np.clip(predictions, 0, 1)\n",
    "        \n",
    "        # Store with naming convention: {llm}_{dimension}\n",
    "        col_name = f\"{llm_key}_{dim}\"\n",
    "        all_ocean_predictions[col_name] = predictions\n",
    "        \n",
    "        print(f\"    {dim}: mean={predictions.mean():.3f}, std={predictions.std():.3f}, range=[{predictions.min():.3f}, {predictions.max():.3f}]\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"All OCEAN predictions complete!\")\n",
    "print(f\"Total columns generated: {len(all_ocean_predictions)}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Combine Features and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining all features...\n",
      "\n",
      "Output dataframe:\n",
      "  Rows: 2,000\n",
      "  Columns: 63\n",
      "\n",
      "OCEAN columns (25):\n",
      "   1. deepseek_agreeableness\n",
      "   2. deepseek_conscientiousness\n",
      "   3. deepseek_extraversion\n",
      "   4. deepseek_neuroticism\n",
      "   5. deepseek_openness\n",
      "   6. gemma_agreeableness\n",
      "   7. gemma_conscientiousness\n",
      "   8. gemma_extraversion\n",
      "   9. gemma_neuroticism\n",
      "  10. gemma_openness\n",
      "  11. gpt_agreeableness\n",
      "  12. gpt_conscientiousness\n",
      "  13. gpt_extraversion\n",
      "  14. gpt_neuroticism\n",
      "  15. gpt_openness\n",
      "  16. llama_agreeableness\n",
      "  17. llama_conscientiousness\n",
      "  18. llama_extraversion\n",
      "  19. llama_neuroticism\n",
      "  20. llama_openness\n",
      "  21. qwen_agreeableness\n",
      "  22. qwen_conscientiousness\n",
      "  23. qwen_extraversion\n",
      "  24. qwen_neuroticism\n",
      "  25. qwen_openness\n",
      "\n",
      "Sample OCEAN statistics:\n",
      "       llama_openness  llama_conscientiousness  llama_extraversion   \n",
      "count     2000.000000             2.000000e+03        2.000000e+03  \\\n",
      "mean         0.342763             5.816327e-01        2.564286e-01   \n",
      "std          0.019935             1.110501e-16        5.552503e-17   \n",
      "min          0.260507             5.816327e-01        2.564286e-01   \n",
      "25%          0.329962             5.816327e-01        2.564286e-01   \n",
      "50%          0.343130             5.816327e-01        2.564286e-01   \n",
      "75%          0.356082             5.816327e-01        2.564286e-01   \n",
      "max          0.407155             5.816327e-01        2.564286e-01   \n",
      "\n",
      "       llama_agreeableness  llama_neuroticism  gpt_openness   \n",
      "count         2.000000e+03        2000.000000     2000.0000  \\\n",
      "mean          6.051786e-01           0.176913        0.2614   \n",
      "std           1.110501e-16           0.000000        0.0000   \n",
      "min           6.051786e-01           0.176913        0.2614   \n",
      "25%           6.051786e-01           0.176913        0.2614   \n",
      "50%           6.051786e-01           0.176913        0.2614   \n",
      "75%           6.051786e-01           0.176913        0.2614   \n",
      "max           6.051786e-01           0.176913        0.2614   \n",
      "\n",
      "       gpt_conscientiousness  gpt_extraversion  gpt_agreeableness   \n",
      "count           2.000000e+03      2.000000e+03       2.000000e+03  \\\n",
      "mean            7.272250e-01      1.844250e-01       4.152750e-01   \n",
      "std             1.110501e-16      5.552503e-17       2.221001e-16   \n",
      "min             7.272250e-01      1.844250e-01       4.152750e-01   \n",
      "25%             7.272250e-01      1.844250e-01       4.152750e-01   \n",
      "50%             7.272250e-01      1.844250e-01       4.152750e-01   \n",
      "75%             7.272250e-01      1.844250e-01       4.152750e-01   \n",
      "max             7.272250e-01      1.844250e-01       4.152750e-01   \n",
      "\n",
      "       gpt_neuroticism  ...  gemma_openness  gemma_conscientiousness   \n",
      "count      2000.000000  ...    2.000000e+03              2000.000000  \\\n",
      "mean          0.324660  ...    2.832500e-01                 0.594625   \n",
      "std           0.000075  ...    1.110501e-16                 0.000000   \n",
      "min           0.324388  ...    2.832500e-01                 0.594625   \n",
      "25%           0.324608  ...    2.832500e-01                 0.594625   \n",
      "50%           0.324657  ...    2.832500e-01                 0.594625   \n",
      "75%           0.324711  ...    2.832500e-01                 0.594625   \n",
      "max           0.324950  ...    2.832500e-01                 0.594625   \n",
      "\n",
      "       gemma_extraversion  gemma_agreeableness  gemma_neuroticism   \n",
      "count            2000.000         2.000000e+03        2000.000000  \\\n",
      "mean                0.278         5.073750e-01           0.202483   \n",
      "std                 0.000         1.110501e-16           0.002882   \n",
      "min                 0.278         5.073750e-01           0.189453   \n",
      "25%                 0.278         5.073750e-01           0.200641   \n",
      "50%                 0.278         5.073750e-01           0.202625   \n",
      "75%                 0.278         5.073750e-01           0.204357   \n",
      "max                 0.278         5.073750e-01           0.211954   \n",
      "\n",
      "       deepseek_openness  deepseek_conscientiousness  deepseek_extraversion   \n",
      "count         2000.00000                   2000.0000           2.000000e+03  \\\n",
      "mean             0.29275                      0.7635           2.577500e-01   \n",
      "std              0.00000                      0.0000           5.552503e-17   \n",
      "min              0.29275                      0.7635           2.577500e-01   \n",
      "25%              0.29275                      0.7635           2.577500e-01   \n",
      "50%              0.29275                      0.7635           2.577500e-01   \n",
      "75%              0.29275                      0.7635           2.577500e-01   \n",
      "max              0.29275                      0.7635           2.577500e-01   \n",
      "\n",
      "       deepseek_agreeableness  deepseek_neuroticism  \n",
      "count            2.000000e+03          2.000000e+03  \n",
      "mean             4.875000e-01          3.820000e-01  \n",
      "std              1.110501e-16          5.552503e-17  \n",
      "min              4.875000e-01          3.820000e-01  \n",
      "25%              4.875000e-01          3.820000e-01  \n",
      "50%              4.875000e-01          3.820000e-01  \n",
      "75%              4.875000e-01          3.820000e-01  \n",
      "max              4.875000e-01          3.820000e-01  \n",
      "\n",
      "[8 rows x 25 columns]\n",
      "\n",
      "Saving to CSV...\n",
      "Saved: ../loan_2k_with_all_ocean.csv\n",
      "File size: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCombining all features...\")\n",
    "\n",
    "# Start with original dataframe\n",
    "df_output = df_sample.copy()\n",
    "\n",
    "# Add all OCEAN predictions\n",
    "for col_name, predictions in all_ocean_predictions.items():\n",
    "    df_output[col_name] = predictions\n",
    "\n",
    "print(f\"\\nOutput dataframe:\")\n",
    "print(f\"  Rows: {len(df_output):,}\")\n",
    "print(f\"  Columns: {len(df_output.columns)}\")\n",
    "\n",
    "# Show OCEAN columns\n",
    "ocean_cols = [col for col in df_output.columns if any(llm in col for llm in CONFIG['elasticnet_models'].keys())]\n",
    "print(f\"\\nOCEAN columns ({len(ocean_cols)}):\")\n",
    "for i, col in enumerate(sorted(ocean_cols), 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Show sample statistics\n",
    "print(f\"\\nSample OCEAN statistics:\")\n",
    "print(df_output[ocean_cols].describe())\n",
    "\n",
    "# Save to CSV\n",
    "print(f\"\\nSaving to CSV...\")\n",
    "df_output.to_csv(CONFIG['output_data'], index=False)\n",
    "print(f\"Saved: {CONFIG['output_data']}\")\n",
    "print(f\"File size: {os.path.getsize(CONFIG['output_data']) / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "1. Data Processing\n",
      "   Input: ../loan_final_desc50plus_with_ocean_bge.csv\n",
      "   Input size: 34,529\n",
      "   Sample size: 2,000\n",
      "   Output: ../loan_2k_with_all_ocean.csv\n",
      "\n",
      "2. LLM Models Processed\n",
      "   - llama\n",
      "   - gpt\n",
      "   - qwen\n",
      "   - gemma\n",
      "   - deepseek\n",
      "\n",
      "3. OCEAN Features Generated\n",
      "   Total: 25 columns\n",
      "   (5 LLMs × 5 dimensions)\n",
      "\n",
      "4. Target Distribution\n",
      "   Fully Paid: 1,707\n",
      "   Charged Off: 293\n",
      "   Default rate: 14.65%\n",
      "\n",
      "5. Output Files\n",
      "   - ../loan_2k_with_all_ocean.csv\n",
      "   - ../bge_embeddings_2k.npy\n",
      "   - ../05g_generation_report.json\n",
      "\n",
      "6. Next Steps\n",
      "   Run 07_xgboost_all_llm_comparison_2k.ipynb to:\n",
      "   - Train 6 XGBoost models (1 baseline + 5 LLM OCEAN)\n",
      "   - Compare performance across all LLMs\n",
      "   - Identify best LLM ground truth for loan default prediction\n",
      "\n",
      "================================================================================\n",
      "05g Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate summary report\n",
    "report = {\n",
    "    'phase': '05g - Sample 2K and Generate All OCEAN Features',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    \n",
    "    'data': {\n",
    "        'input_file': CONFIG['input_data'],\n",
    "        'input_size': int(len(df_full)),\n",
    "        'sample_size': int(len(df_output)),\n",
    "        'output_file': CONFIG['output_data'],\n",
    "        'n_features': int(len(df_output.columns))\n",
    "    },\n",
    "    \n",
    "    'method': {\n",
    "        'embedding_model': CONFIG['bge_model'],\n",
    "        'embedding_dim': CONFIG['embedding_dim'],\n",
    "        'llm_models': list(CONFIG['elasticnet_models'].keys()),\n",
    "        'n_llm_models': len(CONFIG['elasticnet_models']),\n",
    "        'ocean_dimensions': CONFIG['ocean_dims'],\n",
    "        'n_ocean_dimensions': len(CONFIG['ocean_dims'])\n",
    "    },\n",
    "    \n",
    "    'ocean_features': {},\n",
    "    \n",
    "    'target_distribution': {\n",
    "        'fully_paid': int((df_output['target'] == 0).sum()),\n",
    "        'charged_off': int((df_output['target'] == 1).sum()),\n",
    "        'default_rate': float(df_output['target'].mean())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add statistics for each OCEAN feature\n",
    "for col in ocean_cols:\n",
    "    report['ocean_features'][col] = {\n",
    "        'mean': float(df_output[col].mean()),\n",
    "        'std': float(df_output[col].std()),\n",
    "        'min': float(df_output[col].min()),\n",
    "        'max': float(df_output[col].max()),\n",
    "        'median': float(df_output[col].median())\n",
    "    }\n",
    "\n",
    "# Save report\n",
    "with open(CONFIG['output_report'], 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. Data Processing\")\n",
    "print(f\"   Input: {report['data']['input_file']}\")\n",
    "print(f\"   Input size: {report['data']['input_size']:,}\")\n",
    "print(f\"   Sample size: {report['data']['sample_size']:,}\")\n",
    "print(f\"   Output: {report['data']['output_file']}\")\n",
    "\n",
    "print(f\"\\n2. LLM Models Processed\")\n",
    "for llm in report['method']['llm_models']:\n",
    "    print(f\"   - {llm}\")\n",
    "\n",
    "print(f\"\\n3. OCEAN Features Generated\")\n",
    "print(f\"   Total: {len(ocean_cols)} columns\")\n",
    "print(f\"   ({report['method']['n_llm_models']} LLMs × {report['method']['n_ocean_dimensions']} dimensions)\")\n",
    "\n",
    "print(f\"\\n4. Target Distribution\")\n",
    "print(f\"   Fully Paid: {report['target_distribution']['fully_paid']:,}\")\n",
    "print(f\"   Charged Off: {report['target_distribution']['charged_off']:,}\")\n",
    "print(f\"   Default rate: {report['target_distribution']['default_rate']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n5. Output Files\")\n",
    "print(f\"   - {CONFIG['output_data']}\")\n",
    "print(f\"   - {CONFIG['output_embeddings']}\")\n",
    "print(f\"   - {CONFIG['output_report']}\")\n",
    "\n",
    "print(f\"\\n6. Next Steps\")\n",
    "print(f\"   Run 07_xgboost_all_llm_comparison_2k.ipynb to:\")\n",
    "print(f\"   - Train 6 XGBoost models (1 baseline + 5 LLM OCEAN)\")\n",
    "print(f\"   - Compare performance across all LLMs\")\n",
    "print(f\"   - Identify best LLM ground truth for loan default prediction\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"05g Complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
