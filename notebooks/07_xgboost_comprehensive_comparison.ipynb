{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - XGBoost Comprehensive Comparison: Baseline vs OCEAN Methods\n",
    "\n",
    "**Purpose**: Compare XGBoost performance with different OCEAN feature extraction methods\n",
    "\n",
    "## Models to Compare:\n",
    "\n",
    "1. **Baseline**: XGBoost without OCEAN features (36 base features)\n",
    "2. **Ridge-Weighted OCEAN**: Ridge regression mapping from base features to OCEAN (R2 0.15-0.20)\n",
    "3. **BGE + ElasticNet OCEAN**: BGE embeddings + ElasticNet regression (R2 0.127)\n",
    "\n",
    "## Evaluation Metrics:\n",
    "\n",
    "- **Primary**: ROC-AUC (main metric for loan default prediction)\n",
    "- **Secondary**: Precision, Recall, F1-Score\n",
    "- **Feature Analysis**: OCEAN feature importance (XGBoost gain, SHAP values)\n",
    "- **Statistical**: McNemar's test, Bootstrap confidence intervals\n",
    "\n",
    "## Expected Outcomes:\n",
    "\n",
    "Based on methodology:\n",
    "- BGE ElasticNet R2 0.127 â†’ Expected AUC improvement +0.010 ~ +0.025\n",
    "- If AUC improvement < 0.01: OCEAN not useful for loan default prediction\n",
    "- If AUC improvement > 0.02: BGE ElasticNet OCEAN recommended for production\n",
    "\n",
    "**Estimated Time**: 30-60 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Statistical tests\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Data files\n",
    "    'baseline_data': '../data/loan_clean_for_modeling.csv',\n",
    "    'bge_ocean_data': '../loan_with_bge_elasticnet_ocean.csv',\n",
    "    \n",
    "    # Previous results (Ridge-Weighted OCEAN)\n",
    "    'ridge_results': '../results/xgboost_comparison_results.json',\n",
    "    \n",
    "    # Output files\n",
    "    'output_comparison': '../xgboost_comprehensive_comparison.csv',\n",
    "    'output_visualization': '../xgboost_comprehensive_evaluation.png',\n",
    "    'output_report': '../xgboost_comparison_report.json',\n",
    "    \n",
    "    # OCEAN features\n",
    "    'ocean_dims': ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'],\n",
    "    'bge_ocean_prefix': 'bge_elasticnet_',  # Prefix for BGE ElasticNet columns\n",
    "    \n",
    "    # Model parameters\n",
    "    'xgboost_params': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'eval_metric': 'logloss',\n",
    "        'early_stopping_rounds': 10\n",
    "    },\n",
    "    \n",
    "    # Train/test split\n",
    "    'test_size': 0.2,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    \n",
    "    # Features to remove (high cardinality)\n",
    "    'remove_features': ['emp_title', 'title', 'earliest_cr_line', 'desc']\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key not in ['xgboost_params', 'ocean_dims', 'remove_features']:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nXGBoost parameters: {CONFIG['xgboost_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data and Prepare Features\n",
    "\n",
    "We need to align the three datasets:\n",
    "1. Baseline data (all samples)\n",
    "2. BGE OCEAN data (subset with desc >= 50 chars)\n",
    "\n",
    "Strategy: Use intersection of samples that have BGE OCEAN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline data\n",
    "print(\"Loading baseline data...\")\n",
    "df_baseline = pd.read_csv(CONFIG['baseline_data'], low_memory=False)\n",
    "print(f\"  Rows: {len(df_baseline):,}\")\n",
    "print(f\"  Columns: {len(df_baseline.columns)}\")\n",
    "\n",
    "# Load BGE OCEAN data\n",
    "print(\"\\nLoading BGE + ElasticNet OCEAN data...\")\n",
    "df_bge_ocean = pd.read_csv(CONFIG['bge_ocean_data'], low_memory=False)\n",
    "print(f\"  Rows: {len(df_bge_ocean):,}\")\n",
    "print(f\"  Columns: {len(df_bge_ocean.columns)}\")\n",
    "\n",
    "# Verify BGE OCEAN columns exist\n",
    "bge_ocean_cols = [f\"{CONFIG['bge_ocean_prefix']}{dim}\" for dim in CONFIG['ocean_dims']]\n",
    "missing_cols = [col for col in bge_ocean_cols if col not in df_bge_ocean.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing BGE OCEAN columns: {missing_cols}\")\n",
    "\n",
    "print(f\"\\nBGE OCEAN columns found: {bge_ocean_cols}\")\n",
    "\n",
    "# Check target column\n",
    "if 'target' not in df_bge_ocean.columns:\n",
    "    raise ValueError(\"Missing target column!\")\n",
    "\n",
    "print(f\"\\nTarget distribution (BGE data):\")\n",
    "print(df_bge_ocean['target'].value_counts())\n",
    "print(f\"Default rate: {df_bge_ocean['target'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Three Feature Sets\n",
    "\n",
    "1. **Baseline Features**: Original features only (no OCEAN)\n",
    "2. **Ridge OCEAN Features**: Original features + Ridge-Weighted OCEAN (if available in data)\n",
    "3. **BGE OCEAN Features**: Original features + BGE ElasticNet OCEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BGE OCEAN data as base (since it has all features)\n",
    "df = df_bge_ocean.copy()\n",
    "\n",
    "# Separate features and target\n",
    "y = df['target']\n",
    "X_all = df.drop(columns=['target'], errors='ignore')\n",
    "\n",
    "# Remove high cardinality features\n",
    "X_all = X_all.drop(columns=CONFIG['remove_features'], errors='ignore')\n",
    "\n",
    "print(f\"Full feature set shape: {X_all.shape}\")\n",
    "\n",
    "# Prepare three feature sets\n",
    "# 1. Baseline: Remove ALL OCEAN features\n",
    "ocean_related_cols = [col for col in X_all.columns if any(dim in col.lower() for dim in CONFIG['ocean_dims'])]\n",
    "X_baseline = X_all.drop(columns=ocean_related_cols, errors='ignore')\n",
    "\n",
    "print(f\"\\n1. Baseline features: {X_baseline.shape[1]} features\")\n",
    "print(f\"   Removed {len(ocean_related_cols)} OCEAN-related columns\")\n",
    "\n",
    "# 2. Ridge OCEAN: Original features + old OCEAN columns (if they exist)\n",
    "ridge_ocean_cols = [col for col in X_all.columns \n",
    "                    if col in CONFIG['ocean_dims']]  # Old columns without prefix\n",
    "\n",
    "if ridge_ocean_cols:\n",
    "    X_ridge = X_baseline.copy()\n",
    "    for col in ridge_ocean_cols:\n",
    "        X_ridge[col] = X_all[col]\n",
    "    print(f\"\\n2. Ridge OCEAN features: {X_ridge.shape[1]} features\")\n",
    "    print(f\"   Added {len(ridge_ocean_cols)} Ridge OCEAN columns: {ridge_ocean_cols}\")\n",
    "    has_ridge = True\n",
    "else:\n",
    "    print(f\"\\n2. Ridge OCEAN features: NOT FOUND in dataset\")\n",
    "    print(f\"   Will use results from previous run: {CONFIG['ridge_results']}\")\n",
    "    X_ridge = None\n",
    "    has_ridge = False\n",
    "\n",
    "# 3. BGE OCEAN: Original features + BGE ElasticNet OCEAN\n",
    "X_bge = X_baseline.copy()\n",
    "for col in bge_ocean_cols:\n",
    "    X_bge[col] = X_all[col]\n",
    "\n",
    "print(f\"\\n3. BGE ElasticNet OCEAN features: {X_bge.shape[1]} features\")\n",
    "print(f\"   Added {len(bge_ocean_cols)} BGE OCEAN columns: {bge_ocean_cols}\")\n",
    "\n",
    "# Verify all feature sets have same number of samples\n",
    "print(f\"\\nSample counts:\")\n",
    "print(f\"  Baseline: {len(X_baseline)}\")\n",
    "if has_ridge:\n",
    "    print(f\"  Ridge OCEAN: {len(X_ridge)}\")\n",
    "print(f\"  BGE OCEAN: {len(X_bge)}\")\n",
    "print(f\"  Target: {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train/Test Split (Consistent Across All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single train/test split to ensure fair comparison\n",
    "print(\"Performing train/test split (80/20)...\\n\")\n",
    "\n",
    "# Get train/test indices\n",
    "indices = np.arange(len(y))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Split baseline\n",
    "X_baseline_train = X_baseline.iloc[train_idx]\n",
    "X_baseline_test = X_baseline.iloc[test_idx]\n",
    "\n",
    "# Split Ridge (if available)\n",
    "if has_ridge:\n",
    "    X_ridge_train = X_ridge.iloc[train_idx]\n",
    "    X_ridge_test = X_ridge.iloc[test_idx]\n",
    "\n",
    "# Split BGE\n",
    "X_bge_train = X_bge.iloc[train_idx]\n",
    "X_bge_test = X_bge.iloc[test_idx]\n",
    "\n",
    "# Split target\n",
    "y_train = y.iloc[train_idx]\n",
    "y_test = y.iloc[test_idx]\n",
    "\n",
    "print(f\"Training set: {len(y_train):,} samples ({len(y_train)/len(y)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(y_test):,} samples ({len(y_test)/len(y)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain default rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Test default rate: {y_test.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(X):\n",
    "    \"\"\"\n",
    "    Create preprocessing pipeline for given feature set.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature dataframe\n",
    "    \n",
    "    Returns:\n",
    "        ColumnTransformer: Preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Identify feature types\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Numeric preprocessing\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Categorical preprocessing\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Combined preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    return preprocessor, numeric_features, categorical_features\n",
    "\n",
    "\n",
    "# Create preprocessors for each feature set\n",
    "print(\"Creating preprocessing pipelines...\\n\")\n",
    "\n",
    "# Baseline\n",
    "preprocessor_baseline, num_feats_baseline, cat_feats_baseline = create_preprocessor(X_baseline_train)\n",
    "print(f\"Baseline preprocessor:\")\n",
    "print(f\"  Numeric features: {len(num_feats_baseline)}\")\n",
    "print(f\"  Categorical features: {len(cat_feats_baseline)}\")\n",
    "\n",
    "# Ridge (if available)\n",
    "if has_ridge:\n",
    "    preprocessor_ridge, num_feats_ridge, cat_feats_ridge = create_preprocessor(X_ridge_train)\n",
    "    print(f\"\\nRidge OCEAN preprocessor:\")\n",
    "    print(f\"  Numeric features: {len(num_feats_ridge)}\")\n",
    "    print(f\"  Categorical features: {len(cat_feats_ridge)}\")\n",
    "\n",
    "# BGE\n",
    "preprocessor_bge, num_feats_bge, cat_feats_bge = create_preprocessor(X_bge_train)\n",
    "print(f\"\\nBGE OCEAN preprocessor:\")\n",
    "print(f\"  Numeric features: {len(num_feats_bge)}\")\n",
    "print(f\"  Categorical features: {len(cat_feats_bge)}\")\n",
    "\n",
    "print(f\"\\nPreprocessing pipelines created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing data...\\n\")\n",
    "\n",
    "# Baseline\n",
    "print(\"Preprocessing Baseline...\")\n",
    "X_baseline_train_processed = preprocessor_baseline.fit_transform(X_baseline_train)\n",
    "X_baseline_test_processed = preprocessor_baseline.transform(X_baseline_test)\n",
    "print(f\"  Train shape: {X_baseline_train_processed.shape}\")\n",
    "print(f\"  Test shape: {X_baseline_test_processed.shape}\")\n",
    "\n",
    "# Ridge (if available)\n",
    "if has_ridge:\n",
    "    print(\"\\nPreprocessing Ridge OCEAN...\")\n",
    "    X_ridge_train_processed = preprocessor_ridge.fit_transform(X_ridge_train)\n",
    "    X_ridge_test_processed = preprocessor_ridge.transform(X_ridge_test)\n",
    "    print(f\"  Train shape: {X_ridge_train_processed.shape}\")\n",
    "    print(f\"  Test shape: {X_ridge_test_processed.shape}\")\n",
    "\n",
    "# BGE\n",
    "print(\"\\nPreprocessing BGE OCEAN...\")\n",
    "X_bge_train_processed = preprocessor_bge.fit_transform(X_bge_train)\n",
    "X_bge_test_processed = preprocessor_bge.transform(X_bge_test)\n",
    "print(f\"  Train shape: {X_bge_train_processed.shape}\")\n",
    "print(f\"  Test shape: {X_bge_test_processed.shape}\")\n",
    "\n",
    "print(\"\\nData preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train XGBoost Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weight\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Class weight (scale_pos_weight): {scale_pos_weight:.2f}\\n\")\n",
    "\n",
    "# Update XGBoost parameters with class weight\n",
    "xgb_params = CONFIG['xgboost_params'].copy()\n",
    "xgb_params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "# Storage for models and results\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "def train_and_evaluate(name, X_train, X_test, y_train, y_test, params):\n",
    "    \"\"\"\n",
    "    Train XGBoost model and evaluate performance.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model, predictions, and metrics\n",
    "    \"\"\"\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        },\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Training XGBoost Models\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Train Baseline\n",
    "results['baseline'] = train_and_evaluate(\n",
    "    'Baseline (no OCEAN)',\n",
    "    X_baseline_train_processed,\n",
    "    X_baseline_test_processed,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    xgb_params\n",
    ")\n",
    "\n",
    "# Train Ridge (if available)\n",
    "if has_ridge:\n",
    "    print()\n",
    "    results['ridge'] = train_and_evaluate(\n",
    "        'Ridge-Weighted OCEAN',\n",
    "        X_ridge_train_processed,\n",
    "        X_ridge_test_processed,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        xgb_params\n",
    "    )\n",
    "else:\n",
    "    # Load previous Ridge results\n",
    "    print(\"\\nLoading previous Ridge OCEAN results...\")\n",
    "    try:\n",
    "        with open(CONFIG['ridge_results'], 'r') as f:\n",
    "            ridge_data = json.load(f)\n",
    "        \n",
    "        results['ridge'] = {\n",
    "            'model': None,\n",
    "            'y_pred': None,\n",
    "            'y_pred_proba': None,\n",
    "            'metrics': {\n",
    "                'accuracy': ridge_data['ocean']['test_acc'],\n",
    "                'precision': ridge_data['ocean']['precision'],\n",
    "                'recall': ridge_data['ocean']['recall'],\n",
    "                'f1': ridge_data['ocean']['f1'],\n",
    "                'auc': ridge_data['ocean']['auc']\n",
    "            },\n",
    "            'confusion_matrix': None\n",
    "        }\n",
    "        \n",
    "        print(f\"  AUC (from previous run): {results['ridge']['metrics']['auc']:.4f}\")\n",
    "        print(f\"  Note: Using metrics from previous experiment\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load Ridge results: {e}\")\n",
    "        results['ridge'] = None\n",
    "\n",
    "# Train BGE\n",
    "print()\n",
    "results['bge'] = train_and_evaluate(\n",
    "    'BGE + ElasticNet OCEAN',\n",
    "    X_bge_train_processed,\n",
    "    X_bge_test_processed,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    xgb_params\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Model Training Complete\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
