{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05f - Prepare Cross-Encoder Training Data (25 Models)\n",
    "\n",
    "**Purpose**: Prepare training data for Cross-Encoder LoRA fine-tuning on HF AutoTrain\n",
    "\n",
    "**Task**: \n",
    "- Transform OCEAN prediction into a text-pair regression task\n",
    "- Query: OCEAN dimension definition\n",
    "- Document: Loan description\n",
    "- Label: OCEAN score (0-1)\n",
    "\n",
    "**Training Strategy**:\n",
    "- 5 LLMs × 5 OCEAN dimensions = **25 separate models**\n",
    "- Each model fine-tuned independently for best performance\n",
    "- LoRA fine-tuning for parameter efficiency\n",
    "\n",
    "**Output**:\n",
    "- 25 CSV files ready for HF AutoTrain upload\n",
    "- Each file: 500 rows × 3 columns (text_1, text_2, label)\n",
    "- Total data for training: 12,500 text pairs\n",
    "\n",
    "**Next Step**: Upload to HF AutoTrain for LoRA fine-tuning\n",
    "\n",
    "**Estimated Time**: 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define OCEAN Dimension Definitions\n",
    "\n",
    "These definitions will be used as the \"query\" in text-pair classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCEAN_DEFINITIONS = {\n",
    "    'openness': \"This person is imaginative, creative, curious about new experiences, and open to new ideas. They appreciate art, emotion, adventure, unusual ideas, and variety of experience.\",\n",
    "    \n",
    "    'conscientiousness': \"This person is organized, responsible, hardworking, reliable, and goal-oriented. They show self-discipline, act dutifully, and aim for achievement against measures or outside expectations.\",\n",
    "    \n",
    "    'extraversion': \"This person is outgoing, energetic, talkative, sociable, and enjoys being around others. They seek stimulation in the company of others and are assertive and enthusiastic.\",\n",
    "    \n",
    "    'agreeableness': \"This person is friendly, cooperative, compassionate, trusting, and considerate of others. They are generally well-tempered, kind, and value getting along with others.\",\n",
    "    \n",
    "    'neuroticism': \"This person tends to experience negative emotions such as anxiety, anger, or depression. They are emotionally unstable, prone to worry, and have difficulty coping with stress.\"\n",
    "}\n",
    "\n",
    "print(\"OCEAN Definitions:\")\n",
    "print(\"=\"*80)\n",
    "for dim, definition in OCEAN_DEFINITIONS.items():\n",
    "    print(f\"\\n{dim.upper()}:\")\n",
    "    print(f\"  {definition}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Loan Descriptions and OCEAN Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load loan descriptions\n",
    "print(\"Loading loan descriptions...\")\n",
    "df_samples = pd.read_csv('../test_samples_500.csv')\n",
    "print(f\"Loaded {len(df_samples)} samples\")\n",
    "print(f\"Columns: {df_samples.columns.tolist()}\")\n",
    "\n",
    "# Extract descriptions\n",
    "loan_descriptions = df_samples['desc'].tolist()\n",
    "print(f\"\\nSample description:\")\n",
    "print(f\"  {loan_descriptions[0][:200]}...\")\n",
    "\n",
    "# LLM configurations\n",
    "LLM_CONFIGS = {\n",
    "    'llama': {\n",
    "        'name': 'Llama-3.1-8B',\n",
    "        'ocean_file': '../ocean_ground_truth/llama_3.1_8b_ocean_500.csv'\n",
    "    },\n",
    "    'gpt': {\n",
    "        'name': 'GPT-OSS-120B',\n",
    "        'ocean_file': '../ocean_ground_truth/gpt_oss_120b_ocean_500.csv'\n",
    "    },\n",
    "    'gemma': {\n",
    "        'name': 'Gemma-2-9B',\n",
    "        'ocean_file': '../ocean_ground_truth/gemma_2_9b_ocean_500.csv'\n",
    "    },\n",
    "    'deepseek': {\n",
    "        'name': 'DeepSeek-V3.1',\n",
    "        'ocean_file': '../ocean_ground_truth/deepseek_v3.1_ocean_500.csv'\n",
    "    },\n",
    "    'qwen': {\n",
    "        'name': 'Qwen-2.5-72B',\n",
    "        'ocean_file': '../ocean_ground_truth/qwen_2.5_72b_ocean_500.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "OCEAN_DIMS = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  LLM models: {len(LLM_CONFIGS)}\")\n",
    "print(f\"  OCEAN dimensions: {len(OCEAN_DIMS)}\")\n",
    "print(f\"  Total models to train: {len(LLM_CONFIGS) * len(OCEAN_DIMS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Training Data Files\n",
    "\n",
    "**Format for Cross-Encoder Regression**:\n",
    "```\n",
    "text_1,text_2,label\n",
    "\"OCEAN definition\",\"loan description\",0.75\n",
    "...\n",
    "```\n",
    "\n",
    "**File naming**: `crossencoder_train_{llm}_{dimension}.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = '../crossencoder_training_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Storage for metadata\n",
    "training_files_metadata = []\n",
    "total_files = 0\n",
    "total_rows = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generating Training Data Files\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for llm_key, llm_config in LLM_CONFIGS.items():\n",
    "    print(f\"\\n[{llm_key.upper()}] {llm_config['name']}\")\n",
    "    \n",
    "    # Load OCEAN ground truth\n",
    "    ocean_df = pd.read_csv(llm_config['ocean_file'])\n",
    "    \n",
    "    # Check for NaN and drop if necessary\n",
    "    initial_len = len(ocean_df)\n",
    "    ocean_df = ocean_df.dropna()\n",
    "    if len(ocean_df) < initial_len:\n",
    "        print(f\"  Warning: Dropped {initial_len - len(ocean_df)} rows with NaN values\")\n",
    "    \n",
    "    # Ensure descriptions match\n",
    "    if len(ocean_df) != len(loan_descriptions):\n",
    "        # Adjust loan descriptions\n",
    "        valid_descriptions = [loan_descriptions[i] for i in ocean_df.index]\n",
    "    else:\n",
    "        valid_descriptions = loan_descriptions\n",
    "    \n",
    "    for dim in OCEAN_DIMS:\n",
    "        # Create training dataframe\n",
    "        train_data = pd.DataFrame({\n",
    "            'text_1': [OCEAN_DEFINITIONS[dim]] * len(ocean_df),  # Query: OCEAN definition\n",
    "            'text_2': valid_descriptions,  # Document: loan description\n",
    "            'label': ocean_df[dim].values  # Target: OCEAN score (0-1)\n",
    "        })\n",
    "        \n",
    "        # Save to CSV\n",
    "        filename = f'crossencoder_train_{llm_key}_{dim}.csv'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        train_data.to_csv(filepath, index=False)\n",
    "        \n",
    "        # Collect metadata\n",
    "        file_size = os.path.getsize(filepath) / 1024  # KB\n",
    "        training_files_metadata.append({\n",
    "            'llm_key': llm_key,\n",
    "            'llm_name': llm_config['name'],\n",
    "            'dimension': dim,\n",
    "            'filename': filename,\n",
    "            'filepath': filepath,\n",
    "            'num_samples': len(train_data),\n",
    "            'file_size_kb': file_size,\n",
    "            'label_mean': float(train_data['label'].mean()),\n",
    "            'label_std': float(train_data['label'].std()),\n",
    "            'label_min': float(train_data['label'].min()),\n",
    "            'label_max': float(train_data['label'].max())\n",
    "        })\n",
    "        \n",
    "        total_files += 1\n",
    "        total_rows += len(train_data)\n",
    "        \n",
    "        print(f\"  [{dim:<18}] {len(train_data):3d} samples | Label: {train_data['label'].mean():.3f} ± {train_data['label'].std():.3f} | {file_size:.1f} KB\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Total files created: {total_files}\")\n",
    "print(f\"Total training samples: {total_rows}\")\n",
    "print(f\"Average samples per file: {total_rows / total_files:.1f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Metadata and Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    'phase': '05f - Cross-Encoder Training Data Preparation',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'base_model': 'cross-encoder/nli-deberta-v3-large',\n",
    "    'task_type': 'text-regression',\n",
    "    'total_models': total_files,\n",
    "    'total_training_samples': total_rows,\n",
    "    'ocean_definitions': OCEAN_DEFINITIONS,\n",
    "    'training_files': training_files_metadata,\n",
    "    'autotrain_config': {\n",
    "        'learning_rate': 2e-5,\n",
    "        'epochs': '3-5',\n",
    "        'batch_size': 8,\n",
    "        'lora': True,\n",
    "        'lora_r': 8,\n",
    "        'lora_alpha': 32,\n",
    "        'train_split': 0.8,\n",
    "        'eval_split': 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_file = os.path.join(output_dir, 'training_data_metadata.json')\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetadata saved: {metadata_file}\")\n",
    "\n",
    "# Create training files list\n",
    "files_list_file = os.path.join(output_dir, 'TRAINING_FILES_LIST.txt')\n",
    "with open(files_list_file, 'w') as f:\n",
    "    f.write(\"Cross-Encoder Training Files\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(f\"Total files: {total_files}\\n\")\n",
    "    f.write(f\"Total samples: {total_rows}\\n\\n\")\n",
    "    f.write(\"File List:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    for i, meta in enumerate(training_files_metadata, 1):\n",
    "        f.write(f\"{i:2d}. {meta['filename']:<50} ({meta['num_samples']} samples, {meta['file_size_kb']:.1f} KB)\\n\")\n",
    "        f.write(f\"    LLM: {meta['llm_name']:<20} | Dimension: {meta['dimension']:<15}\\n\")\n",
    "        f.write(f\"    Label stats: μ={meta['label_mean']:.3f}, σ={meta['label_std']:.3f}, range=[{meta['label_min']:.3f}, {meta['label_max']:.3f}]\\n\\n\")\n",
    "\n",
    "print(f\"Files list saved: {files_list_file}\")\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Files Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'LLM':<20} {'Dimension':<18} {'Samples':<10} {'Label Mean':<12} {'Size (KB)'}\")\n",
    "print(\"-\" * 80)\n",
    "for meta in training_files_metadata[:10]:  # Show first 10\n",
    "    print(f\"{meta['llm_name']:<20} {meta['dimension']:<18} {meta['num_samples']:<10} {meta['label_mean']:<12.3f} {meta['file_size_kb']:.1f}\")\n",
    "print(\"\\n(Showing first 10 of 25 files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data Quality Checks\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load a sample file for verification\n",
    "sample_file = training_files_metadata[0]['filepath']\n",
    "sample_df = pd.read_csv(sample_file)\n",
    "\n",
    "print(f\"\\nSample file: {training_files_metadata[0]['filename']}\")\n",
    "print(f\"Shape: {sample_df.shape}\")\n",
    "print(f\"Columns: {sample_df.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(sample_df.head(3).to_string())\n",
    "\n",
    "# Check for issues\n",
    "issues = []\n",
    "\n",
    "# Check 1: NaN values\n",
    "nan_count = sample_df.isnull().sum().sum()\n",
    "if nan_count > 0:\n",
    "    issues.append(f\"Found {nan_count} NaN values\")\n",
    "\n",
    "# Check 2: Label range\n",
    "if sample_df['label'].min() < 0 or sample_df['label'].max() > 1:\n",
    "    issues.append(f\"Labels out of range [0,1]: [{sample_df['label'].min()}, {sample_df['label'].max()}]\")\n",
    "\n",
    "# Check 3: Empty texts\n",
    "empty_text1 = (sample_df['text_1'].str.len() < 10).sum()\n",
    "empty_text2 = (sample_df['text_2'].str.len() < 10).sum()\n",
    "if empty_text1 > 0 or empty_text2 > 0:\n",
    "    issues.append(f\"Found short texts: text_1={empty_text1}, text_2={empty_text2}\")\n",
    "\n",
    "# Check 4: Duplicate rows\n",
    "duplicates = sample_df.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    issues.append(f\"Found {duplicates} duplicate rows\")\n",
    "\n",
    "if issues:\n",
    "    print(\"\\n⚠️ Quality Issues Found:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"\\n✓ All quality checks passed!\")\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"  Text 1 (OCEAN def) length: {sample_df['text_1'].str.len().mean():.1f} chars\")\n",
    "print(f\"  Text 2 (loan desc) length: {sample_df['text_2'].str.len().mean():.1f} ± {sample_df['text_2'].str.len().std():.1f} chars\")\n",
    "print(f\"  Label distribution: {sample_df['label'].describe().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Data Preparation Complete!**\n",
    "\n",
    "**Output**:\n",
    "- 📁 Directory: `../crossencoder_training_data/`\n",
    "- 📄 Training files: 25 CSV files (5 LLMs × 5 OCEAN dimensions)\n",
    "- 📊 Total training samples: ~12,500\n",
    "- 📝 Metadata: `training_data_metadata.json`\n",
    "- 📋 Files list: `TRAINING_FILES_LIST.txt`\n",
    "\n",
    "**Next Steps**:\n",
    "\n",
    "1. **Upload to Hugging Face**:\n",
    "   - Go to: https://huggingface.co/spaces/autotrain-projects/autotrain-advanced\n",
    "   - Upload the 25 CSV files\n",
    "\n",
    "2. **Configure AutoTrain** (for each file):\n",
    "   ```\n",
    "   Task: Text Regression\n",
    "   Base Model: cross-encoder/nli-deberta-v3-large\n",
    "   Learning Rate: 2e-5\n",
    "   Epochs: 3-5\n",
    "   Batch Size: 8\n",
    "   LoRA: Enable\n",
    "   LoRA r: 8\n",
    "   LoRA alpha: 32\n",
    "   Train/Eval Split: 80/20\n",
    "   ```\n",
    "\n",
    "3. **Cost Estimate**:\n",
    "   - ~$2-3 per model\n",
    "   - Total: $50-75 for 25 models\n",
    "\n",
    "4. **After Training**:\n",
    "   - Models will be saved to your HF account\n",
    "   - Run `05f_crossencoder_lora_evaluate.ipynb` to evaluate\n",
    "\n",
    "**See**: `HF_AUTOTRAIN_GUIDE.md` for detailed AutoTrain instructions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
