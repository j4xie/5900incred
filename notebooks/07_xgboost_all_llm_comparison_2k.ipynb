{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - XGBoost All LLM Comparison (2K Sample)\n",
    "\n",
    "**Purpose**: Compare XGBoost performance across 6 models to identify best LLM ground truth\n",
    "\n",
    "## Models to Compare:\n",
    "\n",
    "1. **Baseline**: XGBoost without OCEAN features (36 base features)\n",
    "2. **Llama OCEAN**: Base features + Llama ground truth OCEAN (5 features)\n",
    "3. **GPT OCEAN**: Base features + GPT ground truth OCEAN (5 features)\n",
    "4. **Qwen OCEAN**: Base features + Qwen ground truth OCEAN (5 features)\n",
    "5. **Gemma OCEAN**: Base features + Gemma ground truth OCEAN (5 features)\n",
    "6. **DeepSeek OCEAN**: Base features + DeepSeek ground truth OCEAN (5 features)\n",
    "\n",
    "## Evaluation Metrics:\n",
    "\n",
    "- **Primary**: ROC-AUC\n",
    "- **Secondary**: Precision, Recall, F1-Score\n",
    "- **Analysis**: Feature importance, ROC curves, statistical tests\n",
    "\n",
    "## Expected Outcomes:\n",
    "\n",
    "- Identify which LLM ground truth produces best OCEAN features for loan default prediction\n",
    "- Determine if OCEAN features improve prediction compared to baseline\n",
    "\n",
    "**Estimated Time**: 15-20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Input file (from 05g notebook)\n",
    "    'input_data': '../loan_2k_with_all_ocean.csv',\n",
    "    \n",
    "    # Output files\n",
    "    'output_comparison': '../07_xgboost_llm_comparison_2k.csv',\n",
    "    'output_visualization': '../07_xgboost_llm_comparison_2k.png',\n",
    "    'output_report': '../07_xgboost_llm_comparison_2k.json',\n",
    "    \n",
    "    # LLM models to compare\n",
    "    'llm_models': ['llama', 'gpt', 'qwen', 'gemma', 'deepseek'],\n",
    "    \n",
    "    # OCEAN dimensions\n",
    "    'ocean_dims': ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'],\n",
    "    \n",
    "    # Model parameters\n",
    "    'xgboost_params': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'eval_metric': 'logloss',\n",
    "        'early_stopping_rounds': 10\n",
    "    },\n",
    "    \n",
    "    # Train/test split\n",
    "    'test_size': 0.2,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    \n",
    "    # Features to remove\n",
    "    'remove_features': ['emp_title', 'title', 'earliest_cr_line', 'desc']\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Input data: {CONFIG['input_data']}\")\n",
    "print(f\"  LLM models: {CONFIG['llm_models']}\")\n",
    "print(f\"  OCEAN dimensions: {CONFIG['ocean_dims']}\")\n",
    "print(f\"  Test size: {CONFIG['test_size']*100:.0f}%\")\n",
    "print(f\"\\nXGBoost parameters: {CONFIG['xgboost_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(CONFIG['input_data'], low_memory=False)\n",
    "print(f\"  Rows: {len(df):,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "\n",
    "# Check target column\n",
    "if 'target' not in df.columns:\n",
    "    raise ValueError(\"Missing target column!\")\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Default rate: {df['target'].mean()*100:.2f}%\")\n",
    "\n",
    "# Verify all OCEAN columns exist\n",
    "print(f\"\\nVerifying OCEAN columns...\")\n",
    "for llm in CONFIG['llm_models']:\n",
    "    ocean_cols = [f\"{llm}_{dim}\" for dim in CONFIG['ocean_dims']]\n",
    "    missing = [col for col in ocean_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for {llm.upper()}: {missing}\")\n",
    "    print(f\"  {llm.upper()}: {len(ocean_cols)} columns found\")\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Feature Sets\n",
    "\n",
    "Create 6 feature sets:\n",
    "- Baseline: Base features only (no OCEAN)\n",
    "- Llama: Base features + llama_* OCEAN\n",
    "- GPT: Base features + gpt_* OCEAN\n",
    "- Qwen: Base features + qwen_* OCEAN\n",
    "- Gemma: Base features + gemma_* OCEAN\n",
    "- DeepSeek: Base features + deepseek_* OCEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "y = df['target']\n",
    "X_all = df.drop(columns=['target'], errors='ignore')\n",
    "\n",
    "# Remove high cardinality features\n",
    "X_all = X_all.drop(columns=CONFIG['remove_features'], errors='ignore')\n",
    "\n",
    "print(f\"Full feature set shape: {X_all.shape}\")\n",
    "\n",
    "# Identify all OCEAN columns\n",
    "all_ocean_cols = []\n",
    "for llm in CONFIG['llm_models']:\n",
    "    for dim in CONFIG['ocean_dims']:\n",
    "        all_ocean_cols.append(f\"{llm}_{dim}\")\n",
    "\n",
    "print(f\"Total OCEAN columns: {len(all_ocean_cols)}\")\n",
    "\n",
    "# 1. Baseline: Remove ALL OCEAN features\n",
    "X_baseline = X_all.drop(columns=all_ocean_cols, errors='ignore')\n",
    "print(f\"\\n1. Baseline features: {X_baseline.shape[1]} features\")\n",
    "\n",
    "# 2-6. Each LLM: Base + specific LLM OCEAN features\n",
    "X_llm_sets = {}\n",
    "\n",
    "for i, llm in enumerate(CONFIG['llm_models'], start=2):\n",
    "    llm_ocean_cols = [f\"{llm}_{dim}\" for dim in CONFIG['ocean_dims']]\n",
    "    X_llm = X_baseline.copy()\n",
    "    \n",
    "    # Add LLM-specific OCEAN features\n",
    "    for col in llm_ocean_cols:\n",
    "        X_llm[col] = X_all[col]\n",
    "    \n",
    "    X_llm_sets[llm] = X_llm\n",
    "    print(f\"{i}. {llm.upper()} OCEAN features: {X_llm.shape[1]} features ({len(llm_ocean_cols)} OCEAN added)\")\n",
    "\n",
    "print(f\"\\nFeature sets prepared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train/Test Split\n",
    "\n",
    "Use consistent split across all models for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single train/test split\n",
    "print(\"Performing train/test split (80/20)...\\n\")\n",
    "\n",
    "# Get train/test indices\n",
    "indices = np.arange(len(y))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Split target\n",
    "y_train = y.iloc[train_idx]\n",
    "y_test = y.iloc[test_idx]\n",
    "\n",
    "# Split baseline\n",
    "X_baseline_train = X_baseline.iloc[train_idx]\n",
    "X_baseline_test = X_baseline.iloc[test_idx]\n",
    "\n",
    "# Split each LLM set\n",
    "X_llm_train = {}\n",
    "X_llm_test = {}\n",
    "\n",
    "for llm, X_llm in X_llm_sets.items():\n",
    "    X_llm_train[llm] = X_llm.iloc[train_idx]\n",
    "    X_llm_test[llm] = X_llm.iloc[test_idx]\n",
    "\n",
    "print(f\"Training set: {len(y_train):,} samples ({len(y_train)/len(y)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(y_test):,} samples ({len(y_test)/len(y)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain default rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Test default rate: {y_test.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(X):\n",
    "    \"\"\"\n",
    "    Create preprocessing pipeline for given feature set.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature dataframe\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (preprocessor, numeric_features, categorical_features)\n",
    "    \"\"\"\n",
    "    # Identify feature types\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Numeric preprocessing\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Categorical preprocessing\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Combined preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    return preprocessor, numeric_features, categorical_features\n",
    "\n",
    "\n",
    "# Create preprocessors\n",
    "print(\"Creating preprocessing pipelines...\\n\")\n",
    "\n",
    "preprocessors = {}\n",
    "\n",
    "# Baseline\n",
    "preprocessor_baseline, num_feats, cat_feats = create_preprocessor(X_baseline_train)\n",
    "preprocessors['baseline'] = preprocessor_baseline\n",
    "print(f\"Baseline preprocessor:\")\n",
    "print(f\"  Numeric features: {len(num_feats)}\")\n",
    "print(f\"  Categorical features: {len(cat_feats)}\")\n",
    "\n",
    "# Each LLM\n",
    "for llm in CONFIG['llm_models']:\n",
    "    preprocessor_llm, num_feats, cat_feats = create_preprocessor(X_llm_train[llm])\n",
    "    preprocessors[llm] = preprocessor_llm\n",
    "    print(f\"\\n{llm.upper()} preprocessor:\")\n",
    "    print(f\"  Numeric features: {len(num_feats)}\")\n",
    "    print(f\"  Categorical features: {len(cat_feats)}\")\n",
    "\n",
    "print(f\"\\nPreprocessing pipelines created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing data...\\n\")\n",
    "\n",
    "# Storage for processed data\n",
    "X_processed = {}\n",
    "\n",
    "# Baseline\n",
    "print(\"Preprocessing Baseline...\")\n",
    "X_baseline_train_processed = preprocessors['baseline'].fit_transform(X_baseline_train)\n",
    "X_baseline_test_processed = preprocessors['baseline'].transform(X_baseline_test)\n",
    "X_processed['baseline'] = (X_baseline_train_processed, X_baseline_test_processed)\n",
    "print(f\"  Train shape: {X_baseline_train_processed.shape}\")\n",
    "print(f\"  Test shape: {X_baseline_test_processed.shape}\")\n",
    "\n",
    "# Each LLM\n",
    "for llm in CONFIG['llm_models']:\n",
    "    print(f\"\\nPreprocessing {llm.upper()}...\")\n",
    "    X_train_processed = preprocessors[llm].fit_transform(X_llm_train[llm])\n",
    "    X_test_processed = preprocessors[llm].transform(X_llm_test[llm])\n",
    "    X_processed[llm] = (X_train_processed, X_test_processed)\n",
    "    print(f\"  Train shape: {X_train_processed.shape}\")\n",
    "    print(f\"  Test shape: {X_test_processed.shape}\")\n",
    "\n",
    "print(\"\\nData preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train XGBoost Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weight\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Class weight (scale_pos_weight): {scale_pos_weight:.2f}\\n\")\n",
    "\n",
    "# Update XGBoost parameters\n",
    "xgb_params = CONFIG['xgboost_params'].copy()\n",
    "xgb_params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "# Storage for results\n",
    "results = {}\n",
    "\n",
    "\n",
    "def train_and_evaluate(name, X_train, X_test, y_train, y_test, params):\n",
    "    \"\"\"\n",
    "    Train XGBoost model and evaluate performance.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model, predictions, and metrics\n",
    "    \"\"\"\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'metrics': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1': float(f1),\n",
    "            'auc': float(auc)\n",
    "        },\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'name': name\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Training XGBoost Models\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Train Baseline\n",
    "X_train_proc, X_test_proc = X_processed['baseline']\n",
    "results['baseline'] = train_and_evaluate(\n",
    "    'Baseline (No OCEAN)',\n",
    "    X_train_proc, X_test_proc,\n",
    "    y_train, y_test,\n",
    "    xgb_params\n",
    ")\n",
    "\n",
    "# Train each LLM model\n",
    "for llm in CONFIG['llm_models']:\n",
    "    print()\n",
    "    X_train_proc, X_test_proc = X_processed[llm]\n",
    "    results[llm] = train_and_evaluate(\n",
    "        f'{llm.upper()} OCEAN',\n",
    "        X_train_proc, X_test_proc,\n",
    "        y_train, y_test,\n",
    "        xgb_params\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Model Training Complete\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for model_key, result in results.items():\n",
    "    row = {\n",
    "        'Model': result['name'],\n",
    "        'AUC': result['metrics']['auc'],\n",
    "        'Accuracy': result['metrics']['accuracy'],\n",
    "        'Precision': result['metrics']['precision'],\n",
    "        'Recall': result['metrics']['recall'],\n",
    "        'F1': result['metrics']['f1']\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Calculate improvement vs baseline\n",
    "baseline_auc = results['baseline']['metrics']['auc']\n",
    "df_comparison['AUC_vs_Baseline'] = df_comparison['AUC'] - baseline_auc\n",
    "df_comparison['AUC_Improvement_%'] = (df_comparison['AUC_vs_Baseline'] / baseline_auc * 100)\n",
    "\n",
    "# Sort by AUC descending\n",
    "df_comparison = df_comparison.sort_values('AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "df_comparison.to_csv(CONFIG['output_comparison'], index=False)\n",
    "print(f\"\\nComparison saved to: {CONFIG['output_comparison']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Statistical Analysis\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Identify best LLM model\n",
    "best_llm = df_comparison[df_comparison['Model'] != 'Baseline (No OCEAN)'].iloc[0]['Model'].split()[0].lower()\n",
    "best_auc = results[best_llm]['metrics']['auc']\n",
    "\n",
    "print(f\"Best LLM Model: {best_llm.upper()}\")\n",
    "print(f\"  AUC: {best_auc:.4f}\")\n",
    "print(f\"  Improvement vs Baseline: {(best_auc - baseline_auc):.4f} ({(best_auc - baseline_auc)/baseline_auc*100:.2f}%)\")\n",
    "\n",
    "# Paired t-test for prediction probabilities\n",
    "print(f\"\\nPaired t-test: Best LLM ({best_llm.upper()}) vs Baseline\")\n",
    "baseline_proba = results['baseline']['y_pred_proba']\n",
    "best_proba = results[best_llm]['y_pred_proba']\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(best_proba, baseline_proba)\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"  Result: Statistically significant difference (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"  Result: No statistically significant difference (p >= 0.05)\")\n",
    "\n",
    "# Compare all LLMs\n",
    "print(f\"\\nPairwise AUC differences vs Baseline:\")\n",
    "for llm in CONFIG['llm_models']:\n",
    "    llm_auc = results[llm]['metrics']['auc']\n",
    "    diff = llm_auc - baseline_auc\n",
    "    pct = diff / baseline_auc * 100\n",
    "    print(f\"  {llm.upper()}: {diff:+.4f} ({pct:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. AUC Bar Chart\n",
    "ax1 = axes[0, 0]\n",
    "models = df_comparison['Model'].tolist()\n",
    "aucs = df_comparison['AUC'].tolist()\n",
    "colors = ['#1f77b4' if 'Baseline' in m else '#ff7f0e' for m in models]\n",
    "\n",
    "bars = ax1.barh(range(len(models)), aucs, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_yticks(range(len(models)))\n",
    "ax1.set_yticklabels(models)\n",
    "ax1.set_xlabel('ROC-AUC', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('XGBoost Performance Comparison\\n(2K Sample, 6 Models)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, auc) in enumerate(zip(bars, aucs)):\n",
    "    ax1.text(auc + 0.002, bar.get_y() + bar.get_height()/2, \n",
    "             f'{auc:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# Add baseline reference line\n",
    "ax1.axvline(baseline_auc, color='red', linestyle='--', linewidth=1.5, \n",
    "            label=f'Baseline: {baseline_auc:.4f}', alpha=0.7)\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "\n",
    "# 2. AUC Improvement Bar Chart\n",
    "ax2 = axes[0, 1]\n",
    "llm_rows = df_comparison[df_comparison['Model'] != 'Baseline (No OCEAN)']\n",
    "llm_models = llm_rows['Model'].tolist()\n",
    "improvements = llm_rows['AUC_vs_Baseline'].tolist()\n",
    "colors_imp = ['green' if x > 0 else 'red' for x in improvements]\n",
    "\n",
    "bars2 = ax2.barh(range(len(llm_models)), improvements, color=colors_imp, alpha=0.7, edgecolor='black')\n",
    "ax2.set_yticks(range(len(llm_models)))\n",
    "ax2.set_yticklabels(llm_models)\n",
    "ax2.set_xlabel('AUC Improvement vs Baseline', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('OCEAN Feature Impact\\n(AUC Delta from Baseline)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, imp in zip(bars2, improvements):\n",
    "    x_pos = imp + 0.001 if imp > 0 else imp - 0.001\n",
    "    ha = 'left' if imp > 0 else 'right'\n",
    "    ax2.text(x_pos, bar.get_y() + bar.get_height()/2, \n",
    "             f'{imp:+.4f}', va='center', ha=ha, fontsize=9)\n",
    "\n",
    "\n",
    "# 3. ROC Curves\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "# Baseline ROC\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, results['baseline']['y_pred_proba'])\n",
    "ax3.plot(fpr_base, tpr_base, label=f\"Baseline (AUC={baseline_auc:.4f})\", \n",
    "         linewidth=2.5, linestyle='--', color='blue')\n",
    "\n",
    "# Each LLM ROC\n",
    "colors_roc = plt.cm.Set2(np.linspace(0, 1, len(CONFIG['llm_models'])))\n",
    "for llm, color in zip(CONFIG['llm_models'], colors_roc):\n",
    "    fpr, tpr, _ = roc_curve(y_test, results[llm]['y_pred_proba'])\n",
    "    auc_val = results[llm]['metrics']['auc']\n",
    "    ax3.plot(fpr, tpr, label=f\"{llm.upper()} (AUC={auc_val:.4f})\", \n",
    "             linewidth=2, color=color)\n",
    "\n",
    "# Diagonal reference\n",
    "ax3.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.3)\n",
    "\n",
    "ax3.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('ROC Curves Comparison', fontsize=12, fontweight='bold')\n",
    "ax3.legend(loc='lower right', fontsize=8)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "\n",
    "# 4. All Metrics Comparison\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "metrics = ['AUC', 'Accuracy', 'Precision', 'Recall', 'F1']\n",
    "x_pos = np.arange(len(metrics))\n",
    "width = 0.12\n",
    "\n",
    "# Plot baseline\n",
    "baseline_vals = [results['baseline']['metrics'][m.lower()] for m in metrics]\n",
    "ax4.bar(x_pos - 2.5*width, baseline_vals, width, label='Baseline', \n",
    "        color='blue', alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Plot each LLM\n",
    "for i, (llm, color) in enumerate(zip(CONFIG['llm_models'], colors_roc)):\n",
    "    llm_vals = [results[llm]['metrics'][m.lower()] for m in metrics]\n",
    "    offset = (i - 2) * width + 0.5 * width\n",
    "    ax4.bar(x_pos + offset, llm_vals, width, label=llm.upper(), \n",
    "            color=color, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax4.set_xlabel('Metrics', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('All Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(metrics)\n",
    "ax4.legend(loc='lower right', fontsize=8, ncol=2)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_visualization'], dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nVisualization saved to: {CONFIG['output_visualization']}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "report = {\n",
    "    'experiment': '07_xgboost_all_llm_comparison_2k',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset': {\n",
    "        'total_samples': len(df),\n",
    "        'train_samples': len(y_train),\n",
    "        'test_samples': len(y_test),\n",
    "        'default_rate': float(y.mean())\n",
    "    },\n",
    "    'models': {},\n",
    "    'best_model': {\n",
    "        'name': best_llm.upper(),\n",
    "        'auc': float(best_auc),\n",
    "        'improvement_vs_baseline': float(best_auc - baseline_auc),\n",
    "        'improvement_percent': float((best_auc - baseline_auc) / baseline_auc * 100)\n",
    "    },\n",
    "    'statistical_test': {\n",
    "        't_statistic': float(t_stat),\n",
    "        'p_value': float(p_value),\n",
    "        'significant': p_value < 0.05\n",
    "    },\n",
    "    'comparison_table': df_comparison.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "# Add individual model results\n",
    "for model_key, result in results.items():\n",
    "    report['models'][model_key] = {\n",
    "        'name': result['name'],\n",
    "        'metrics': result['metrics'],\n",
    "        'confusion_matrix': result['confusion_matrix']\n",
    "    }\n",
    "\n",
    "# Save report\n",
    "with open(CONFIG['output_report'], 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest Performing Model: {best_llm.upper()} OCEAN\")\n",
    "print(f\"  AUC: {best_auc:.4f}\")\n",
    "print(f\"  Baseline AUC: {baseline_auc:.4f}\")\n",
    "print(f\"  Improvement: {(best_auc - baseline_auc):.4f} ({(best_auc - baseline_auc)/baseline_auc*100:.2f}%)\")\n",
    "print(f\"\\nStatistical Significance: {'Yes' if p_value < 0.05 else 'No'} (p={p_value:.4f})\")\n",
    "\n",
    "print(f\"\\nRanking by AUC:\")\n",
    "for i, row in df_comparison.iterrows():\n",
    "    print(f\"  {i+1}. {row['Model']}: {row['AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\nReport saved to: {CONFIG['output_report']}\")\n",
    "print(f\"\\nAnalysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
