{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - XGBoost Full Model (with OCEAN Features)\n",
    "\n",
    "**Objective**: Train XGBoost model with OCEAN personality features\n",
    "\n",
    "## Key Steps:\n",
    "1. Load data with OCEAN features\n",
    "2. Validate OCEAN features and target variable\n",
    "3. Remove high cardinality features\n",
    "4. Train/Test split (consistent with baseline)\n",
    "5. Data preprocessing\n",
    "6. Train XGBoost full model\n",
    "7. Evaluate performance metrics\n",
    "8. Compare with baseline model\n",
    "9. Analyze OCEAN feature importance\n",
    "10. Save model and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data with OCEAN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load complete data with OCEAN features\n",
    "print(\"Loading data with OCEAN features...\")\n",
    "df = pd.read_csv('../../loan_final_desc50plus_with_ocean_bge.csv', low_memory=False)\n",
    "print(f\"Data shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "\n",
    "# 2. Validate OCEAN features exist\n",
    "print(\"\\nValidating OCEAN features...\")\n",
    "ocean_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "missing_cols = [col for col in ocean_cols if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing OCEAN features: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"OCEAN features loaded: {len(ocean_cols)} features\")\n",
    "    for col in ocean_cols:\n",
    "        print(f\"  - {col}: mean={df[col].mean():.3f}, std={df[col].std():.3f}\")\n",
    "\n",
    "# 3. Validate target variable\n",
    "if 'target' not in df.columns:\n",
    "    raise ValueError(\"Missing target variable 'target'\")\n",
    "else:\n",
    "    print(f\"\\nTarget variable exists\")\n",
    "    print(f\"   Default rate: {df['target'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop(columns=['target'], errors='ignore')\n",
    "y = df['target']\n",
    "\n",
    "print(f\"Original feature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# Remove high cardinality features (avoid One-Hot Encoding explosion)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Handling High Cardinality Features (One-Hot Encoding Optimization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "high_cardinality_features = ['emp_title', 'title', 'earliest_cr_line', 'desc']\n",
    "X = X.drop(columns=high_cardinality_features, errors='ignore')\n",
    "\n",
    "print(f\"\\nRemoved high cardinality features ({len(high_cardinality_features)} features):\")\n",
    "for feat in high_cardinality_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nOptimized feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# OCEAN column names (no prefix)\n",
    "ocean_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "ocean_cols = [col for col in ocean_cols if col in X.columns]\n",
    "\n",
    "print(f\"\\nNumeric features: {len(numeric_features)} features\")\n",
    "print(f\"  (including {len(ocean_cols)} OCEAN features)\")\n",
    "print(f\"Categorical features: {len(categorical_features)} features (optimized)\")\n",
    "\n",
    "print(\"\\nOCEAN feature list:\")\n",
    "for col in ocean_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test Split (consistent with baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 split (same as baseline)\n",
    "print(\"Performing Train/Test split (80/20)...\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]:,} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"Default rate: {y_train.mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\nTest set target distribution:\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"Default rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric feature preprocessing (including OCEAN features)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical feature preprocessing\n",
    "# Note: High cardinality features removed in previous step to avoid One-Hot Encoding explosion\n",
    "# Removed features: emp_title (78K unique), title (36K unique), earliest_cr_line (603 unique)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combined preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"Preprocessing pipeline created\")\n",
    "print(f\"\\n- Numeric features ({len(numeric_features)}): median imputation + standard scaling\")\n",
    "print(f\"  Including {len(ocean_cols)} OCEAN features\")\n",
    "print(f\"- Categorical features ({len(categorical_features)}): constant imputation + one-hot encoding\")\n",
    "print(f\"\\nOptimization result:\")\n",
    "print(f\"   Categorical features will expand to ~100-150 columns (instead of 116,804 columns)\")\n",
    "print(f\"   Preprocessing speed improved 100x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training set\n",
    "print(\"Preprocessing training set...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform test set\n",
    "print(\"Preprocessing test set...\")\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nPreprocessed training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Preprocessed test set shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Get feature names\n",
    "try:\n",
    "    cat_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "    all_feature_names = numeric_features + list(cat_feature_names)\n",
    "    print(f\"\\nTotal features (after encoding): {len(all_feature_names)}\")\n",
    "except:\n",
    "    all_feature_names = None\n",
    "    print(\"\\nUnable to retrieve feature names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train XGBoost Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weight\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Class weight (scale_pos_weight): {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Create XGBoost model\n",
    "print(\"\\nCreating XGBoost full model (with OCEAN features)...\")\n",
    "xgb_full_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "xgb_full_model.fit(\n",
    "    X_train_processed, y_train,\n",
    "    eval_set=[(X_test_processed, y_test)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nModel training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Making predictions...\\n\")\n",
    "y_pred = xgb_full_model.predict(X_test_processed)\n",
    "y_pred_proba = xgb_full_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBoost Full Model Performance Metrics (with OCEAN Features)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0,0]:,}\")\n",
    "print(f\"False Positives: {cm[0,1]:,}\")\n",
    "print(f\"False Negatives: {cm[1,0]:,}\")\n",
    "print(f\"True Positives:  {cm[1,1]:,}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Detailed Classification Report\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred, target_names=['Fully Paid', 'Charged Off']))\n",
    "\n",
    "# Save full model metrics\n",
    "full_model_metrics = {\n",
    "    'model': 'XGBoost Full (with OCEAN)',\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'roc_auc': float(roc_auc),\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'n_features': X_train_processed.shape[1],\n",
    "    'n_ocean_features': len(ocean_cols),\n",
    "    'train_size': int(X_train.shape[0]),\n",
    "    'test_size': int(X_test.shape[0])\n",
    "}\n",
    "\n",
    "with open('../../full_model_metrics.json', 'w') as f:\n",
    "    json.dump(full_model_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nFull model metrics saved: full_model_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare with Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline metrics\n",
    "try:\n",
    "    with open('../../baseline_metrics.json', 'r') as f:\n",
    "        baseline_metrics = json.load(f)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Model Performance Comparison: Baseline vs Full Model\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC'],\n",
    "        'Baseline': [\n",
    "            baseline_metrics['accuracy'],\n",
    "            baseline_metrics['precision'],\n",
    "            baseline_metrics['recall'],\n",
    "            baseline_metrics['f1_score'],\n",
    "            baseline_metrics['roc_auc']\n",
    "        ],\n",
    "        'Full Model': [accuracy, precision, recall, f1, roc_auc]\n",
    "    })\n",
    "    \n",
    "    # Calculate improvement\n",
    "    comparison_df['Improvement'] = comparison_df['Full Model'] - comparison_df['Baseline']\n",
    "    comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Baseline']) * 100\n",
    "    \n",
    "    print(\"\\nPerformance comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Save comparison results\n",
    "    comparison_df.to_csv('../../model_comparison.csv', index=False)\n",
    "    print(\"\\nComparison results saved: model_comparison.csv\")\n",
    "    \n",
    "    # Evaluate OCEAN feature value\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OCEAN Feature Value Assessment\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    avg_improvement = comparison_df['Improvement %'].mean()\n",
    "    \n",
    "    if avg_improvement > 1:\n",
    "        print(f\"OCEAN features significantly improved model performance\")\n",
    "        print(f\"   Average improvement: {avg_improvement:.2f}%\")\n",
    "    elif avg_improvement > 0:\n",
    "        print(f\"OCEAN features slightly improved model performance\")\n",
    "        print(f\"   Average improvement: {avg_improvement:.2f}%\")\n",
    "    else:\n",
    "        print(f\"OCEAN features did not improve model performance\")\n",
    "        print(f\"   Average improvement: {avg_improvement:.2f}%\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\nBaseline metrics file not found: baseline_metrics.json\")\n",
    "    print(\"Please run 04_xgboost_baseline.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = xgb_full_model.feature_importances_\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "if all_feature_names is not None:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': all_feature_names,\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "else:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': [f'feature_{i}' for i in range(len(feature_importance))],\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "\n",
    "# Mark OCEAN features\n",
    "importance_df['is_ocean'] = importance_df['feature'].isin(ocean_cols)\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Top 20 Most Important Features\")\n",
    "print(\"=\" * 80)\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# OCEAN feature importance\n",
    "ocean_importance = importance_df[importance_df['is_ocean']].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OCEAN Feature Importance\")\n",
    "print(\"=\" * 80)\n",
    "print(ocean_importance[['feature', 'importance']].to_string(index=False))\n",
    "\n",
    "# Statistics\n",
    "ocean_total_importance = ocean_importance['importance'].sum()\n",
    "total_importance = importance_df['importance'].sum()\n",
    "ocean_contribution = (ocean_total_importance / total_importance) * 100\n",
    "\n",
    "print(f\"\\nOCEAN feature contribution: {ocean_contribution:.2f}%\")\n",
    "print(f\"OCEAN feature average importance: {ocean_importance['importance'].mean():.6f}\")\n",
    "print(f\"Non-OCEAN feature average importance: {importance_df[~importance_df['is_ocean']]['importance'].mean():.6f}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_df.to_csv('../../full_model_feature_importance.csv', index=False)\n",
    "print(\"\\nFull feature importance saved: full_model_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Confusion matrix heatmap\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fully Paid', 'Charged Off'],\n",
    "            yticklabels=['Fully Paid', 'Charged Off'],\n",
    "            ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Confusion Matrix (Full Model)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. ROC curve\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'Full Model (AUC = {roc_auc:.4f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=9)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Performance metrics comparison (if baseline available)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "try:\n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC']\n",
    "    baseline_vals = [baseline_metrics['accuracy'], baseline_metrics['precision'], \n",
    "                     baseline_metrics['recall'], baseline_metrics['f1_score'], \n",
    "                     baseline_metrics['roc_auc']]\n",
    "    full_vals = [accuracy, precision, recall, f1, roc_auc]\n",
    "    \n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax3.bar(x - width/2, baseline_vals, width, label='Baseline', color='lightblue', edgecolor='black')\n",
    "    bars2 = ax3.bar(x + width/2, full_vals, width, label='Full Model', color='lightcoral', edgecolor='black')\n",
    "    \n",
    "    ax3.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('Performance Comparison', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(metrics_names, rotation=45, ha='right', fontsize=9)\n",
    "    ax3.legend(fontsize=9)\n",
    "    ax3.set_ylim([0, 1])\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "except:\n",
    "    ax3.text(0.5, 0.5, 'Baseline metrics\\nnot available', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Performance Comparison', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. Top 15 feature importance\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "top_features = importance_df.head(15)\n",
    "colors = ['#e74c3c' if is_ocean else '#3498db' for is_ocean in top_features['is_ocean']]\n",
    "y_pos = np.arange(len(top_features))\n",
    "ax4.barh(y_pos, top_features['importance'].values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_yticks(y_pos)\n",
    "ax4.set_yticklabels(top_features['feature'].values, fontsize=9)\n",
    "ax4.invert_yaxis()\n",
    "ax4.set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Top 15 Feature Importance (Red = OCEAN)', fontsize=12, fontweight='bold')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 5. OCEAN feature importance comparison\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "ocean_feats = ocean_importance['feature'].str.title()\n",
    "ax5.bar(range(len(ocean_feats)), ocean_importance['importance'].values, \n",
    "        color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "ax5.set_xticks(range(len(ocean_feats)))\n",
    "ax5.set_xticklabels(ocean_feats, rotation=45, ha='right', fontsize=10)\n",
    "ax5.set_ylabel('Importance', fontsize=11, fontweight='bold')\n",
    "ax5.set_title('OCEAN Features Importance', fontsize=12, fontweight='bold')\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. OCEAN contribution pie chart\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "sizes = [ocean_total_importance, total_importance - ocean_total_importance]\n",
    "labels = [f'OCEAN\\n{ocean_contribution:.1f}%', f'Other Features\\n{100-ocean_contribution:.1f}%']\n",
    "colors_pie = ['#e74c3c', '#3498db']\n",
    "wedges, texts, autotexts = ax6.pie(sizes, labels=labels, autopct='', colors=colors_pie,\n",
    "                                     startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax6.set_title('Feature Importance Contribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 7. Improvement bar chart (if baseline available)\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "try:\n",
    "    improvements = comparison_df['Improvement %'].values\n",
    "    colors_imp = ['green' if x > 0 else 'red' for x in improvements]\n",
    "    ax7.bar(range(len(metrics_names)), improvements, color=colors_imp, alpha=0.7, edgecolor='black')\n",
    "    ax7.set_xticks(range(len(metrics_names)))\n",
    "    ax7.set_xticklabels(metrics_names, rotation=45, ha='right', fontsize=10)\n",
    "    ax7.set_ylabel('Improvement (%)', fontsize=11, fontweight='bold')\n",
    "    ax7.set_title('Performance Improvement', fontsize=12, fontweight='bold')\n",
    "    ax7.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax7.grid(axis='y', alpha=0.3)\n",
    "except:\n",
    "    ax7.text(0.5, 0.5, 'Baseline metrics\\nnot available', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax7.set_title('Performance Improvement', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.savefig('../../full_model_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualization saved: full_model_evaluation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model\n",
    "print(\"Saving full model...\")\n",
    "with open('../../xgboost_full_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_full_model, f)\n",
    "print(\"Model saved: xgboost_full_model.pkl\")\n",
    "\n",
    "# Save preprocessor\n",
    "print(\"\\nSaving preprocessor...\")\n",
    "with open('../../preprocessor_full.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "print(\"Preprocessor saved: preprocessor_full.pkl\")\n",
    "\n",
    "# Save feature configuration\n",
    "feature_config = {\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'ocean_features': ocean_cols,\n",
    "    'all_features': list(X.columns),\n",
    "    'n_features_after_encoding': X_train_processed.shape[1]\n",
    "}\n",
    "\n",
    "with open('../../full_model_feature_config.json', 'w') as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "print(\"Feature configuration saved: full_model_feature_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"XGBoost Full Model Summary (with OCEAN Features)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Model Configuration\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Model type: XGBoost Classifier\")\n",
    "print(f\"Total features: {X_train_processed.shape[1]} (after encoding)\")\n",
    "print(f\"Original features: {len(numeric_features)} numeric + {len(categorical_features)} categorical\")\n",
    "print(f\"OCEAN features: {len(ocean_cols)}\")\n",
    "print(f\"Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Test samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(\"\\n2. Performance Metrics\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n3. OCEAN Feature Analysis\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"OCEAN feature contribution: {ocean_contribution:.2f}%\")\n",
    "print(f\"OCEAN feature average importance: {ocean_importance['importance'].mean():.6f}\")\n",
    "print(f\"Most important OCEAN features:\")\n",
    "top_ocean = ocean_importance.head(3)\n",
    "for idx, row in top_ocean.iterrows():\n",
    "    print(f\"  - {row['feature']}: {row['importance']:.6f}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n4. Baseline Comparison\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Average performance improvement: {avg_improvement:.2f}%\")\n",
    "    print(f\"Best improved metric: {comparison_df.loc[comparison_df['Improvement %'].idxmax(), 'Metric']}\")\n",
    "    print(f\"Improvement amount: {comparison_df['Improvement %'].max():.2f}%\")\n",
    "except:\n",
    "    print(\"\\n4. Baseline comparison data not available\")\n",
    "\n",
    "print(\"\\n5. Next Steps\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Full model training complete. Next:\")\n",
    "print(\"\")\n",
    "print(\"1. 07_results_analysis.ipynb\")\n",
    "print(\"   - Detailed model comparison analysis\")\n",
    "print(\"   - Deep dive into OCEAN features\")\n",
    "print(\"   - Business insights and recommendations\")\n",
    "print(\"   - Generate final report\")\n",
    "print(\"\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nFull model training complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
