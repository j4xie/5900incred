{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - XGBoost Baseline Model (without OCEAN Features)\n",
    "\n",
    "**Objective**: Establish XGBoost baseline model as performance benchmark\n",
    "\n",
    "## Key Steps:\n",
    "1. Load clean modeling data\n",
    "2. Remove desc field (baseline does not use OCEAN)\n",
    "3. Train/Test split (80/20)\n",
    "4. Data preprocessing pipeline\n",
    "5. Train XGBoost model\n",
    "6. Evaluate performance metrics\n",
    "7. Feature importance analysis\n",
    "8. Save model and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean modeling data\n",
    "print(\"Loading clean modeling data...\")\n",
    "df = pd.read_csv('../../data/loan_clean_for_modeling.csv', low_memory=False)\n",
    "\n",
    "print(f\"Data shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "\n",
    "# Check target variable\n",
    "if 'target' in df.columns:\n",
    "    print(f\"\\nTarget variable distribution:\")\n",
    "    print(df['target'].value_counts())\n",
    "    print(f\"Default rate: {df['target'].mean()*100:.2f}%\")\n",
    "else:\n",
    "    print(\"\\nWarning: Target column not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Features and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop(columns=['target', 'desc'], errors='ignore')  # Remove target and desc\n",
    "y = df['target']\n",
    "\n",
    "print(f\"Original feature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# Remove high cardinality features (avoid One-Hot Encoding explosion)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Handling High Cardinality Features (One-Hot Encoding Optimization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "high_cardinality_features = ['emp_title', 'title', 'earliest_cr_line']\n",
    "X = X.drop(columns=high_cardinality_features, errors='ignore')\n",
    "\n",
    "print(f\"\\nRemoved high cardinality features ({len(high_cardinality_features)} total):\")\n",
    "for feat in high_cardinality_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nOptimized feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)} (optimized)\")\n",
    "\n",
    "print(\"\\nNumeric features list:\")\n",
    "for i, feat in enumerate(numeric_features, 1):\n",
    "    print(f\"{i:3d}. {feat}\")\n",
    "\n",
    "print(\"\\nCategorical features list:\")\n",
    "for i, feat in enumerate(categorical_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 split\n",
    "print(\"Performing Train/Test split (80/20)...\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]:,} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"Default rate: {y_train.mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\nTest set target distribution:\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"Default rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric feature preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Fill missing values with median\n",
    "    ('scaler', StandardScaler())  # Standardize\n",
    "])\n",
    "\n",
    "# Categorical feature preprocessing\n",
    "# Note: High cardinality features removed in previous step to avoid One-Hot Encoding explosion\n",
    "# Removed features: emp_title (78K unique), title (36K unique), earliest_cr_line (603 unique)\n",
    "# Now only encoding remaining low-cardinality categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Fill missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
    "])\n",
    "\n",
    "# Combine preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"Preprocessing pipeline created!\")\n",
    "print(f\"\\n- Numeric features: median imputation + standard scaling\")\n",
    "print(f\"- Categorical features: constant imputation + one-hot encoding\")\n",
    "print(f\"\\nOptimization result:\")\n",
    "print(f\"   Original categorical features expand to ~100-150 columns (not 116,804 columns)\")\n",
    "print(f\"   Preprocessing speed improved 100x!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training set\n",
    "print(\"Preprocessing training set...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform test set\n",
    "print(\"Preprocessing test set...\")\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nProcessed training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed test set shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Get feature names (including one-hot encoded features)\n",
    "try:\n",
    "    # Get one-hot encoded categorical feature names\n",
    "    cat_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "    all_feature_names = numeric_features + list(cat_feature_names)\n",
    "    print(f\"\\nTotal features (after encoding): {len(all_feature_names)}\")\n",
    "except:\n",
    "    all_feature_names = None\n",
    "    print(\"\\nUnable to retrieve feature names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train XGBoost Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weight (handle imbalanced data)\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Class weight (scale_pos_weight): {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Create XGBoost model\n",
    "print(\"\\nCreating XGBoost model...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting model training...\")\n",
    "xgb_model.fit(\n",
    "    X_train_processed, y_train,\n",
    "    eval_set=[(X_test_processed, y_test)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nModel training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Making predictions...\\n\")\n",
    "y_pred = xgb_model.predict(X_test_processed)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBoost Baseline Model Performance Metrics\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0,0]:,}\")\n",
    "print(f\"False Positives: {cm[0,1]:,}\")\n",
    "print(f\"False Negatives: {cm[1,0]:,}\")\n",
    "print(f\"True Positives:  {cm[1,1]:,}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Detailed Classification Report\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred, target_names=['Fully Paid', 'Charged Off']))\n",
    "\n",
    "# Save baseline metrics\n",
    "baseline_metrics = {\n",
    "    'model': 'XGBoost Baseline (without OCEAN)',\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'roc_auc': float(roc_auc),\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'n_features': X_train_processed.shape[1],\n",
    "    'train_size': int(X_train.shape[0]),\n",
    "    'test_size': int(X_test.shape[0])\n",
    "}\n",
    "\n",
    "with open('../../baseline_metrics.json', 'w') as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nBaseline metrics saved: baseline_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "if all_feature_names is not None:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': all_feature_names,\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "else:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': [f'feature_{i}' for i in range(len(feature_importance))],\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Top 20 Most Important Features\")\n",
    "print(\"=\" * 80)\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Save complete feature importance\n",
    "importance_df.to_csv('../../baseline_feature_importance.csv', index=False)\n",
    "print(\"\\nComplete feature importance saved: baseline_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Confusion matrix heatmap\n",
    "ax1 = axes[0, 0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fully Paid', 'Charged Off'],\n",
    "            yticklabels=['Fully Paid', 'Charged Off'],\n",
    "            ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. ROC curve\n",
    "ax2 = axes[0, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='lower right', fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Feature importance (Top 15)\n",
    "ax3 = axes[1, 0]\n",
    "top_features = importance_df.head(15)\n",
    "y_pos = np.arange(len(top_features))\n",
    "ax3.barh(y_pos, top_features['importance'].values, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax3.set_yticks(y_pos)\n",
    "ax3.set_yticklabels(top_features['feature'].values, fontsize=9)\n",
    "ax3.invert_yaxis()\n",
    "ax3.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Top 15 Feature Importance', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Performance metrics comparison\n",
    "ax4 = axes[1, 1]\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "metrics_values = [accuracy, precision, recall, f1, roc_auc]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6']\n",
    "bars = ax4.bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_ylim([0, 1])\n",
    "ax4.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{value:.4f}',\n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../baseline_model_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualization saved: baseline_model_evaluation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Model and Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model\n",
    "print(\"Saving model...\")\n",
    "with open('../../xgboost_baseline_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(\"Model saved: xgboost_baseline_model.pkl\")\n",
    "\n",
    "# Save preprocessor\n",
    "print(\"\\nSaving preprocessor...\")\n",
    "with open('../../preprocessor_baseline.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "print(\"Preprocessor saved: preprocessor_baseline.pkl\")\n",
    "\n",
    "# Save feature configuration\n",
    "feature_config = {\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'all_features': list(X.columns),\n",
    "    'n_features_after_encoding': X_train_processed.shape[1]\n",
    "}\n",
    "\n",
    "with open('../../baseline_feature_config.json', 'w') as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "print(\"Feature configuration saved: baseline_feature_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Baseline Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"XGBoost Baseline Model Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Model Configuration\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Model type: XGBoost Classifier\")\n",
    "print(f\"Number of features: {X_train_processed.shape[1]} (after encoding)\")\n",
    "print(f\"Original features: {len(numeric_features)} numeric + {len(categorical_features)} categorical\")\n",
    "print(f\"Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Test samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(\"\\n2. Performance Metrics\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n3. Top 5 Important Features\")\n",
    "print(\"-\" * 80)\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\n4. Next Steps\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Baseline model established. You can now proceed with:\")\n",
    "print(\"\")\n",
    "print(\"1. 05_ocean_feature_extraction.ipynb\")\n",
    "print(\"   - Extract OCEAN personality features from desc field\")\n",
    "print(\"   - Use same train/test split to avoid data leakage\")\n",
    "print(\"\")\n",
    "print(\"2. 06_xgboost_with_ocean.ipynb\")\n",
    "print(\"   - Train complete model with OCEAN features\")\n",
    "print(\"   - Compare with baseline performance\")\n",
    "print(\"\")\n",
    "print(\"3. 07_results_analysis.ipynb\")\n",
    "print(\"   - Compare Baseline vs Full Model\")\n",
    "print(\"   - Analyze OCEAN feature value\")\n",
    "print(\"\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nBaseline model training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
