{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05e - Extract BGE-Large Embeddings (500 Test Samples)\n",
    "\n",
    "**Purpose**: Extract BGE-Large embeddings from 500 test samples as input features for Ridge regression model training\n",
    "\n",
    "**Input Files**:\n",
    "- test_samples_500.csv - 500 samples\n",
    "- ocean_ground_truth/ - OCEAN ground truth (select best model)\n",
    "\n",
    "**Output Files**:\n",
    "- bge_embeddings_500.npy - BGE embeddings matrix (500x1024)\n",
    "- ocean_targets_500.csv - Corresponding OCEAN scores (500x5)\n",
    "- 05e_extraction_summary.json - Extraction statistics report\n",
    "\n",
    "**Estimated Time**: Approximately 15-20 minutes (500 API calls, 0.3 second delay each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Test Data and OCEAN Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 500 test samples\n",
    "print(\"Loading test data...\")\n",
    "df_samples = pd.read_csv('../test_samples_500.csv')\n",
    "print(f\"Loaded {len(df_samples)} samples\")\n",
    "print(f\"\\nSample info:\")\n",
    "print(df_samples.head())\n",
    "print(f\"\\nColumns: {df_samples.columns.tolist()}\")\n",
    "\n",
    "# Load OCEAN ground truth (use best model)\n",
    "print(\"\\nLoading OCEAN ground truth...\")\n",
    "ocean_gt_file = '../ocean_ground_truth/deepseek_v3.1_ocean_500.csv'\n",
    "\n",
    "if not os.path.exists(ocean_gt_file):\n",
    "    # If file doesn't exist, try other models\n",
    "    print(f\"WARNING: {ocean_gt_file} does not exist, searching for other models...\")\n",
    "    ocean_dir = '../ocean_ground_truth'\n",
    "    if os.path.exists(ocean_dir):\n",
    "        files = [f for f in os.listdir(ocean_dir) if f.endswith('_ocean_500.csv')]\n",
    "        if files:\n",
    "            ocean_gt_file = os.path.join(ocean_dir, files[0])\n",
    "            print(f\"Using: {files[0]}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Cannot find OCEAN ground truth file\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Cannot find {ocean_dir} directory\")\n",
    "\n",
    "df_ocean = pd.read_csv(ocean_gt_file)\n",
    "print(f\"Loaded {len(df_ocean)} OCEAN scores\")\n",
    "print(f\"\\nOCEAN features:\")\n",
    "print(df_ocean.head())\n",
    "print(f\"\\nStatistics:\")\n",
    "print(df_ocean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define BGE Embedding Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load HF Token\ndef load_hf_token():\n    try:\n        with open('../.env', 'r') as f:\n            for line in f:\n                if line.strip() and not line.startswith('#'):\n                    key, value = line.strip().split('=', 1)\n                    if key == 'HF_TOKEN':\n                        return value\n    except:\n        pass\n    return os.getenv('HF_TOKEN', '')\n\nhf_token = load_hf_token()\nprint(f\"HF Token loaded: {'yes' if hf_token else 'no'}\")\n\n# Define BGE embedding extraction function with enhanced retry logic\ndef extract_bge_embedding(text: str, max_retries: int = 5, base_delay: int = 3) -> np.ndarray:\n    \"\"\"\n    Call HF Inference API to extract BGE-Large embeddings with exponential backoff\n    \n    Args:\n        text: Input text\n        max_retries: Maximum retry attempts (increased to 5)\n        base_delay: Base retry delay in seconds\n    \n    Returns:\n        1024-dimensional embedding vector\n    \"\"\"\n    api_url = \"https://api-inference.huggingface.co/models/BAAI/bge-large-en-v1.5\"\n    headers = {\n        \"Authorization\": f\"Bearer {hf_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.post(\n                api_url,\n                headers=headers,\n                json={\"inputs\": text},\n                timeout=60  # Increased timeout\n            )\n            \n            if response.status_code == 200:\n                features = response.json()\n                \n                # Handle different response formats\n                if isinstance(features, list):\n                    if len(features) > 0:\n                        if isinstance(features[0], list):\n                            # features is [token_embeddings]\n                            avg_feature = np.mean(features, axis=0)\n                        else:\n                            # features is already the embedding\n                            avg_feature = features\n                    else:\n                        raise ValueError(\"Empty features list\")\n                else:\n                    # Assume it's already an embedding\n                    avg_feature = features\n                \n                return np.array(avg_feature)\n            \n            elif response.status_code == 500:\n                # Internal server error - exponential backoff\n                if attempt < max_retries - 1:\n                    delay = base_delay * (2 ** attempt)  # Exponential backoff: 3s, 6s, 12s, 24s\n                    print(f\"    API 500 error (attempt {attempt+1}/{max_retries}), waiting {delay}s...\")\n                    time.sleep(delay)\n                    continue\n                else:\n                    raise Exception(f\"API Error 500 after {max_retries} retries\")\n            \n            elif response.status_code == 503:\n                # Model loading\n                if attempt < max_retries - 1:\n                    delay = base_delay * 2  # Wait longer for model loading\n                    print(f\"    Model loading... waiting {delay}s (attempt {attempt+1}/{max_retries})\")\n                    time.sleep(delay)\n                    continue\n                else:\n                    raise Exception(f\"API Error 503 after {max_retries} retries\")\n            \n            elif response.status_code == 429:\n                # Rate limit - wait even longer\n                if attempt < max_retries - 1:\n                    delay = base_delay * (attempt + 2)  # Linear increase: 6s, 9s, 12s\n                    print(f\"    Rate limited, waiting {delay}s...\")\n                    time.sleep(delay)\n                    continue\n                else:\n                    raise Exception(f\"Rate limited after {max_retries} retries\")\n            \n            else:\n                error_msg = response.text[:200] if hasattr(response, 'text') else 'Unknown error'\n                if attempt < max_retries - 1:\n                    print(f\"    API Error {response.status_code}, retrying...\")\n                    time.sleep(base_delay * (attempt + 1))\n                    continue\n                else:\n                    raise Exception(f\"API Error {response.status_code}: {error_msg}\")\n        \n        except requests.exceptions.Timeout:\n            if attempt < max_retries - 1:\n                delay = base_delay * (attempt + 1)\n                print(f\"    Request timeout, waiting {delay}s...\")\n                time.sleep(delay)\n                continue\n            else:\n                raise Exception(\"Request timeout after all retries\")\n        \n        except requests.exceptions.RequestException as e:\n            if attempt < max_retries - 1:\n                delay = base_delay * (attempt + 1)\n                print(f\"    Network error: {str(e)[:50]}, waiting {delay}s...\")\n                time.sleep(delay)\n                continue\n            else:\n                raise\n        \n        except Exception as e:\n            if attempt < max_retries - 1:\n                delay = base_delay * (attempt + 1)\n                print(f\"    Error: {str(e)[:50]}, waiting {delay}s...\")\n                time.sleep(delay)\n                continue\n            else:\n                raise\n    \n    raise Exception(\"Failed to extract embedding after all retries\")\n\nprint(\"\\nâœ“ BGE embedding extraction function defined (with enhanced retry)\")\nprint(\"  - Max retries: 5\")\nprint(\"  - Exponential backoff for 500 errors\")\nprint(\"  - Timeout: 60s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Batch Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"Starting BGE Embeddings extraction (500 samples)\")\nprint(\"=\"*80)\n\nembeddings = []\nsuccess_count = 0\nerror_count = 0\nerror_indices = []\n\nstart_time = time.time()\ntotal_samples = len(df_samples)\n\n# Increased delay to avoid API rate limits and 500 errors\nDELAY_BETWEEN_REQUESTS = 0.5  # Increased from 0.3 to 0.5 seconds\n\nfor idx, (_, row) in enumerate(df_samples.iterrows(), 1):\n    text = row.get('desc', '')\n    \n    if len(text.strip()) < 10:\n        # Skip too short descriptions\n        embeddings.append(np.zeros(1024))\n        error_count += 1\n        error_indices.append(idx - 1)\n        print(f\"  [{idx}] Skipping: text too short\")\n        continue\n    \n    try:\n        # Extract embedding\n        emb = extract_bge_embedding(text)\n        \n        if emb is not None and len(emb) == 1024:\n            embeddings.append(emb)\n            success_count += 1\n        else:\n            embeddings.append(np.zeros(1024))\n            error_count += 1\n            error_indices.append(idx - 1)\n            print(f\"  [{idx}] Error: Invalid embedding dimension\")\n    \n    except Exception as e:\n        embeddings.append(np.zeros(1024))\n        error_count += 1\n        error_indices.append(idx - 1)\n        print(f\"  [{idx}] Error: {str(e)[:100]}\")\n    \n    # Show progress\n    if idx % 25 == 0 or idx == total_samples:  # Show progress more frequently\n        elapsed = time.time() - start_time\n        rate = idx / elapsed if elapsed > 0 else 0\n        eta = (total_samples - idx) / rate if rate > 0 else 0\n        \n        progress = idx / total_samples * 100\n        success_rate = success_count / idx * 100 if idx > 0 else 0\n        print(f\"[{idx:3d}/{total_samples}] {progress:5.1f}% | âœ“{success_count} âœ—{error_count} ({success_rate:.1f}% success) | {rate:.2f} samples/s | ETA: {eta/60:.1f}min\")\n    \n    # Add delay to avoid rate limiting and reduce 500 errors\n    time.sleep(DELAY_BETWEEN_REQUESTS)\n\nelapsed_total = time.time() - start_time\n\nprint(f\"\\n\" + \"=\"*80)\nprint(f\"Embedding extraction complete\")\nprint(f\"=\"*80)\nprint(f\"\\nTime elapsed: {elapsed_total/60:.1f} minutes ({elapsed_total:.1f} seconds)\")\nprint(f\"Success: {success_count}/{total_samples} ({success_count/total_samples*100:.1f}%)\")\nprint(f\"Failed: {error_count}/{total_samples} ({error_count/total_samples*100:.1f}%)\")\n\nif error_count > 0:\n    print(f\"\\nFailed indices (first 20): {error_indices[:20]}\")\n\n# Convert to numpy array\nX = np.array(embeddings)\nprint(f\"\\nEmbedding matrix shape: {X.shape}\")\nprint(f\"Data type: {X.dtype}\")\nprint(f\"Memory usage: {X.nbytes / 1024 / 1024:.1f} MB\")\nprint(f\"Average time per sample: {elapsed_total/total_samples:.2f}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Embeddings and Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving results...\\n\")\n",
    "\n",
    "# Save embeddings\n",
    "embedding_file = '../bge_embeddings_500.npy'\n",
    "np.save(embedding_file, X)\n",
    "print(f\"Embeddings saved: {embedding_file}\")\n",
    "print(f\"  Shape: {X.shape}\")\n",
    "print(f\"  Size: {os.path.getsize(embedding_file) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Save OCEAN targets\n",
    "ocean_target_file = '../ocean_targets_500.csv'\n",
    "df_ocean.to_csv(ocean_target_file, index=False)\n",
    "print(f\"\\nOCEAN targets saved: {ocean_target_file}\")\n",
    "print(f\"  Shape: {df_ocean.shape}\")\n",
    "print(f\"  Columns: {df_ocean.columns.tolist()}\")\n",
    "\n",
    "# Verify data consistency\n",
    "if len(X) == len(df_ocean):\n",
    "    print(f\"\\nData consistency check passed\")\n",
    "    print(f\"   Embeddings count: {len(X)}\")\n",
    "    print(f\"   OCEAN targets count: {len(df_ocean)}\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Data inconsistency\")\n",
    "    print(f\"   Embeddings: {len(X)}\")\n",
    "    print(f\"   OCEAN targets: {len(df_ocean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Statistics Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "summary = {\n",
    "    'phase': '05e - Extract BGE Embeddings',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_samples': int(total_samples),\n",
    "    'success_count': int(success_count),\n",
    "    'error_count': int(error_count),\n",
    "    'success_rate': f\"{success_count/total_samples*100:.2f}%\",\n",
    "    'embedding_model': 'BAAI/bge-large-en-v1.5',\n",
    "    'embedding_dimension': 1024,\n",
    "    'embedding_file': embedding_file,\n",
    "    'ocean_target_file': ocean_target_file,\n",
    "    'ocean_features': df_ocean.columns.tolist(),\n",
    "    'ocean_statistics': {},\n",
    "    'processing_time_seconds': elapsed_total,\n",
    "    'samples_per_second': success_count / elapsed_total if elapsed_total > 0 else 0\n",
    "}\n",
    "\n",
    "# Add OCEAN statistics\n",
    "for col in df_ocean.columns:\n",
    "    summary['ocean_statistics'][col] = {\n",
    "        'mean': float(df_ocean[col].mean()),\n",
    "        'std': float(df_ocean[col].std()),\n",
    "        'min': float(df_ocean[col].min()),\n",
    "        'max': float(df_ocean[col].max())\n",
    "    }\n",
    "\n",
    "# Save summary\n",
    "summary_file = '../05e_extraction_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Statistics report saved: {summary_file}\")\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Summary\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(summary, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Step 05e Complete\n",
    "\n",
    "**Output Files**:\n",
    "- `bge_embeddings_500.npy` - 500x1024 embeddings matrix\n",
    "- `ocean_targets_500.csv` - 500x5 OCEAN scores\n",
    "- `05e_extraction_summary.json` - Extraction report\n",
    "\n",
    "**Next Step**:\n",
    "Run `05f_train_ridge_models.ipynb` to train Ridge regression models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}