{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 05f - Train Elastic Net Models (All LLMs, MPNet Embeddings)\n",
    "\n",
    "**Purpose**: Train Elastic Net regression models for all 5 LLMs using MPNet embeddings to learn the mapping to OCEAN scores\n",
    "\n",
    "**Why MPNet over MiniLM?**\n",
    "- MPNet-base-v2: 110M parameters, 768 dimensions (official recommendation)\n",
    "- MiniLM-L12-v2: 33M parameters, 384 dimensions (baseline)\n",
    "- MPNet is 3.3x larger → better semantic understanding\n",
    "- Expected R² improvement: +0.03~0.05 vs MiniLM\n",
    "\n",
    "**Why Elastic Net?**\n",
    "- Basic Ridge showed severe overfitting: train R² ≈ 0.999, test R² < 0\n",
    "- Problem: 768 features vs ~400 samples → dimension curse\n",
    "- Elastic Net = L1 + L2 regularization:\n",
    "  - L1 (Lasso): Feature selection, removes noise features\n",
    "  - L2 (Ridge): Stability, prevents overfitting\n",
    "- ElasticNetCV: Automatic hyperparameter tuning via 5-fold cross-validation\n",
    "\n",
    "**Input Files**:\n",
    "- mpnet_embeddings_500.npy - MPNet embeddings (500x768)\n",
    "- ocean_ground_truth/[llm]_ocean_500.csv - OCEAN ground truth for each LLM\n",
    "\n",
    "**Output Files** (per LLM):\n",
    "- elasticnet_models_mpnet_[llm].pkl - 5 Elastic Net models + Scaler\n",
    "- 05f_elasticnet_training_report_mpnet_[llm].json - Training report with feature importance\n",
    "\n",
    "**Summary Output**:\n",
    "- 05f_mpnet_elasticnet_comparison.png - Performance comparison across LLMs\n",
    "- 05f_mpnet_vs_minilm.csv - MPNet vs MiniLM comparison\n",
    "\n",
    "**Estimated Time**: Approximately 10-15 minutes (5 LLMs x CV grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM configurations\n",
    "LLM_CONFIGS = {\n",
    "    'llama': {\n",
    "        'name': 'Llama-3.1-8B',\n",
    "        'ocean_file': '../ocean_ground_truth/llama_3.1_8b_ocean_500.csv'\n",
    "    },\n",
    "    'gpt': {\n",
    "        'name': 'GPT-OSS-120B',\n",
    "        'ocean_file': '../ocean_ground_truth/gpt_oss_120b_ocean_500.csv'\n",
    "    },\n",
    "    'gemma': {\n",
    "        'name': 'Gemma-2-9B',\n",
    "        'ocean_file': '../ocean_ground_truth/gemma_2_9b_ocean_500.csv'\n",
    "    },\n",
    "    'deepseek': {\n",
    "        'name': 'DeepSeek-V3.1',\n",
    "        'ocean_file': '../ocean_ground_truth/deepseek_v3.1_ocean_500.csv'\n",
    "    },\n",
    "    'qwen': {\n",
    "        'name': 'Qwen-2.5-72B',\n",
    "        'ocean_file': '../ocean_ground_truth/qwen_2.5_72b_ocean_500.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# OCEAN dimensions\n",
    "OCEAN_DIMS = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "\n",
    "# ElasticNetCV hyperparameters\n",
    "ALPHAS = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "L1_RATIOS = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99]\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  LLM models: {len(LLM_CONFIGS)}\")\n",
    "print(f\"  OCEAN dimensions: {len(OCEAN_DIMS)}\")\n",
    "print(f\"  Alpha grid: {len(ALPHAS)} values\")\n",
    "print(f\"  L1 ratio grid: {len(L1_RATIOS)} values\")\n",
    "print(f\"  Total CV combinations: {len(ALPHAS) * len(L1_RATIOS)} per dimension\")\n",
    "print(f\"  CV folds: {CV_FOLDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Load MPNet Embeddings (Shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading MPNet Embeddings\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "embedding_file = '../mpnet_embeddings_500.npy'\n",
    "print(f\"\\nLoading: {embedding_file}\")\n",
    "X_full = np.load(embedding_file)\n",
    "print(f\"Embeddings shape: {X_full.shape}\")\n",
    "print(f\"  Expected: (500, 768) - MPNet has 768 dimensions\")\n",
    "print(f\"  Data type: {X_full.dtype}\")\n",
    "print(f\"  Memory usage: {X_full.nbytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  Value range: [{X_full.min():.4f}, {X_full.max():.4f}]\")\n",
    "\n",
    "# Verify dimensions\n",
    "assert X_full.shape[1] == 768, f\"Expected 768 dimensions for MPNet, got {X_full.shape[1]}\"\n",
    "print(f\"\\n✓ Dimension check passed: {X_full.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Train Elastic Net Models for Each LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for all results\n",
    "all_results = {}\n",
    "minilm_comparison = {}\n",
    "\n",
    "for llm_key, llm_config in LLM_CONFIGS.items():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Training Elastic Net Models: {llm_config['name']} (MPNet Embeddings)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load OCEAN targets\n",
    "    print(f\"\\n[1/7] Loading OCEAN targets...\")\n",
    "    ocean_file = llm_config['ocean_file']\n",
    "    y_df = pd.read_csv(ocean_file)\n",
    "    print(f\"  Shape: {y_df.shape}\")\n",
    "    print(f\"  Columns: {y_df.columns.tolist()}\")\n",
    "    \n",
    "    # Check and handle NaN values\n",
    "    nan_count_total = y_df.isnull().sum().sum()\n",
    "    if nan_count_total > 0:\n",
    "        print(f\"  Warning: Found {nan_count_total} NaN values\")\n",
    "        nan_indices = y_df[y_df.isnull().any(axis=1)].index\n",
    "        y_df = y_df.dropna()\n",
    "        X = np.delete(X_full, nan_indices, axis=0)\n",
    "        print(f\"  After dropping NaN: {len(y_df)} samples\")\n",
    "    else:\n",
    "        X = X_full.copy()\n",
    "    \n",
    "    # Verify consistency\n",
    "    if len(X) != len(y_df):\n",
    "        raise ValueError(f\"Data inconsistency: X={len(X)}, y={len(y_df)}\")\n",
    "    \n",
    "    # Train/test split\n",
    "    print(f\"\\n[2/7] Splitting data (80/20)...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_df,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    print(f\"  Training: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "    print(f\"  Feature-to-sample ratio: {X_train.shape[1] / X_train.shape[0]:.2f}:1\")\n",
    "    print(f\"  (768 features / {X_train.shape[0]} samples)\")\n",
    "    \n",
    "    # Standardize\n",
    "    print(f\"\\n[3/7] Standardizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(f\"  Train mean={X_train_scaled.mean():.6f}, std={X_train_scaled.std():.6f}\")\n",
    "    print(f\"  Test mean={X_test_scaled.mean():.6f}, std={X_test_scaled.std():.6f}\")\n",
    "    \n",
    "    # Train models\n",
    "    print(f\"\\n[4/7] Training Elastic Net models (5 dimensions)...\")\n",
    "    print(f\"  ElasticNetCV parameters:\")\n",
    "    print(f\"    - Alphas: {ALPHAS}\")\n",
    "    print(f\"    - L1 ratios: {L1_RATIOS}\")\n",
    "    print(f\"    - CV folds: {CV_FOLDS}\")\n",
    "    print(f\"    - Total combinations: {len(ALPHAS) * len(L1_RATIOS)}\")\n",
    "    \n",
    "    elasticnet_models = {}\n",
    "    training_results = {}\n",
    "    \n",
    "    for i, dim in enumerate(OCEAN_DIMS):\n",
    "        print(f\"\\n  [{i+1}/5] Training {dim}...\")\n",
    "        \n",
    "        # Get target\n",
    "        y_train_dim = y_train[dim].values\n",
    "        y_test_dim = y_test[dim].values\n",
    "        \n",
    "        # Train ElasticNetCV\n",
    "        model = ElasticNetCV(\n",
    "            alphas=ALPHAS,\n",
    "            l1_ratio=L1_RATIOS,\n",
    "            cv=CV_FOLDS,\n",
    "            random_state=RANDOM_STATE,\n",
    "            max_iter=10000,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train_dim)\n",
    "        \n",
    "        # Predict\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Metrics\n",
    "        train_r2 = r2_score(y_train_dim, y_train_pred)\n",
    "        test_r2 = r2_score(y_test_dim, y_test_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_dim, y_train_pred))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test_dim, y_test_pred))\n",
    "        train_mae = mean_absolute_error(y_train_dim, y_train_pred)\n",
    "        test_mae = mean_absolute_error(y_test_dim, y_test_pred)\n",
    "        \n",
    "        # Feature importance analysis\n",
    "        coefficients = model.coef_\n",
    "        non_zero_count = np.sum(np.abs(coefficients) > 1e-6)\n",
    "        sparsity = (1 - non_zero_count / len(coefficients)) * 100\n",
    "        \n",
    "        # Top features\n",
    "        top_indices = np.argsort(np.abs(coefficients))[-20:][::-1]\n",
    "        top_features = [\n",
    "            {'index': int(idx), 'coefficient': float(coefficients[idx])}\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        # Save model and results\n",
    "        elasticnet_models[dim] = model\n",
    "        training_results[dim] = {\n",
    "            'train_r2': float(train_r2),\n",
    "            'test_r2': float(test_r2),\n",
    "            'train_rmse': float(train_rmse),\n",
    "            'test_rmse': float(test_rmse),\n",
    "            'train_mae': float(train_mae),\n",
    "            'test_mae': float(test_mae),\n",
    "            'best_alpha': float(model.alpha_),\n",
    "            'best_l1_ratio': float(model.l1_ratio_),\n",
    "            'non_zero_features': int(non_zero_count),\n",
    "            'sparsity_percent': float(sparsity),\n",
    "            'top_20_features': top_features,\n",
    "            'model_intercept': float(model.intercept_)\n",
    "        }\n",
    "        \n",
    "        print(f\"      Best alpha: {model.alpha_:.2f}, l1_ratio: {model.l1_ratio_:.2f}\")\n",
    "        print(f\"      Non-zero features: {non_zero_count}/{len(coefficients)} ({100-sparsity:.1f}%)\")\n",
    "        print(f\"      Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "        print(f\"      Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "    \n",
    "    # Save models\n",
    "    print(f\"\\n[5/7] Saving models...\")\n",
    "    model_data = {\n",
    "        'models': elasticnet_models,\n",
    "        'scaler': scaler,\n",
    "        'ocean_dims': OCEAN_DIMS,\n",
    "        'training_results': training_results,\n",
    "        'training_timestamp': datetime.now().isoformat(),\n",
    "        'llm_model': llm_config['name'],\n",
    "        'embedding_model': 'sentence-transformers/all-mpnet-base-v2',\n",
    "        'embedding_dimension': 768,\n",
    "        'hyperparameters': {\n",
    "            'alphas': ALPHAS,\n",
    "            'l1_ratios': L1_RATIOS,\n",
    "            'cv_folds': CV_FOLDS\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    model_file = f'../elasticnet_models_mpnet_{llm_key}.pkl'\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    print(f\"  Saved: {model_file} ({os.path.getsize(model_file) / 1024:.1f} KB)\")\n",
    "    \n",
    "    # Try loading MiniLM results for comparison\n",
    "    print(f\"\\n[6/7] Looking for MiniLM results for comparison...\")\n",
    "    minilm_report_file = f'../05f_elasticnet_training_report_minilm_{llm_key}.json'\n",
    "    try:\n",
    "        with open(minilm_report_file, 'r') as f:\n",
    "            minilm_data = json.load(f)\n",
    "        minilm_comparison[llm_key] = minilm_data['training_results']\n",
    "        print(f\"  ✓ MiniLM report loaded: {minilm_report_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ MiniLM report not found (will skip comparison)\")\n",
    "        minilm_comparison[llm_key] = None\n",
    "    \n",
    "    # Generate report\n",
    "    print(f\"\\n[7/7] Generating training report...\")\n",
    "    report = {\n",
    "        'phase': f'05f - Train Elastic Net Models ({llm_config[\"name\"]}, MPNet Embeddings)',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'llm_model': llm_config['name'],\n",
    "        'embedding_model': 'sentence-transformers/all-mpnet-base-v2',\n",
    "        'embedding_parameters': '110M',\n",
    "        'embedding_dimension': 768,\n",
    "        'training_samples': int(X_train.shape[0]),\n",
    "        'test_samples': int(X_test.shape[0]),\n",
    "        'model_type': 'Elastic Net (L1+L2 Regularization)',\n",
    "        'hyperparameters': model_data['hyperparameters'],\n",
    "        'ocean_dimensions': OCEAN_DIMS,\n",
    "        'model_file': model_file,\n",
    "        'training_results': training_results\n",
    "    }\n",
    "    \n",
    "    # Summary metrics\n",
    "    test_r2_scores = [training_results[dim]['test_r2'] for dim in OCEAN_DIMS]\n",
    "    test_rmse_scores = [training_results[dim]['test_rmse'] for dim in OCEAN_DIMS]\n",
    "    test_mae_scores = [training_results[dim]['test_mae'] for dim in OCEAN_DIMS]\n",
    "    avg_sparsity = np.mean([training_results[dim]['sparsity_percent'] for dim in OCEAN_DIMS])\n",
    "    \n",
    "    report['summary_metrics'] = {\n",
    "        'avg_test_r2': float(np.mean(test_r2_scores)),\n",
    "        'avg_test_rmse': float(np.mean(test_rmse_scores)),\n",
    "        'avg_test_mae': float(np.mean(test_mae_scores)),\n",
    "        'min_test_r2': float(np.min(test_r2_scores)),\n",
    "        'max_test_r2': float(np.max(test_r2_scores)),\n",
    "        'avg_sparsity_percent': float(avg_sparsity)\n",
    "    }\n",
    "    \n",
    "    report_file = f'../05f_elasticnet_training_report_mpnet_{llm_key}.json'\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"  Report saved: {report_file}\")\n",
    "    \n",
    "    # Store for final comparison\n",
    "    all_results[llm_key] = {\n",
    "        'name': llm_config['name'],\n",
    "        'training_results': training_results,\n",
    "        'summary': report['summary_metrics']\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n  Summary for {llm_config['name']}:\")\n",
    "    print(f\"    Avg Test R²: {report['summary_metrics']['avg_test_r2']:.4f}\")\n",
    "    print(f\"    Avg Sparsity: {avg_sparsity:.1f}%\")\n",
    "    print(f\"    Test R² range: [{report['summary_metrics']['min_test_r2']:.4f}, {report['summary_metrics']['max_test_r2']:.4f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All MPNet models trained successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Generate Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generating Comparison Visualizations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for llm_key, results in all_results.items():\n",
    "    for dim in OCEAN_DIMS:\n",
    "        mpnet_r2 = results['training_results'][dim]['test_r2']\n",
    "        mpnet_rmse = results['training_results'][dim]['test_rmse']\n",
    "        sparsity = results['training_results'][dim]['sparsity_percent']\n",
    "        \n",
    "        # Get MiniLM results if available\n",
    "        if minilm_comparison.get(llm_key):\n",
    "            minilm_r2 = minilm_comparison[llm_key][dim]['test_r2']\n",
    "            minilm_rmse = minilm_comparison[llm_key][dim]['test_rmse']\n",
    "        else:\n",
    "            minilm_r2 = None\n",
    "            minilm_rmse = None\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'LLM': results['name'],\n",
    "            'llm_key': llm_key,\n",
    "            'Dimension': dim,\n",
    "            'MPNet_R2': mpnet_r2,\n",
    "            'MiniLM_R2': minilm_r2,\n",
    "            'R2_Improvement': mpnet_r2 - minilm_r2 if minilm_r2 else None,\n",
    "            'MPNet_RMSE': mpnet_rmse,\n",
    "            'MiniLM_RMSE': minilm_rmse,\n",
    "            'Sparsity_%': sparsity\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_file = '../05f_mpnet_vs_minilm.csv'\n",
    "comparison_df.to_csv(comparison_file, index=False)\n",
    "print(f\"\\nComparison table saved: {comparison_file}\")\n",
    "print(f\"\\nPreview:\")\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('MPNet vs MiniLM Performance Comparison (All LLMs)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Test R² Comparison\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x_pos - width/2, comparison_df['MPNet_R2'], width, label='MPNet (768d)', color='#3498db', alpha=0.8)\n",
    "if comparison_df['MiniLM_R2'].notna().any():\n",
    "    ax1.bar(x_pos + width/2, comparison_df['MiniLM_R2'], width, label='MiniLM (384d)', color='#95a5a6', alpha=0.8)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_xlabel('Model-Dimension', fontsize=10)\n",
    "ax1.set_ylabel('Test R² Score', fontsize=10)\n",
    "ax1.set_title('Test R² Comparison (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=90, labelsize=7)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f\"{row['llm_key'][:3]}-{row['Dimension'][:3]}\" for _, row in comparison_df.iterrows()])\n",
    "\n",
    "# 2. Average R² by LLM\n",
    "ax2 = axes[0, 1]\n",
    "avg_by_llm = comparison_df.groupby('LLM')[['MPNet_R2', 'MiniLM_R2']].mean()\n",
    "avg_by_llm.plot(kind='bar', ax=ax2, color=['#3498db', '#95a5a6'], alpha=0.8)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_xlabel('LLM Model', fontsize=10)\n",
    "ax2.set_ylabel('Average Test R²', fontsize=10)\n",
    "ax2.set_title('Average Test R² by LLM', fontsize=12, fontweight='bold')\n",
    "ax2.legend(['MPNet (768d)', 'MiniLM (384d)'])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. R² Improvement (MPNet - MiniLM)\n",
    "ax3 = axes[1, 0]\n",
    "if comparison_df['R2_Improvement'].notna().any():\n",
    "    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in comparison_df['R2_Improvement']]\n",
    "    ax3.bar(x_pos, comparison_df['R2_Improvement'], color=colors, alpha=0.8)\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax3.set_xlabel('Model-Dimension', fontsize=10)\n",
    "    ax3.set_ylabel('R² Improvement', fontsize=10)\n",
    "    ax3.set_title('R² Improvement (MPNet - MiniLM)', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.tick_params(axis='x', rotation=90, labelsize=7)\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels([f\"{row['llm_key'][:3]}-{row['Dimension'][:3]}\" for _, row in comparison_df.iterrows()])\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'MiniLM data not available', ha='center', va='center', transform=ax3.transAxes)\n",
    "\n",
    "# 4. Sparsity Analysis\n",
    "ax4 = axes[1, 1]\n",
    "sparsity_by_llm = comparison_df.groupby('LLM')['Sparsity_%'].mean().sort_values(ascending=False)\n",
    "sparsity_by_llm.plot(kind='barh', ax=ax4, color='#9b59b6', alpha=0.8)\n",
    "ax4.set_xlabel('Average Sparsity (%)', fontsize=10)\n",
    "ax4.set_ylabel('LLM Model', fontsize=10)\n",
    "ax4.set_title('Feature Sparsity by LLM (% Features Set to Zero)', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "viz_file = '../05f_mpnet_elasticnet_comparison.png'\n",
    "plt.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nVisualization saved: {viz_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 6: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY (MPNet Embeddings)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Overall Performance (MPNet):\")\n",
    "print(f\"   Average Test R² across all models: {comparison_df['MPNet_R2'].mean():.4f}\")\n",
    "print(f\"   Best Test R²: {comparison_df['MPNet_R2'].max():.4f}\")\n",
    "print(f\"   Worst Test R²: {comparison_df['MPNet_R2'].min():.4f}\")\n",
    "print(f\"   Std Dev: {comparison_df['MPNet_R2'].std():.4f}\")\n",
    "\n",
    "if comparison_df['MiniLM_R2'].notna().any():\n",
    "    print(\"\\n2. Comparison with MiniLM:\")\n",
    "    print(f\"   Average MiniLM Test R²: {comparison_df['MiniLM_R2'].mean():.4f}\")\n",
    "    print(f\"   Average Improvement: {comparison_df['R2_Improvement'].mean():.4f}\")\n",
    "    print(f\"   Models improved: {(comparison_df['R2_Improvement'] > 0).sum()}/{len(comparison_df)}\")\n",
    "    print(f\"   Best improvement: {comparison_df['R2_Improvement'].max():.4f}\")\n",
    "    print(f\"   Worst change: {comparison_df['R2_Improvement'].min():.4f}\")\n",
    "\n",
    "print(\"\\n3. Feature Selection (Sparsity):\")\n",
    "print(f\"   Average sparsity: {comparison_df['Sparsity_%'].mean():.1f}%\")\n",
    "print(f\"   Average features retained: {768 * (1 - comparison_df['Sparsity_%'].mean()/100):.0f}/768\")\n",
    "\n",
    "print(\"\\n4. Best Performing LLM:\")\n",
    "best_llm = comparison_df.groupby('LLM')['MPNet_R2'].mean().idxmax()\n",
    "best_r2 = comparison_df.groupby('LLM')['MPNet_R2'].mean().max()\n",
    "print(f\"   {best_llm}: {best_r2:.4f}\")\n",
    "\n",
    "print(\"\\n5. Best Performing Dimension:\")\n",
    "best_dim = comparison_df.groupby('Dimension')['MPNet_R2'].mean().idxmax()\n",
    "best_dim_r2 = comparison_df.groupby('Dimension')['MPNet_R2'].mean().max()\n",
    "print(f\"   {best_dim}: {best_dim_r2:.4f}\")\n",
    "\n",
    "print(\"\\n6. Model Comparison:\")\n",
    "print(f\"   MPNet: 110M parameters, 768 dimensions\")\n",
    "print(f\"   MiniLM: 33M parameters, 384 dimensions\")\n",
    "print(f\"   Parameter ratio: 3.3x larger\")\n",
    "print(f\"   Dimension ratio: 2x larger\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Output Files Generated:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Models:\")\n",
    "for llm_key in LLM_CONFIGS.keys():\n",
    "    print(f\"  - elasticnet_models_mpnet_{llm_key}.pkl\")\n",
    "print(\"\\nReports:\")\n",
    "for llm_key in LLM_CONFIGS.keys():\n",
    "    print(f\"  - 05f_elasticnet_training_report_mpnet_{llm_key}.json\")\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  - 05f_mpnet_vs_minilm.csv\")\n",
    "print(f\"  - 05f_mpnet_elasticnet_comparison.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"05f Complete (MPNet)!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Analysis Notes\n",
    "\n",
    "**Expected Results**:\n",
    "\n",
    "1. **Performance Improvement**: \n",
    "   - MPNet should show +0.03~0.05 R² improvement vs MiniLM\n",
    "   - Predicted R²: 0.22-0.27 (vs MiniLM's 0.19-0.24)\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - L1 regularization should eliminate 90-95% of features\n",
    "   - 768 dims → ~40-80 retained features per dimension\n",
    "\n",
    "3. **Model Comparison**:\n",
    "   - MPNet (110M): Better semantic understanding\n",
    "   - MiniLM (33M): Faster but less accurate\n",
    "   - BGE (326M): Most powerful but slowest\n",
    "\n",
    "4. **Next Steps**:\n",
    "   - If MPNet R² > MiniLM: Use MPNet for production\n",
    "   - If improvement < 0.03: Stick with MiniLM for speed\n",
    "   - Consider trying larger models (BGE, GTR-T5-XL) if more improvement needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
