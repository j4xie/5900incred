{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 05e - Extract MiniLM Embeddings (500 Test Samples)\n\n**Purpose**: Extract all-MiniLM-L12-v2 embeddings from 500 test samples using HF Inference API\n\n**Why MiniLM-L12-v2?**\n- MiniLM-L12-v2: 33M parameters, 384 dimensions, fast and efficient\n- BGE-Large: 326M parameters, 1024 dimensions\n- MiniLM is designed specifically for sentence embeddings\n- Part of the popular sentence-transformers library\n\n**Input Files**:\n- test_samples_500.csv - 500 loan descriptions\n- ocean_ground_truth/ - OCEAN ground truth (for consistency check)\n\n**Output Files**:\n- deberta_embeddings_500.npy - MiniLM embeddings matrix (500x384)\n- 05e_deberta_extraction_summary.json - Extraction statistics report\n\n**Note**: File names kept as \"deberta\" for compatibility, but using MiniLM model\n**Note**: Output dimension is 384 (different from BGE's 1024)\n\n**Estimated Time**: Approximately 10-15 minutes (500 API calls, smaller model = faster)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport requests\nimport json\nimport os\nimport time\nfrom datetime import datetime\nimport warnings\nfrom huggingface_hub import InferenceClient\nwarnings.filterwarnings('ignore')\n\nprint(\"Libraries loaded successfully\")\nprint(f\"Timestamp: {datetime.now()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load HF Token and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF Token\n",
    "def load_hf_token():\n",
    "    try:\n",
    "        with open('../.env', 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip() and not line.startswith('#'):\n",
    "                    key, value = line.strip().split('=', 1)\n",
    "                    if key == 'HF_TOKEN':\n",
    "                        return value\n",
    "    except:\n",
    "        pass\n",
    "    return os.getenv('HF_TOKEN', '')\n",
    "\n",
    "hf_token = load_hf_token()\n",
    "print(f\"HF Token loaded: {'yes' if hf_token else 'no'}\")\n",
    "\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN not found. Please set it in .env file or environment variable\")\n",
    "\n",
    "# Load 500 test samples\n",
    "print(\"\\nLoading test data...\")\n",
    "df_samples = pd.read_csv('../test_samples_500.csv')\n",
    "print(f\"Loaded {len(df_samples)} samples\")\n",
    "print(f\"\\nColumns: {df_samples.columns.tolist()}\")\n",
    "print(f\"\\nSample preview:\")\n",
    "print(df_samples.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Define MiniLM Embedding Extraction Function\n\n**MiniLM-L12-v2 Embedding Strategy**:\n- Model: `sentence-transformers/all-MiniLM-L12-v2` (33M parameters)\n- Method: Feature extraction via InferenceClient\n- Output: 384-dimensional embedding per text\n- No special prefix required (unlike E5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_deberta_embedding(text: str, max_retries: int = 3, retry_delay: int = 3) -> np.ndarray:\n    \"\"\"\n    Call HF Inference API to extract MiniLM embeddings using InferenceClient\n    \n    Args:\n        text: Input text\n        max_retries: Maximum retry attempts\n        retry_delay: Retry delay (seconds)\n    \n    Returns:\n        384-dimensional embedding vector\n    \"\"\"\n    # Create InferenceClient with HF Pro provider\n    client = InferenceClient(\n        provider=\"hf-inference\",\n        api_key=hf_token\n    )\n    \n    for attempt in range(max_retries):\n        try:\n            # Use feature_extraction method for embeddings\n            result = client.feature_extraction(\n                text=text,\n                model=\"sentence-transformers/all-MiniLM-L12-v2\"\n            )\n            \n            # Handle the result\n            if result is not None:\n                # Convert to numpy array\n                embeddings_array = np.array(result)\n                \n                # Handle different output formats\n                if len(embeddings_array.shape) == 2:\n                    # Shape: (seq_len, hidden_dim) - do mean pooling\n                    mean_embedding = np.mean(embeddings_array, axis=0)\n                elif len(embeddings_array.shape) == 1:\n                    # Already a single embedding vector\n                    mean_embedding = embeddings_array\n                else:\n                    raise ValueError(f\"Unexpected embedding shape: {embeddings_array.shape}\")\n                \n                # Verify dimension (MiniLM-L12-v2 outputs 384 dimensions)\n                if len(mean_embedding) == 384:\n                    return mean_embedding\n                else:\n                    raise ValueError(f\"Expected 384 dimensions, got {len(mean_embedding)}\")\n            else:\n                raise ValueError(\"Received None from API\")\n        \n        except Exception as e:\n            error_msg = str(e)\n            \n            # Handle rate limiting\n            if \"rate\" in error_msg.lower() or \"429\" in error_msg:\n                if attempt < max_retries - 1:\n                    wait_time = retry_delay * (attempt + 2)\n                    print(f\"    Rate limited... waiting {wait_time}s\")\n                    time.sleep(wait_time)\n                    continue\n                else:\n                    raise Exception(f\"Rate limited after {max_retries} retries\")\n            \n            # Handle model loading\n            elif \"loading\" in error_msg.lower() or \"503\" in error_msg:\n                if attempt < max_retries - 1:\n                    wait_time = retry_delay * (attempt + 1)\n                    print(f\"    Model loading... waiting {wait_time}s\")\n                    time.sleep(wait_time)\n                    continue\n                else:\n                    raise Exception(f\"Model still loading after {max_retries} retries\")\n            \n            # Other errors - retry\n            else:\n                if attempt < max_retries - 1:\n                    print(f\"    Error: {error_msg[:100]} ... retrying\")\n                    time.sleep(retry_delay)\n                    continue\n                else:\n                    raise\n    \n    raise Exception(\"Failed to extract embedding after all retries\")\n\nprint(\"\\nMiniLM-L12-v2 embedding extraction function defined (using InferenceClient)\")\n\n# Test with a sample\nprint(\"\\nTesting embedding extraction...\")\ntest_text = \"This is a test sentence for embedding extraction.\"\ntry:\n    test_emb = extract_deberta_embedding(test_text)\n    print(f\"✅ Test successful! Embedding shape: {test_emb.shape}\")\n    print(f\"  Dimension: {len(test_emb)}\")\n    print(f\"  Sample values: {test_emb[:5]}\")\nexcept Exception as e:\n    print(f\"❌ Test failed: {str(e)}\")\n    print(\"\\nNote: First API call may take longer as model loads. This is normal.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Batch Extract MiniLM Embeddings\n\n**Processing Strategy**:\n- Process 500 samples sequentially\n- 1 second delay between requests (smaller model = faster)\n- Automatic retry on errors\n- Progress updates every 50 samples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"Starting MiniLM-L12-v2 Embeddings Extraction (500 samples)\")\nprint(\"=\"*80)\nprint(\"\\nNote: MiniLM-L12-v2 is a compact model (33M parameters, 384 dimensions).\")\nprint(\"This should be faster than larger models.\")\nprint(\"Estimated time: 10-15 minutes\\n\")\n\nembeddings = []\nsuccess_count = 0\nerror_count = 0\nerror_indices = []\n\nstart_time = time.time()\ntotal_samples = len(df_samples)\n\nfor idx, (_, row) in enumerate(df_samples.iterrows(), 1):\n    text = row.get('desc', '')\n    \n    # Skip very short descriptions\n    if len(text.strip()) < 10:\n        embeddings.append(np.zeros(384))  # 384 dimensions for MiniLM\n        error_count += 1\n        error_indices.append(idx - 1)\n        print(f\"  [{idx:3d}] Skipped: text too short\")\n        continue\n    \n    try:\n        # Extract embedding\n        emb = extract_deberta_embedding(text)\n        \n        if emb is not None and len(emb) == 384:\n            embeddings.append(emb)\n            success_count += 1\n        else:\n            embeddings.append(np.zeros(384))\n            error_count += 1\n            error_indices.append(idx - 1)\n            print(f\"  [{idx:3d}] Error: Invalid embedding dimension\")\n    \n    except Exception as e:\n        embeddings.append(np.zeros(384))\n        error_count += 1\n        error_indices.append(idx - 1)\n        \n        # Log first few errors and periodic errors\n        if idx <= 10 or error_count % 10 == 1:\n            print(f\"  [{idx:3d}] ERROR: {str(e)[:80]}\")\n    \n    # Progress report\n    if idx % 50 == 0 or idx == total_samples:\n        elapsed = time.time() - start_time\n        rate = idx / elapsed if elapsed > 0 else 0\n        eta = (total_samples - idx) / rate if rate > 0 else 0\n        \n        progress = idx / total_samples * 100\n        print(f\"\\n[{idx:3d}/{total_samples}] ({progress:5.1f}%) | Success: {success_count}, Failed: {error_count}\")\n        print(f\"  Rate: {rate:.2f} samples/s | Elapsed: {elapsed/60:.1f}min | ETA: {eta/60:.1f}min\\n\")\n    \n    # Delay to avoid rate limiting (shorter for smaller model)\n    time.sleep(1.0)\n\nelapsed_total = time.time() - start_time\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MiniLM-L12-v2 Embedding Extraction Complete\")\nprint(\"=\"*80)\nprint(f\"\\nTotal time: {elapsed_total/60:.1f} minutes ({elapsed_total:.1f} seconds)\")\nprint(f\"Success: {success_count}/{total_samples} ({success_count/total_samples*100:.1f}%)\")\nprint(f\"Failed: {error_count}/{total_samples} ({error_count/total_samples*100:.1f}%)\")\nprint(f\"Average rate: {success_count/elapsed_total:.2f} samples/second\")\n\nif error_count > 0:\n    print(f\"\\nError indices (first 20): {error_indices[:20]}\")\n\n# Convert to numpy array\nX = np.array(embeddings)\nprint(f\"\\nEmbedding matrix shape: {X.shape}\")\nprint(f\"Data type: {X.dtype}\")\nprint(f\"Memory usage: {X.nbytes / 1024 / 1024:.2f} MB\")\nprint(f\"Value range: [{X.min():.4f}, {X.max():.4f}]\")\nprint(f\"Mean: {X.mean():.4f}, Std: {X.std():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Save MiniLM Embeddings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSaving MiniLM-L12-v2 embeddings...\")\n\n# Save embeddings (keeping filename as deberta for compatibility)\nembedding_file = '../deberta_embeddings_500.npy'\nnp.save(embedding_file, X)\nprint(f\"\\nEmbeddings saved: {embedding_file}\")\nprint(f\"  Model: sentence-transformers/all-MiniLM-L12-v2\")\nprint(f\"  Shape: {X.shape}\")\nprint(f\"  Dimensions: 384 (note: different from BGE's 1024)\")\nprint(f\"  File size: {os.path.getsize(embedding_file) / 1024 / 1024:.2f} MB\")\n\n# Verify loading\nX_loaded = np.load(embedding_file)\nprint(f\"\\nVerification: Loaded embeddings shape = {X_loaded.shape}\")\nassert np.array_equal(X, X_loaded), \"Verification failed!\"\nprint(\"Verification passed ✓\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Statistics Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate summary report\nsummary = {\n    'phase': '05e - Extract MiniLM-L12-v2 Embeddings',\n    'timestamp': datetime.now().isoformat(),\n    'model': 'sentence-transformers/all-MiniLM-L12-v2',\n    'model_parameters': '33M',\n    'embedding_dimension': 384,\n    'extraction_method': 'HF Inference API (InferenceClient) + Mean Pooling',\n    'total_samples': int(total_samples),\n    'success_count': int(success_count),\n    'error_count': int(error_count),\n    'success_rate': f\"{success_count/total_samples*100:.2f}%\",\n    'processing_time_seconds': float(elapsed_total),\n    'processing_time_minutes': float(elapsed_total / 60),\n    'samples_per_second': float(success_count / elapsed_total if elapsed_total > 0 else 0),\n    'embedding_file': embedding_file,\n    'embedding_statistics': {\n        'mean': float(X.mean()),\n        'std': float(X.std()),\n        'min': float(X.min()),\n        'max': float(X.max()),\n        'non_zero_embeddings': int(success_count)\n    },\n    'comparison_with_bge': {\n        'bge_parameters': '326M',\n        'minilm_parameters': '33M',\n        'bge_dimensions': 1024,\n        'minilm_dimensions': 384,\n        'parameter_ratio': 'MiniLM is 10x smaller',\n        'dimension_ratio': 'MiniLM has 2.7x fewer dimensions',\n        'expected_comparison': 'MiniLM is compact but efficient, good for rapid experimentation'\n    }\n}\n\n# Save summary\nsummary_file = '../05e_deberta_extraction_summary.json'\nwith open(summary_file, 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(f\"\\nStatistics report saved: {summary_file}\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"Summary\")\nprint(\"=\"*80)\nprint(json.dumps(summary, indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Compare with BGE Embeddings (Optional)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load BGE embeddings for comparison\ntry:\n    print(\"\\nLoading BGE embeddings for comparison...\")\n    X_bge = np.load('../bge_embeddings_500.npy')\n    \n    print(f\"\\nEmbedding Comparison:\")\n    print(f\"{'Metric':<20} {'BGE':>15} {'MiniLM-L12':>15}\")\n    print(f\"{'-'*50}\")\n    print(f\"{'Shape':<20} {str(X_bge.shape):>15} {str(X.shape):>15}\")\n    print(f\"{'Mean':<20} {X_bge.mean():>15.4f} {X.mean():>15.4f}\")\n    print(f\"{'Std':<20} {X_bge.std():>15.4f} {X.std():>15.4f}\")\n    print(f\"{'Min':<20} {X_bge.min():>15.4f} {X.min():>15.4f}\")\n    print(f\"{'Max':<20} {X_bge.max():>15.4f} {X.max():>15.4f}\")\n    \n    print(f\"\\nNote: Cannot compute cosine similarity - different dimensions (1024 vs 384)\")\n    print(f\"Both embeddings will be evaluated separately for OCEAN prediction.\")\n    \nexcept FileNotFoundError:\n    print(\"\\nBGE embeddings not found. Skipping comparison.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n**Step 05e Complete - MiniLM-L12-v2 Embeddings**\n\n**Output Files**:\n- `deberta_embeddings_500.npy` - 500x384 MiniLM embeddings\n- `05e_deberta_extraction_summary.json` - Extraction statistics\n\n**Model Used**:\n- **Name**: sentence-transformers/all-MiniLM-L12-v2\n- **Size**: 33M parameters (10x smaller than BGE)\n- **Dimensions**: 384 (vs BGE's 1024)\n- **Specialization**: Compact sentence embeddings, part of sentence-transformers\n\n**Key Features**:\n- Fast extraction (smaller model)\n- Good quality despite compact size\n- Different dimensionality from BGE - will need separate Ridge/ElasticNet models\n\n**Important Note**:\n- This model has **384 dimensions** instead of 1024\n- You'll need to train separate regression models for this embedding\n- Or you can stick with BGE (1024 dims) if dimension consistency is important\n\n**Next Steps**:\n1. Run `05f_train_ridge_all_models.ipynb` with MiniLM embeddings (384 dims)\n2. Run `05f_train_elasticnet_all_models.ipynb` with MiniLM embeddings\n3. Compare: BGE (1024d) vs MiniLM (384d) performance on OCEAN prediction"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}