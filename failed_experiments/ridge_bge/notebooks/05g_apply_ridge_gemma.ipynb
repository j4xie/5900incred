{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 05g - Apply Ridge Models to Generate Full Dataset OCEAN Features (Gemma-2-9B)\n",
    "\n",
    "**Purpose**: Apply trained Ridge models (Gemma-2-9B) to all 34,529 samples to generate complete OCEAN features\n",
    "\n",
    "**Input Files**:\n",
    "- data/loan_final_desc50plus.csv - Full dataset (34,529 samples)\n",
    "- ridge_models_gemma.pkl - Trained Ridge models (Gemma-2-9B)\n",
    "\n",
    "**Output Files**:\n",
    "- loan_with_ocean_gemma.csv - Complete dataset with OCEAN features (34,529xN)\n",
    "- bge_large_embeddings_full.npy - Full BGE embeddings (34,529x1024)\n",
    "- 05g_full_data_summary_gemma.json - Processing report\n",
    "\n",
    "**WARNING: Estimated Time: 3-4 hours** (34,529 API calls, 0.3-0.5 second delay each)\n",
    "\n",
    "**Recommendation**: Run this notebook in background using screen or tmux to avoid connection interruption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Load Full Dataset and Ridge Models (Gemma-2-9B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Loading data and models (Gemma-2-9B)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Load full dataset\n",
    "print(\"\\nLoading full dataset...\")\n",
    "data_file = '../data/loan_final_desc50plus.csv'\n",
    "df_full = pd.read_csv(data_file, low_memory=False)\n",
    "print(f\"Data loaded successfully: {len(df_full)} rows x {len(df_full.columns)} columns\")\n",
    "print(f\"  Columns: {df_full.columns.tolist()[:5]}... (showing first 5 columns)\")\n",
    "\n",
    "# Load Ridge models and Scaler (Gemma-2-9B)\n",
    "print(\"\\nLoading Ridge models (Gemma-2-9B)...\")\n",
    "model_file = '../ridge_models_gemma.pkl'\n",
    "with open(model_file, 'rb') as f:\n",
    "    model_data = pickle.load(f)\n",
    "    ridge_models = model_data['models']\n",
    "    scaler = model_data['scaler']\n",
    "    OCEAN_DIMS = model_data['ocean_dims']\n",
    "\n",
    "print(f\"Models loaded successfully (Gemma-2-9B)\")\n",
    "print(f\"  OCEAN dimensions: {OCEAN_DIMS}\")\n",
    "print(f\"  Scaler: StandardScaler\")\n",
    "\n",
    "# Load HF Token\n",
    "def load_hf_token():\n",
    "    try:\n",
    "        with open('../.env', 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip() and not line.startswith('#'):\n",
    "                    key, value = line.strip().split('=', 1)\n",
    "                    if key == 'HF_TOKEN':\n",
    "                        return value\n",
    "    except:\n",
    "        pass\n",
    "    return os.getenv('HF_TOKEN', '')\n",
    "\n",
    "hf_token = load_hf_token()\n",
    "print(f\"\\nHF Token loaded: {'yes' if hf_token else 'no'}\")\n",
    "\n",
    "total_samples = len(df_full)\n",
    "print(f\"\\nTotal samples to process: {total_samples:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Define Embedding Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bge_embedding(text: str, max_retries: int = 3, retry_delay: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Call HF Inference API to extract BGE-Large embeddings\n",
    "    \n",
    "    Returns zero vector on failure to maintain index alignment\n",
    "    \"\"\"\n",
    "    api_url = \"https://api-inference.huggingface.co/models/BAAI/bge-large-en-v1.5\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {hf_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                api_url,\n",
    "                headers=headers,\n",
    "                json={\"inputs\": text},\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                features = response.json()\n",
    "                \n",
    "                if isinstance(features, list):\n",
    "                    if len(features) > 0:\n",
    "                        if isinstance(features[0], list):\n",
    "                            avg_feature = np.mean(features, axis=0)\n",
    "                        else:\n",
    "                            avg_feature = features\n",
    "                    else:\n",
    "                        return np.zeros(1024)\n",
    "                else:\n",
    "                    avg_feature = features\n",
    "                \n",
    "                return np.array(avg_feature)\n",
    "            \n",
    "            elif response.status_code == 503:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(retry_delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    return np.zeros(1024)\n",
    "            \n",
    "            elif response.status_code == 429:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(retry_delay * (attempt + 1))\n",
    "                    continue\n",
    "                else:\n",
    "                    return np.zeros(1024)\n",
    "            \n",
    "            else:\n",
    "                return np.zeros(1024)\n",
    "        \n",
    "        except:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            else:\n",
    "                return np.zeros(1024)\n",
    "    \n",
    "    return np.zeros(1024)\n",
    "\n",
    "print(\"Embedding extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Batch Extract All Sample Embeddings (Takes 3-4 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Step 1: Extract Full BGE-Large Embeddings (34,529 samples)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nEstimated time: 3-4 hours\")\n",
    "print(f\"WARNING: This step will make {total_samples:,} API calls\")\n",
    "print(f\"\\nStart time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nProgress:\\n\")\n",
    "\n",
    "all_embeddings = []\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "\n",
    "start_time = time.time()\n",
    "checkpoint_interval = 2000  # Save checkpoint every 2000 samples\n",
    "\n",
    "for idx in range(total_samples):\n",
    "    text = df_full.iloc[idx].get('desc', '')\n",
    "    \n",
    "    if len(text.strip()) < 10:\n",
    "        all_embeddings.append(np.zeros(1024))\n",
    "        error_count += 1\n",
    "    else:\n",
    "        emb = extract_bge_embedding(text)\n",
    "        all_embeddings.append(emb)\n",
    "        \n",
    "        if np.sum(emb) == 0:  # Zero vector indicates error\n",
    "            error_count += 1\n",
    "        else:\n",
    "            success_count += 1\n",
    "    \n",
    "    # Show progress (update every 100 samples)\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (idx + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (total_samples - idx - 1) / rate if rate > 0 else 0\n",
    "        \n",
    "        progress = (idx + 1) / total_samples * 100\n",
    "        \n",
    "        hours = int(remaining // 3600)\n",
    "        minutes = int((remaining % 3600) // 60)\n",
    "        \n",
    "        print(f\"[{idx+1:5d}/{total_samples}] ({progress:5.1f}%) | Success: {success_count:5d}, Failed: {error_count:5d} | \"\n",
    "              f\"Rate: {rate:.2f} samples/s | Remaining: {hours}h {minutes}m\")\n",
    "    \n",
    "    # Checkpoint save\n",
    "    if (idx + 1) % checkpoint_interval == 0:\n",
    "        print(f\"\\n  Checkpoint: Processed {idx+1:,} samples...\")\n",
    "    \n",
    "    # Adjust delay based on success rate\n",
    "    if success_count + error_count > 0:\n",
    "        success_rate = success_count / (success_count + error_count)\n",
    "        if success_rate < 0.5:  # Low success rate, increase delay\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            time.sleep(0.3)\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(f\"Embedding extraction complete\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nTime elapsed: {elapsed_total:.1f} seconds ({elapsed_total/60:.1f} minutes)\")\n",
    "print(f\"Completion time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Success: {success_count:,}/{total_samples:,} ({success_count/total_samples*100:.2f}%)\")\n",
    "print(f\"  Failed: {error_count:,}/{total_samples:,} ({error_count/total_samples*100:.2f}%)\")\n",
    "print(f\"  Average rate: {total_samples/elapsed_total:.2f} samples/second\")\n",
    "\n",
    "# Convert to numpy array\n",
    "X_full = np.array(all_embeddings)\n",
    "print(f\"\\nEmbedding matrix:\")\n",
    "print(f\"  Shape: {X_full.shape}\")\n",
    "print(f\"  Data type: {X_full.dtype}\")\n",
    "print(f\"  Memory usage: {X_full.nbytes / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Use Ridge Models (Gemma-2-9B) to Generate OCEAN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Step 2: Generate OCEAN features using Gemma-2-9B Ridge models\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Standardize embeddings\n",
    "print(f\"\\nStandardizing embeddings...\")\n",
    "X_full_scaled = scaler.transform(X_full)\n",
    "print(f\"Standardization complete\")\n",
    "\n",
    "# Use Ridge models to predict OCEAN scores\n",
    "print(f\"\\nUsing Gemma-2-9B Ridge models to predict OCEAN scores...\")\n",
    "ocean_predictions = {}\n",
    "\n",
    "for dim in OCEAN_DIMS:\n",
    "    print(f\"  {dim}...\", end=' ', flush=True)\n",
    "    model = ridge_models[dim]\n",
    "    predictions = model.predict(X_full_scaled)\n",
    "    \n",
    "    # Clip to [0, 1] range\n",
    "    predictions = np.clip(predictions, 0.0, 1.0)\n",
    "    ocean_predictions[dim] = predictions\n",
    "    \n",
    "    print(f\"mean={predictions.mean():.3f}, std={predictions.std():.3f}\")\n",
    "\n",
    "print(f\"\\nOCEAN feature generation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 6: Merge Data and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Step 3: Merge data and save (Gemma-2-9B)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Create dataset with OCEAN features\n",
    "print(f\"\\nCreating dataset with OCEAN features...\")\n",
    "df_full_with_ocean = df_full.copy()\n",
    "\n",
    "for dim in OCEAN_DIMS:\n",
    "    df_full_with_ocean[dim] = ocean_predictions[dim]\n",
    "\n",
    "print(f\"Data merge complete\")\n",
    "print(f\"  Original columns: {len(df_full.columns)}\")\n",
    "print(f\"  New OCEAN columns: {len(OCEAN_DIMS)}\")\n",
    "print(f\"  Total columns: {len(df_full_with_ocean.columns)}\")\n",
    "print(f\"  Total rows: {len(df_full_with_ocean)}\")\n",
    "\n",
    "# Save complete dataset\n",
    "print(f\"\\nSaving complete dataset (Gemma-2-9B)...\")\n",
    "output_csv = '../loan_with_ocean_gemma.csv'\n",
    "df_full_with_ocean.to_csv(output_csv, index=False)\n",
    "print(f\"Saved: {output_csv}\")\n",
    "print(f\"  File size: {os.path.getsize(output_csv) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Save embeddings\n",
    "print(f\"\\nSaving embeddings...\")\n",
    "embedding_file = '../bge_large_embeddings_full.npy'\n",
    "np.save(embedding_file, X_full)\n",
    "print(f\"Saved: {embedding_file}\")\n",
    "print(f\"  File size: {os.path.getsize(embedding_file) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Verify data\n",
    "print(f\"\\nData verification...\")\n",
    "print(f\"Row count match: {len(df_full_with_ocean) == len(X_full)}\")\n",
    "print(f\"OCEAN columns exist: {all(col in df_full_with_ocean.columns for col in OCEAN_DIMS)}\")\n",
    "print(f\"\\nOCEAN feature statistics (Gemma-2-9B):\")\n",
    "for col in OCEAN_DIMS:\n",
    "    print(f\"  {col:20s}: mean={df_full_with_ocean[col].mean():.3f}, std={df_full_with_ocean[col].std():.3f}, \"\n",
    "          f\"min={df_full_with_ocean[col].min():.3f}, max={df_full_with_ocean[col].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 7: Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Generating summary report (Gemma-2-9B)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Create summary report\n",
    "summary = {\n",
    "    'phase': '05g - Apply Ridge to Full Data (Gemma-2-9B)',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'llm_model': 'Gemma-2-9B',\n",
    "    'embedding_model': 'BAAI/bge-large-en-v1.5',\n",
    "    'total_samples': int(total_samples),\n",
    "    'embedding_success_count': int(success_count),\n",
    "    'embedding_error_count': int(error_count),\n",
    "    'embedding_success_rate': f\"{success_count/total_samples*100:.2f}%\",\n",
    "    'embedding_dimension': 1024,\n",
    "    'processing_time_seconds': elapsed_total,\n",
    "    'processing_time_minutes': elapsed_total / 60,\n",
    "    'processing_time_hours': elapsed_total / 3600,\n",
    "    'samples_per_second': total_samples / elapsed_total if elapsed_total > 0 else 0,\n",
    "    'output_files': {\n",
    "        'csv': output_csv,\n",
    "        'embeddings': embedding_file\n",
    "    },\n",
    "    'ocean_dimensions': OCEAN_DIMS,\n",
    "    'ocean_statistics': {}\n",
    "}\n",
    "\n",
    "# Add OCEAN statistics\n",
    "for dim in OCEAN_DIMS:\n",
    "    summary['ocean_statistics'][dim] = {\n",
    "        'mean': float(df_full_with_ocean[dim].mean()),\n",
    "        'std': float(df_full_with_ocean[dim].std()),\n",
    "        'min': float(df_full_with_ocean[dim].min()),\n",
    "        'max': float(df_full_with_ocean[dim].max()),\n",
    "        'median': float(df_full_with_ocean[dim].median())\n",
    "    }\n",
    "\n",
    "# Save report\n",
    "summary_file = '../05g_full_data_summary_gemma.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary report saved: {summary_file}\")\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(\"Final Summary (Gemma-2-9B)\")\n",
    "print(\"=\"*100)\n",
    "print(json.dumps(summary, indent=2, default=str))\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(\"05g Complete (Gemma-2-9B)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  1. {output_csv}\")\n",
    "print(f\"     - 34,529 rows x {len(df_full_with_ocean.columns)} columns\")\n",
    "print(f\"     - Contains all original features + 5 OCEAN features (Gemma-2-9B)\")\n",
    "print(f\"\\n  2. {embedding_file}\")\n",
    "print(f\"     - 34,529 x 1024 embeddings\")\n",
    "print(f\"\\n  3. {summary_file}\")\n",
    "print(f\"     - Processing report and statistics\")\n",
    "print(f\"\\nNext step: Run XGBoost training with Gemma-2-9B OCEAN features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Step 05g Complete (Gemma-2-9B)\n",
    "\n",
    "**Key Achievements**:\n",
    "- Extracted BGE-Large embeddings for 34,529 samples\n",
    "- Generated 5 OCEAN features using Gemma-2-9B Ridge models\n",
    "- Created complete dataset with OCEAN features\n",
    "\n",
    "**Output Files**:\n",
    "- `loan_with_ocean_gemma.csv` - Complete dataset\n",
    "- `bge_large_embeddings_full.npy` - Embeddings\n",
    "- `05g_full_data_summary_gemma.json` - Report\n",
    "\n",
    "**Next Steps**:\n",
    "1. Use Gemma-2-9B OCEAN features for XGBoost training\n",
    "2. Compare Gemma-2-9B performance with other models\n",
    "3. Evaluate OCEAN feature impact"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
