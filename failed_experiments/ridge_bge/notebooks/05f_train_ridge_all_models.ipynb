{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05f - Train Ridge Regression Models (All LLMs)\n",
    "\n",
    "**Purpose**: Train Ridge regression models for all 5 LLMs to learn the mapping from BGE embeddings to OCEAN scores\n",
    "\n",
    "**Why Ridge?**\n",
    "- Simple linear model with L2 regularization\n",
    "- Baseline comparison for more complex methods (Elastic Net, PCA, etc.)\n",
    "- Fast training and inference\n",
    "\n",
    "**Input Files**:\n",
    "- bge_embeddings_500.npy - BGE embeddings (500x1024)\n",
    "- ocean_ground_truth/[llm]_ocean_500.csv - OCEAN ground truth for each LLM\n",
    "\n",
    "**Output Files** (per LLM):\n",
    "- ridge_models_[llm].pkl - 5 Ridge models + Scaler\n",
    "- 05f_ridge_training_report_[llm].json - Training report\n",
    "\n",
    "**Summary Output**:\n",
    "- 05f_ridge_comparison.csv - Performance comparison across LLMs\n",
    "- 05f_ridge_visualization.png - Performance visualization\n",
    "\n",
    "**Estimated Time**: Approximately 5-10 minutes (5 LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM configurations\n",
    "LLM_CONFIGS = {\n",
    "    'llama': {\n",
    "        'name': 'Llama-3.1-8B',\n",
    "        'ocean_file': '../ocean_ground_truth/llama_3.1_8b_ocean_500.csv'\n",
    "    },\n",
    "    'gpt': {\n",
    "        'name': 'GPT-OSS-120B',\n",
    "        'ocean_file': '../ocean_ground_truth/gpt_oss_120b_ocean_500.csv'\n",
    "    },\n",
    "    'gemma': {\n",
    "        'name': 'Gemma-2-9B',\n",
    "        'ocean_file': '../ocean_ground_truth/gemma_2_9b_ocean_500.csv'\n",
    "    },\n",
    "    'deepseek': {\n",
    "        'name': 'DeepSeek-V3.1',\n",
    "        'ocean_file': '../ocean_ground_truth/deepseek_v3.1_ocean_500.csv'\n",
    "    },\n",
    "    'qwen': {\n",
    "        'name': 'Qwen-2.5-72B',\n",
    "        'ocean_file': '../ocean_ground_truth/qwen_2.5_72b_ocean_500.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# OCEAN dimensions\n",
    "OCEAN_DIMS = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "\n",
    "# Ridge hyperparameters\n",
    "RIDGE_ALPHA = 1.0  # Default Ridge regularization strength\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  LLM models: {len(LLM_CONFIGS)}\")\n",
    "print(f\"  OCEAN dimensions: {len(OCEAN_DIMS)}\")\n",
    "print(f\"  Ridge alpha: {RIDGE_ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load BGE Embeddings (Shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading BGE Embeddings\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "embedding_file = '../bge_embeddings_500.npy'\n",
    "print(f\"\\nLoading: {embedding_file}\")\n",
    "X_full = np.load(embedding_file)\n",
    "print(f\"Embeddings shape: {X_full.shape}\")\n",
    "print(f\"  Data type: {X_full.dtype}\")\n",
    "print(f\"  Memory usage: {X_full.nbytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  Value range: [{X_full.min():.4f}, {X_full.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Ridge Models for Each LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for all results\n",
    "all_results = {}\n",
    "\n",
    "for llm_key, llm_config in LLM_CONFIGS.items():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Training Ridge Models: {llm_config['name']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load OCEAN targets\n",
    "    print(f\"\\n[1/6] Loading OCEAN targets...\")\n",
    "    ocean_file = llm_config['ocean_file']\n",
    "    y_df = pd.read_csv(ocean_file)\n",
    "    print(f\"  Shape: {y_df.shape}\")\n",
    "    print(f\"  Columns: {y_df.columns.tolist()}\")\n",
    "    \n",
    "    # Check and handle NaN values\n",
    "    nan_count_total = y_df.isnull().sum().sum()\n",
    "    if nan_count_total > 0:\n",
    "        print(f\"  Warning: Found {nan_count_total} NaN values\")\n",
    "        nan_indices = y_df[y_df.isnull().any(axis=1)].index\n",
    "        y_df = y_df.dropna()\n",
    "        X = np.delete(X_full, nan_indices, axis=0)\n",
    "        print(f\"  After dropping NaN: {len(y_df)} samples\")\n",
    "    else:\n",
    "        X = X_full.copy()\n",
    "    \n",
    "    # Verify consistency\n",
    "    if len(X) != len(y_df):\n",
    "        raise ValueError(f\"Data inconsistency: X={len(X)}, y={len(y_df)}\")\n",
    "    \n",
    "    # Train/test split\n",
    "    print(f\"\\n[2/6] Splitting data (80/20)...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_df,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    print(f\"  Training: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "    print(f\"  Feature-to-sample ratio: {X_train.shape[1] / X_train.shape[0]:.2f}:1\")\n",
    "    \n",
    "    # Standardize\n",
    "    print(f\"\\n[3/6] Standardizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(f\"  Train mean={X_train_scaled.mean():.6f}, std={X_train_scaled.std():.6f}\")\n",
    "    print(f\"  Test mean={X_test_scaled.mean():.6f}, std={X_test_scaled.std():.6f}\")\n",
    "    \n",
    "    # Train models\n",
    "    print(f\"\\n[4/6] Training Ridge models (5 dimensions)...\")\n",
    "    print(f\"  Ridge alpha: {RIDGE_ALPHA}\")\n",
    "    \n",
    "    ridge_models = {}\n",
    "    training_results = {}\n",
    "    \n",
    "    for i, dim in enumerate(OCEAN_DIMS):\n",
    "        print(f\"\\n  [{i+1}/5] Training {dim}...\")\n",
    "        \n",
    "        # Get target\n",
    "        y_train_dim = y_train[dim].values\n",
    "        y_test_dim = y_test[dim].values\n",
    "        \n",
    "        # Train Ridge\n",
    "        model = Ridge(\n",
    "            alpha=RIDGE_ALPHA,\n",
    "            random_state=RANDOM_STATE,\n",
    "            max_iter=10000\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train_dim)\n",
    "        \n",
    "        # Predict\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Metrics\n",
    "        train_r2 = r2_score(y_train_dim, y_train_pred)\n",
    "        test_r2 = r2_score(y_test_dim, y_test_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_dim, y_train_pred))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test_dim, y_test_pred))\n",
    "        train_mae = mean_absolute_error(y_train_dim, y_train_pred)\n",
    "        test_mae = mean_absolute_error(y_test_dim, y_test_pred)\n",
    "        \n",
    "        # Save model and results\n",
    "        ridge_models[dim] = model\n",
    "        training_results[dim] = {\n",
    "            'train_r2': float(train_r2),\n",
    "            'test_r2': float(test_r2),\n",
    "            'train_rmse': float(train_rmse),\n",
    "            'test_rmse': float(test_rmse),\n",
    "            'train_mae': float(train_mae),\n",
    "            'test_mae': float(test_mae),\n",
    "            'model_coef_shape': model.coef_.shape,\n",
    "            'model_intercept': float(model.intercept_)\n",
    "        }\n",
    "        \n",
    "        print(f\"      Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "        print(f\"      Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "    \n",
    "    # Save models\n",
    "    print(f\"\\n[5/6] Saving models...\")\n",
    "    model_data = {\n",
    "        'models': ridge_models,\n",
    "        'scaler': scaler,\n",
    "        'ocean_dims': OCEAN_DIMS,\n",
    "        'training_results': training_results,\n",
    "        'training_timestamp': datetime.now().isoformat(),\n",
    "        'llm_model': llm_config['name'],\n",
    "        'hyperparameters': {\n",
    "            'alpha': RIDGE_ALPHA\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    model_file = f'../ridge_models_{llm_key}.pkl'\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    print(f\"  Saved: {model_file} ({os.path.getsize(model_file) / 1024:.1f} KB)\")\n",
    "    \n",
    "    # Generate report\n",
    "    print(f\"\\n[6/6] Generating training report...\")\n",
    "    report = {\n",
    "        'phase': f'05f - Train Ridge Models ({llm_config[\"name\"]})',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'llm_model': llm_config['name'],\n",
    "        'embedding_model': 'BAAI/bge-large-en-v1.5',\n",
    "        'embedding_dimension': 1024,\n",
    "        'training_samples': int(X_train.shape[0]),\n",
    "        'test_samples': int(X_test.shape[0]),\n",
    "        'model_type': 'Ridge Regression',\n",
    "        'model_alpha': RIDGE_ALPHA,\n",
    "        'ocean_dimensions': OCEAN_DIMS,\n",
    "        'model_file': model_file,\n",
    "        'training_results': training_results\n",
    "    }\n",
    "    \n",
    "    # Summary metrics\n",
    "    test_r2_scores = [training_results[dim]['test_r2'] for dim in OCEAN_DIMS]\n",
    "    test_rmse_scores = [training_results[dim]['test_rmse'] for dim in OCEAN_DIMS]\n",
    "    test_mae_scores = [training_results[dim]['test_mae'] for dim in OCEAN_DIMS]\n",
    "    \n",
    "    report['summary_metrics'] = {\n",
    "        'avg_test_r2': float(np.mean(test_r2_scores)),\n",
    "        'avg_test_rmse': float(np.mean(test_rmse_scores)),\n",
    "        'avg_test_mae': float(np.mean(test_mae_scores)),\n",
    "        'min_test_r2': float(np.min(test_r2_scores)),\n",
    "        'max_test_r2': float(np.max(test_r2_scores))\n",
    "    }\n",
    "    \n",
    "    report_file = f'../05f_ridge_training_report_{llm_key}.json'\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"  Report saved: {report_file}\")\n",
    "    \n",
    "    # Store for final comparison\n",
    "    all_results[llm_key] = {\n",
    "        'name': llm_config['name'],\n",
    "        'training_results': training_results,\n",
    "        'summary': report['summary_metrics']\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n  Summary for {llm_config['name']}:\")\n",
    "    print(f\"    Avg Test R²: {report['summary_metrics']['avg_test_r2']:.4f}\")\n",
    "    print(f\"    Test R² range: [{report['summary_metrics']['min_test_r2']:.4f}, {report['summary_metrics']['max_test_r2']:.4f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All Ridge models trained successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generating Comparison Report\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for llm_key, results in all_results.items():\n",
    "    for dim in OCEAN_DIMS:\n",
    "        ridge_r2 = results['training_results'][dim]['test_r2']\n",
    "        ridge_rmse = results['training_results'][dim]['test_rmse']\n",
    "        ridge_mae = results['training_results'][dim]['test_mae']\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'LLM': results['name'],\n",
    "            'llm_key': llm_key,\n",
    "            'Dimension': dim,\n",
    "            'Test_R2': ridge_r2,\n",
    "            'Test_RMSE': ridge_rmse,\n",
    "            'Test_MAE': ridge_mae\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_file = '../05f_ridge_comparison.csv'\n",
    "comparison_df.to_csv(comparison_file, index=False)\n",
    "print(f\"\\nComparison table saved: {comparison_file}\")\n",
    "print(f\"\\nPreview:\")\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Ridge Regression Performance (All LLMs, BGE Embeddings)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Test R² by Model-Dimension\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in comparison_df['Test_R2']]\n",
    "ax1.bar(x_pos, comparison_df['Test_R2'], color=colors, alpha=0.8)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_xlabel('Model-Dimension', fontsize=10)\n",
    "ax1.set_ylabel('Test R² Score', fontsize=10)\n",
    "ax1.set_title('Test R² by Model-Dimension', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=90, labelsize=7)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f\"{row['llm_key'][:3]}-{row['Dimension'][:3]}\" for _, row in comparison_df.iterrows()])\n",
    "\n",
    "# 2. Average R² by LLM\n",
    "ax2 = axes[0, 1]\n",
    "avg_by_llm = comparison_df.groupby('LLM')['Test_R2'].mean().sort_values(ascending=False)\n",
    "colors_llm = ['#e74c3c' if x < 0 else '#2ecc71' for x in avg_by_llm.values]\n",
    "avg_by_llm.plot(kind='bar', ax=ax2, color=colors_llm, alpha=0.8)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_xlabel('LLM Model', fontsize=10)\n",
    "ax2.set_ylabel('Average Test R²', fontsize=10)\n",
    "ax2.set_title('Average Test R² by LLM', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Average R² by OCEAN Dimension\n",
    "ax3 = axes[1, 0]\n",
    "avg_by_dim = comparison_df.groupby('Dimension')['Test_R2'].mean().sort_values(ascending=False)\n",
    "colors_dim = ['#e74c3c' if x < 0 else '#2ecc71' for x in avg_by_dim.values]\n",
    "avg_by_dim.plot(kind='barh', ax=ax3, color=colors_dim, alpha=0.8)\n",
    "ax3.axvline(x=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax3.set_xlabel('Average Test R²', fontsize=10)\n",
    "ax3.set_ylabel('OCEAN Dimension', fontsize=10)\n",
    "ax3.set_title('Average Test R² by OCEAN Dimension', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. RMSE by LLM\n",
    "ax4 = axes[1, 1]\n",
    "avg_rmse_by_llm = comparison_df.groupby('LLM')['Test_RMSE'].mean().sort_values()\n",
    "avg_rmse_by_llm.plot(kind='barh', ax=ax4, color='#3498db', alpha=0.8)\n",
    "ax4.set_xlabel('Average Test RMSE', fontsize=10)\n",
    "ax4.set_ylabel('LLM Model', fontsize=10)\n",
    "ax4.set_title('Average Test RMSE by LLM (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "viz_file = '../05f_ridge_visualization.png'\n",
    "plt.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nVisualization saved: {viz_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY - Ridge Regression (BGE Embeddings)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Overall Performance:\")\n",
    "print(f\"   Average Test R² across all models: {comparison_df['Test_R2'].mean():.4f}\")\n",
    "print(f\"   Best Test R²: {comparison_df['Test_R2'].max():.4f}\")\n",
    "print(f\"   Worst Test R²: {comparison_df['Test_R2'].min():.4f}\")\n",
    "print(f\"   Std Dev: {comparison_df['Test_R2'].std():.4f}\")\n",
    "print(f\"   Models with positive R²: {(comparison_df['Test_R2'] > 0).sum()}/{len(comparison_df)}\")\n",
    "\n",
    "print(\"\\n2. Best Performing LLM:\")\n",
    "best_llm = comparison_df.groupby('LLM')['Test_R2'].mean().idxmax()\n",
    "best_r2 = comparison_df.groupby('LLM')['Test_R2'].mean().max()\n",
    "print(f\"   {best_llm}: {best_r2:.4f}\")\n",
    "\n",
    "print(\"\\n3. Best Performing Dimension:\")\n",
    "best_dim = comparison_df.groupby('Dimension')['Test_R2'].mean().idxmax()\n",
    "best_dim_r2 = comparison_df.groupby('Dimension')['Test_R2'].mean().max()\n",
    "print(f\"   {best_dim}: {best_dim_r2:.4f}\")\n",
    "\n",
    "print(\"\\n4. Worst Performing:\")\n",
    "worst_row = comparison_df.loc[comparison_df['Test_R2'].idxmin()]\n",
    "print(f\"   {worst_row['LLM']} - {worst_row['Dimension']}: {worst_row['Test_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Output Files Generated:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Models:\")\n",
    "for llm_key in LLM_CONFIGS.keys():\n",
    "    print(f\"  - ridge_models_{llm_key}.pkl\")\n",
    "print(\"\\nReports:\")\n",
    "for llm_key in LLM_CONFIGS.keys():\n",
    "    print(f\"  - 05f_ridge_training_report_{llm_key}.json\")\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  - 05f_ridge_comparison.csv\")\n",
    "print(f\"  - 05f_ridge_visualization.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"05f Ridge Training Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: Ridge shows severe overfitting (train R² ≈ 0.999, test R² < 0)\")\n",
    "print(\"Consider using Elastic Net (05f_train_elasticnet_all_models.ipynb) for better results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
