{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama OCEAN ç‰¹å¾ç”Ÿæˆå®Œæ•´æµç¨‹\n",
    "## ä½¿ç”¨ Llama-3.1-8B-Instruct æ¨¡å‹ï¼ˆå…è´¹ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### æµç¨‹æ¦‚è§ˆ\n",
    "\n",
    "1. **Llama æ‰“æ ‡ç­¾** (500æ ·æœ¬) â†’ OCEAN ground truth\n",
    "2. **å­¦ä¹ æƒé‡** (Ridge Regression) â†’ categorical â†’ OCEAN æ˜ å°„\n",
    "3. **ç”Ÿæˆç‰¹å¾** (å…¨é‡æ•°æ®) â†’ 10000è¡Œ Ã— 5åˆ— OCEAN\n",
    "4. **XGBoost å¯¹æ¯”** â†’ Baseline vs Baseline+OCEAN\n",
    "\n",
    "**æˆæœ¬**: $0 (Llama å…è´¹)  \n",
    "**æ—¶é—´**: ~20åˆ†é’Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# é¡¹ç›®æ¨¡å—\n",
    "from utils.io import load_lending_club_data, prepare_binary_target\n",
    "from utils.seed import set_seed\n",
    "from utils.metrics import compute_all_metrics, delong_test\n",
    "from text_features.ocean_llama_labeler import OceanLlamaLabeler, OCEAN_DIMS\n",
    "from utils.ocean_weight_learner import OceanWeightLearner\n",
    "from utils.ocean_feature_generator import OceanFeatureGenerator\n",
    "from utils.ocean_evaluator import OceanEvaluator\n",
    "\n",
    "set_seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. åŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½æ•°æ®\n",
    "path = kagglehub.dataset_download(\"ethon0426/lending-club-20072020q1\")\n",
    "file_path = path + \"/Loan_status_2007-2020Q3.gzip\"\n",
    "\n",
    "# åŠ è½½æ•°æ®ï¼ˆå…ˆç”¨ 10000 è¡Œæµ‹è¯•ï¼‰\n",
    "ROW_LIMIT = 10000\n",
    "\n",
    "df = load_lending_club_data(file_path, row_limit=ROW_LIMIT)\n",
    "df = prepare_binary_target(df, target_col=\"loan_status\")\n",
    "\n",
    "print(f\"\\næ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "print(f\"è¿çº¦ç‡: {df['target'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Llama æ‰“æ ‡ç­¾ï¼ˆç”Ÿæˆ Ground Truthï¼‰\n",
    "\n",
    "**è¯´æ˜**: ä½¿ç”¨ Llama æ¨¡å‹ç»™ 500 ä¸ªæ ·æœ¬æ‰“ OCEAN äººæ ¼åˆ†æ•°  \n",
    "**æˆæœ¬**: $0 (å…è´¹)  \n",
    "**æ—¶é—´**: ~10-15åˆ†é’Ÿ  \n",
    "\n",
    "**âš ï¸ é‡è¦**: éœ€è¦å…ˆé…ç½® `.env` æ–‡ä»¶ä¸­çš„ `HF_TOKEN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–æ ‡æ³¨å™¨\n",
    "labeler = OceanLlamaLabeler()\n",
    "\n",
    "# æ‰¹é‡æ‰“æ ‡ç­¾ï¼ˆ500æ ·æœ¬ï¼Œåˆ†å±‚æŠ½æ ·ï¼‰\n",
    "df_truth = labeler.label_batch(\n",
    "    df, \n",
    "    sample_size=500, \n",
    "    stratified=True,\n",
    "    rate_limit_delay=0.5  # API é™æµ\n",
    ")\n",
    "\n",
    "# ä¿å­˜ ground truth\n",
    "df_truth.to_csv('../artifacts/ground_truth_llama.csv', index=False)\n",
    "print(\"\\nâœ… Ground Truth å·²ä¿å­˜åˆ°: artifacts/ground_truth_llama.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 è¯„ä¼° Ground Truth è´¨é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = OceanEvaluator()\n",
    "truth_quality = evaluator.evaluate_ground_truth_quality(df_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å­¦ä¹ æƒé‡ï¼ˆRidge Regressionï¼‰\n",
    "\n",
    "**è¯´æ˜**: å­¦ä¹  categorical variables â†’ OCEAN çš„æ˜ å°„è§„å¾‹  \n",
    "**æ–¹æ³•**: Ridge Regression (L2 æ­£åˆ™åŒ–)  \n",
    "**è¾“å‡º**: æ¯ä¸ª OCEAN ç»´åº¦çš„æƒé‡ç³»æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ categorical variables\n",
    "CATEGORICAL_VARS = [\n",
    "    'grade', 'purpose', 'term', 'home_ownership',\n",
    "    'emp_length', 'verification_status', 'application_type'\n",
    "]\n",
    "\n",
    "# è¿‡æ»¤å­˜åœ¨çš„åˆ—\n",
    "CATEGORICAL_VARS = [c for c in CATEGORICAL_VARS if c in df_truth.columns]\n",
    "\n",
    "print(f\"ä½¿ç”¨çš„ categorical variables: {CATEGORICAL_VARS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–å­¦ä¹ å™¨\n",
    "learner = OceanWeightLearner(method='ridge', alpha=0.1)\n",
    "\n",
    "# å­¦ä¹ æƒé‡\n",
    "weights, encoder = learner.fit(\n",
    "    X_categorical=df_truth[CATEGORICAL_VARS],\n",
    "    y_ocean_truth=df_truth[[f'{d}_truth' for d in OCEAN_DIMS]],\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# ä¿å­˜æƒé‡\n",
    "joblib.dump(\n",
    "    {'weights': weights, 'encoder': encoder},\n",
    "    '../artifacts/ocean_weights_llama.pkl'\n",
    ")\n",
    "print(\"\\nâœ… æƒé‡å·²ä¿å­˜åˆ°: artifacts/ocean_weights_llama.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 æŸ¥çœ‹å­¦ä¹ ç»“æœæ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æŸ¥çœ‹å„ç»´åº¦ Top ç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in OCEAN_DIMS:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"{dim.upper()} - Top 10 ç‰¹å¾\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    display(learner.get_top_features(dim, top_n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ç”Ÿæˆå…¨é‡ OCEAN ç‰¹å¾\n",
    "\n",
    "**è¯´æ˜**: ä½¿ç”¨å­¦åˆ°çš„æƒé‡ç»™å…¨é‡æ•°æ®ï¼ˆ10000è¡Œï¼‰ç”Ÿæˆ OCEAN ç‰¹å¾  \n",
    "**æˆæœ¬**: $0 (æœ¬åœ°è®¡ç®—)  \n",
    "**æ—¶é—´**: å³æ—¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–ç”Ÿæˆå™¨\n",
    "generator = OceanFeatureGenerator(weights, encoder)\n",
    "\n",
    "# ç”Ÿæˆ OCEAN ç‰¹å¾\n",
    "df_full = generator.generate_features(df)\n",
    "\n",
    "print(\"\\nâœ… OCEAN ç‰¹å¾å·²æ·»åŠ åˆ°æ•°æ®é›†\")\n",
    "print(f\"æ–°å¢åˆ—: {OCEAN_DIMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 è¯„ä¼°ç”Ÿæˆç‰¹å¾çš„é¢„æµ‹èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_power = evaluator.evaluate_predictive_power(df_full, target_col='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‡æ€»æŠ¥å‘Š\n",
    "summary = evaluator.generate_summary_report()\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost A/B å¯¹æ¯”æµ‹è¯•\n",
    "\n",
    "**å¯¹æ¯”æ–¹æ¡ˆ**:\n",
    "- **æ–¹æ¡ˆ A**: Baseline (ç»“æ„åŒ–å˜é‡)\n",
    "- **æ–¹æ¡ˆ B**: Baseline + OCEAN (5ä¸ªæ€§æ ¼ç‰¹å¾)\n",
    "\n",
    "**è¯„ä¼°æŒ‡æ ‡**: ROC-AUC, PR-AUC, KS, Brier Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ baseline ç‰¹å¾\n",
    "numeric_features = [\n",
    "    \"loan_amnt\", \"int_rate\", \"installment\", \"annual_inc\", \"dti\",\n",
    "    \"inq_last_6mths\", \"open_acc\", \"pub_rec\", \"revol_bal\", \"revol_util\",\n",
    "    \"total_acc\"\n",
    "]\n",
    "numeric_features = [c for c in numeric_features if c in df_full.columns]\n",
    "\n",
    "categorical_features_model = [c for c in CATEGORICAL_VARS if c in df_full.columns]\n",
    "\n",
    "baseline_features = numeric_features + categorical_features_model\n",
    "ocean_features = OCEAN_DIMS\n",
    "\n",
    "print(f\"Baseline ç‰¹å¾æ•°: {len(baseline_features)}\")\n",
    "print(f\"OCEAN ç‰¹å¾æ•°: {len(ocean_features)}\")\n",
    "print(f\"æ€»ç‰¹å¾æ•° (Baseline+OCEAN): {len(baseline_features) + len(ocean_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„å¤„ç†ï¼šOneHot ç¼–ç  categorical features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# æ•°å€¼ç‰¹å¾å¤„ç†\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "# categorical ç‰¹å¾å¤„ç†\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "])\n",
    "\n",
    "# æ–¹æ¡ˆ A: Baseline\n",
    "preprocessor_A = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features_model),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# æ–¹æ¡ˆ B: Baseline + OCEAN\n",
    "preprocessor_B = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"ocean\", \"passthrough\", ocean_features),  # OCEAN ç›´æ¥é€šè¿‡\n",
    "        (\"cat\", categorical_transformer, categorical_features_model),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "y = df_full['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_full, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬\")\n",
    "print(f\"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬\")\n",
    "print(f\"è®­ç»ƒé›†è¿çº¦ç‡: {y_train.mean():.2%}\")\n",
    "print(f\"æµ‹è¯•é›†è¿çº¦ç‡: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 è®­ç»ƒæ–¹æ¡ˆ A: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nè®­ç»ƒæ–¹æ¡ˆ A: Baseline (æ—  OCEAN)...\\n\")\n",
    "\n",
    "# é¢„å¤„ç†\n",
    "X_train_A = preprocessor_A.fit_transform(X_train)\n",
    "X_test_A = preprocessor_A.transform(X_test)\n",
    "\n",
    "# è®­ç»ƒ XGBoost\n",
    "pos = int((y_train == 1).sum())\n",
    "neg = int((y_train == 0).sum())\n",
    "scale_pos_weight = neg / max(1, pos)\n",
    "\n",
    "model_A = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric=\"auc\"\n",
    ")\n",
    "\n",
    "model_A.fit(X_train_A, y_train, verbose=False)\n",
    "\n",
    "# é¢„æµ‹\n",
    "y_proba_A = model_A.predict_proba(X_test_A)[:, 1]\n",
    "metrics_A = compute_all_metrics(y_test, y_proba_A)\n",
    "\n",
    "print(\"\\næ–¹æ¡ˆ A ç»“æœ:\")\n",
    "for k, v in metrics_A.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 è®­ç»ƒæ–¹æ¡ˆ B: Baseline + OCEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nè®­ç»ƒæ–¹æ¡ˆ B: Baseline + OCEAN...\\n\")\n",
    "\n",
    "# é¢„å¤„ç†\n",
    "X_train_B = preprocessor_B.fit_transform(X_train)\n",
    "X_test_B = preprocessor_B.transform(X_test)\n",
    "\n",
    "# è®­ç»ƒ XGBoost\n",
    "model_B = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric=\"auc\"\n",
    ")\n",
    "\n",
    "model_B.fit(X_train_B, y_train, verbose=False)\n",
    "\n",
    "# é¢„æµ‹\n",
    "y_proba_B = model_B.predict_proba(X_test_B)[:, 1]\n",
    "metrics_B = compute_all_metrics(y_test, y_proba_B)\n",
    "\n",
    "print(\"\\næ–¹æ¡ˆ B ç»“æœ:\")\n",
    "for k, v in metrics_B.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 å¯¹æ¯”ç»“æœä¸ç»Ÿè®¡æ˜¾è‘—æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = evaluator.compare_models(y_test, y_proba_A, y_proba_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 å¯è§†åŒ–å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ROC Curve\n",
    "fpr_A, tpr_A, _ = roc_curve(y_test, y_proba_A)\n",
    "fpr_B, tpr_B, _ = roc_curve(y_test, y_proba_B)\n",
    "\n",
    "axes[0].plot(fpr_A, tpr_A, label=f\"Baseline (AUC={auc(fpr_A, tpr_A):.3f})\", linewidth=2)\n",
    "axes[0].plot(fpr_B, tpr_B, label=f\"+ OCEAN (AUC={auc(fpr_B, tpr_B):.3f})\", linewidth=2)\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec_A, rec_A, _ = precision_recall_curve(y_test, y_proba_A)\n",
    "prec_B, rec_B, _ = precision_recall_curve(y_test, y_proba_B)\n",
    "\n",
    "axes[1].plot(rec_A, prec_A, label=f\"Baseline (PR-AUC={metrics_A['pr_auc']:.3f})\", linewidth=2)\n",
    "axes[1].plot(rec_B, prec_B, label=f\"+ OCEAN (PR-AUC={metrics_B['pr_auc']:.3f})\", linewidth=2)\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/results/llama_ocean_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ä¿å­˜ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "joblib.dump(model_B, '../artifacts/xgb_ocean_llama.pkl')\n",
    "print(\"âœ… æ¨¡å‹å·²ä¿å­˜: artifacts/xgb_ocean_llama.pkl\")\n",
    "\n",
    "# ä¿å­˜å¯¹æ¯”ç»“æœ\n",
    "results_df = pd.DataFrame([\n",
    "    {'model': 'Baseline', **metrics_A},\n",
    "    {'model': 'Baseline+OCEAN', **metrics_B}\n",
    "])\n",
    "results_df.to_csv('../artifacts/results/llama_ocean_results.csv', index=False)\n",
    "print(\"âœ… ç»“æœå·²ä¿å­˜: artifacts/results/llama_ocean_results.csv\")\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€»ç»“\n",
    "\n",
    "### âœ… å®Œæˆçš„å·¥ä½œ\n",
    "\n",
    "1. ä½¿ç”¨ Llama æ¨¡å‹ä¸º 500 ä¸ªæ ·æœ¬ç”Ÿæˆ OCEAN ground truth\n",
    "2. ä½¿ç”¨ Ridge Regression å­¦ä¹  categorical â†’ OCEAN æ˜ å°„æƒé‡\n",
    "3. ä¸ºå…¨é‡ 10000 æ ·æœ¬ç”Ÿæˆ OCEAN ç‰¹å¾\n",
    "4. XGBoost A/B å¯¹æ¯”æµ‹è¯•\n",
    "\n",
    "### ğŸ“Š ç»“æœæ‘˜è¦\n",
    "\n",
    "ï¼ˆè¿è¡Œåå¡«å†™ï¼‰\n",
    "\n",
    "- **ROC-AUC æå‡**: +_____ \n",
    "- **PR-AUC æå‡**: +_____\n",
    "- **KS æå‡**: +_____\n",
    "- **ç»Ÿè®¡æ˜¾è‘—æ€§**: p = _____\n",
    "\n",
    "### ğŸ’° æˆæœ¬\n",
    "\n",
    "- **æ€»æˆæœ¬**: $0 (Llama å…è´¹)\n",
    "- **æ€»æ—¶é—´**: ~20åˆ†é’Ÿ\n",
    "\n",
    "### ğŸ¯ ä¸‹ä¸€æ­¥\n",
    "\n",
    "1. å¦‚æœæ•ˆæœæ˜¾è‘—ï¼šæ‰©å±•åˆ°å®Œæ•´æ•°æ®é›†ï¼ˆ100k+æ ·æœ¬ï¼‰\n",
    "2. å°è¯•å…¶ä»–æ–‡æœ¬æºï¼ˆå¦‚æœæœ‰ `desc` å­—æ®µï¼‰\n",
    "3. ç‰¹å¾è§£é‡Šåˆ†æï¼ˆSHAPï¼‰\n",
    "4. ç”Ÿäº§éƒ¨ç½²"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
