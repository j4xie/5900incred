"""
Llama OCEAN ç‰¹å¾ç”Ÿæˆå®Œæ•´æµç¨‹
ä¸€é”®è¿è¡Œè„šæœ¬
"""
import sys
import os
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import kagglehub
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier

# é¡¹ç›®æ¨¡å—
from utils.io import load_lending_club_data, prepare_binary_target
from utils.seed import set_seed
from utils.metrics import compute_all_metrics, delong_test
from text_features.ocean_llama_labeler import OceanLlamaLabeler, OCEAN_DIMS
from utils.ocean_weight_learner import OceanWeightLearner
from utils.ocean_feature_generator import OceanFeatureGenerator
from utils.ocean_evaluator import OceanEvaluator

# è®¾ç½®éšæœºç§å­
set_seed(42)


def main():
    print("=" * 80)
    print("Llama OCEAN ç‰¹å¾ç”Ÿæˆå®Œæ•´æµç¨‹")
    print("=" * 80)
    print()

    # ========== 0. åŠ è½½æ•°æ® ==========
    print("\n[Step 0] åŠ è½½æ•°æ®...")
    print("-" * 80)

    # ç›´æ¥ä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ•°æ®æ–‡ä»¶ï¼ˆé¿å… kagglehub ä¸‹è½½å¡ä½ï¼‰
    file_path = os.path.expanduser("~/.cache/kagglehub/datasets/ethon0426/lending-club-20072020q1/versions/3/Loan_status_2007-2020Q3.gzip")

    if not os.path.exists(file_path):
        print("æœ¬åœ°æ•°æ®ä¸å­˜åœ¨ï¼Œä½¿ç”¨ kagglehub ä¸‹è½½...")
        path = kagglehub.dataset_download("ethon0426/lending-club-20072020q1")
        file_path = path + "/Loan_status_2007-2020Q3.gzip"

    ROW_LIMIT = 10000  # æµ‹è¯•ç”¨ï¼Œå¯ä»¥æ”¹ä¸º None åŠ è½½å…¨éƒ¨æ•°æ®

    df = load_lending_club_data(file_path, row_limit=ROW_LIMIT)
    df = prepare_binary_target(df, target_col="loan_status")

    # æ¸…ç†ç™¾åˆ†æ¯”åˆ—
    percent_cols = ["int_rate", "revol_util"]
    for col in percent_cols:
        if col in df.columns and df[col].dtype == object:
            df[col] = pd.to_numeric(df[col].astype(str).str.rstrip("%"), errors="coerce")

    print(f"\næ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"è¿çº¦ç‡: {df['target'].mean():.2%}")

    # ========== 1. Llama æ‰“æ ‡ç­¾ ==========
    print("\n[Step 1] Llama æ‰“æ ‡ç­¾ï¼ˆç”Ÿæˆ Ground Truthï¼‰...")
    print("-" * 80)
    print("â±ï¸  é¢„è®¡æ—¶é—´: 10-15åˆ†é’Ÿ")
    print("ğŸ’° æˆæœ¬: $0 (å…è´¹)")
    print()

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ ground truthï¼ˆé¿å…é‡å¤æ ‡æ³¨ï¼‰
    ground_truth_path = 'artifacts/ground_truth_llama.csv'
    if os.path.exists(ground_truth_path):
        print(f"âœ… å‘ç°å·²å­˜åœ¨çš„ ground truth: {ground_truth_path}")
        print("   è‡ªåŠ¨ä½¿ç”¨å·²æœ‰æ–‡ä»¶ï¼ˆèŠ‚çœæ—¶é—´ï¼‰")
        df_truth = pd.read_csv(ground_truth_path)
        print(f"   åŠ è½½äº† {len(df_truth)} ä¸ªå·²æ ‡æ³¨æ ·æœ¬")
    else:
        print("æœªæ‰¾åˆ°å·²æœ‰ ground truthï¼Œå¼€å§‹æ–°æ ‡æ³¨...")
        os.makedirs('artifacts', exist_ok=True)
        labeler = OceanLlamaLabeler()
        df_truth = labeler.label_batch(
            df,
            sample_size=500,
            stratified=True,
            rate_limit_delay=0.5
        )
        df_truth.to_csv(ground_truth_path, index=False)
        print(f"\nâœ… Ground Truth å·²ä¿å­˜åˆ°: {ground_truth_path}")

    # è¯„ä¼° Ground Truth è´¨é‡
    evaluator = OceanEvaluator()
    evaluator.evaluate_ground_truth_quality(df_truth)

    # ========== 2. å­¦ä¹ æƒé‡ ==========
    print("\n[Step 2] å­¦ä¹ æƒé‡ï¼ˆRidge Regressionï¼‰...")
    print("-" * 80)

    CATEGORICAL_VARS = [
        'grade', 'purpose', 'term', 'home_ownership',
        'emp_length', 'verification_status', 'application_type'
    ]
    CATEGORICAL_VARS = [c for c in CATEGORICAL_VARS if c in df_truth.columns]

    learner = OceanWeightLearner(method='ridge', alpha=0.1)

    # å‡†å¤‡ OCEAN ground truthï¼ˆé‡å‘½ååˆ—ä»¥åŒ¹é…å­¦ä¹ å™¨æœŸæœ›ï¼‰
    y_ocean_truth = df_truth[[f'{d}_truth' for d in OCEAN_DIMS]].copy()
    y_ocean_truth.columns = OCEAN_DIMS  # é‡å‘½åï¼šopenness_truth â†’ openness

    weights, encoder = learner.fit(
        X_categorical=df_truth[CATEGORICAL_VARS],
        y_ocean_truth=y_ocean_truth,
        cv=5
    )

    # ä¿å­˜æƒé‡
    os.makedirs('artifacts', exist_ok=True)
    weights_path = 'artifacts/ocean_weights_llama.pkl'
    joblib.dump({'weights': weights, 'encoder': encoder}, weights_path)
    print(f"\nâœ… æƒé‡å·²ä¿å­˜åˆ°: {weights_path}")

    # æ˜¾ç¤ºå­¦ä¹ æ‘˜è¦
    print("\nå­¦ä¹ ç»“æœæ‘˜è¦:")
    print(learner.get_summary())

    # ========== 3. ç”Ÿæˆå…¨é‡ OCEAN ç‰¹å¾ ==========
    print("\n[Step 3] ç”Ÿæˆå…¨é‡ OCEAN ç‰¹å¾...")
    print("-" * 80)

    generator = OceanFeatureGenerator(weights, encoder)
    df_full = generator.generate_features(df)

    print("\nâœ… OCEAN ç‰¹å¾å·²æ·»åŠ åˆ°æ•°æ®é›†")

    # è¯„ä¼°é¢„æµ‹èƒ½åŠ›
    evaluator.evaluate_predictive_power(df_full, target_col='target')
    summary = evaluator.generate_summary_report()
    print("\nOCEAN ç‰¹å¾æ±‡æ€»:")
    print(summary)

    # ========== 4. XGBoost A/B å¯¹æ¯”æµ‹è¯• ==========
    print("\n[Step 4] XGBoost A/B å¯¹æ¯”æµ‹è¯•...")
    print("-" * 80)

    # å®šä¹‰ç‰¹å¾
    numeric_features = [
        "loan_amnt", "int_rate", "installment", "annual_inc", "dti",
        "inq_last_6mths", "open_acc", "pub_rec", "revol_bal", "revol_util",
        "total_acc"
    ]
    numeric_features = [c for c in numeric_features if c in df_full.columns]
    categorical_features_model = [c for c in CATEGORICAL_VARS if c in df_full.columns]

    baseline_features = numeric_features + categorical_features_model
    ocean_features = OCEAN_DIMS

    print(f"\nBaseline ç‰¹å¾æ•°: {len(baseline_features)}")
    print(f"OCEAN ç‰¹å¾æ•°: {len(ocean_features)}")

    # é¢„å¤„ç†å™¨
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median"))
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=True))
    ])

    # æ–¹æ¡ˆ A: Baseline
    preprocessor_A = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features_model),
        ],
        remainder="drop"
    )

    # æ–¹æ¡ˆ B: Baseline + OCEAN
    preprocessor_B = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("ocean", "passthrough", ocean_features),
            ("cat", categorical_transformer, categorical_features_model),
        ],
        remainder="drop"
    )

    # Train-Test Split
    y = df_full['target'].values
    X_train, X_test, y_train, y_test = train_test_split(
        df_full, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\nè®­ç»ƒé›†: {len(X_train)} æ ·æœ¬ | è¿çº¦ç‡: {y_train.mean():.2%}")
    print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬ | è¿çº¦ç‡: {y_test.mean():.2%}")

    # è®­ç»ƒæ–¹æ¡ˆ A: Baseline
    print("\nè®­ç»ƒæ–¹æ¡ˆ A: Baseline (æ—  OCEAN)...")
    X_train_A = preprocessor_A.fit_transform(X_train)
    X_test_A = preprocessor_A.transform(X_test)

    pos = int((y_train == 1).sum())
    neg = int((y_train == 0).sum())
    scale_pos_weight = neg / max(1, pos)

    model_A = XGBClassifier(
        objective="binary:logistic",
        tree_method="hist",
        n_estimators=300,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        eval_metric="auc",
        verbosity=0
    )
    model_A.fit(X_train_A, y_train)

    y_proba_A = model_A.predict_proba(X_test_A)[:, 1]
    metrics_A = compute_all_metrics(y_test, y_proba_A)

    print("\næ–¹æ¡ˆ A ç»“æœ:")
    for k, v in metrics_A.items():
        print(f"  {k}: {v:.4f}")

    # è®­ç»ƒæ–¹æ¡ˆ B: Baseline + OCEAN
    print("\nè®­ç»ƒæ–¹æ¡ˆ B: Baseline + OCEAN...")
    X_train_B = preprocessor_B.fit_transform(X_train)
    X_test_B = preprocessor_B.transform(X_test)

    model_B = XGBClassifier(
        objective="binary:logistic",
        tree_method="hist",
        n_estimators=300,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        eval_metric="auc",
        verbosity=0
    )
    model_B.fit(X_train_B, y_train)

    y_proba_B = model_B.predict_proba(X_test_B)[:, 1]
    metrics_B = compute_all_metrics(y_test, y_proba_B)

    print("\næ–¹æ¡ˆ B ç»“æœ:")
    for k, v in metrics_B.items():
        print(f"  {k}: {v:.4f}")

    # å¯¹æ¯”ç»“æœ
    comparison = evaluator.compare_models(y_test, y_proba_A, y_proba_B)

    # ========== 5. ä¿å­˜ç»“æœ ==========
    print("\n[Step 5] ä¿å­˜ç»“æœ...")
    print("-" * 80)

    os.makedirs('artifacts/results', exist_ok=True)

    # ä¿å­˜æ¨¡å‹
    joblib.dump(model_B, 'artifacts/xgb_ocean_llama.pkl')
    print("âœ… æ¨¡å‹å·²ä¿å­˜: artifacts/xgb_ocean_llama.pkl")

    # ä¿å­˜å¯¹æ¯”ç»“æœ
    results_df = pd.DataFrame([
        {'model': 'Baseline', **metrics_A},
        {'model': 'Baseline+OCEAN', **metrics_B}
    ])
    results_df.to_csv('artifacts/results/llama_ocean_results.csv', index=False)
    print("âœ… ç»“æœå·²ä¿å­˜: artifacts/results/llama_ocean_results.csv")

    print("\n" + "=" * 80)
    print("ğŸ‰ å®Œæˆï¼")
    print("=" * 80)
    print("\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(results_df.to_string(index=False))

    print(f"\nğŸ’° æ€»æˆæœ¬: $0")
    print(f"â±ï¸  æ€»è€—æ—¶: ~20åˆ†é’Ÿ")

    # è¯„ä¼°æ˜¯å¦è¾¾æ ‡
    delta_auc = metrics_B['roc_auc'] - metrics_A['roc_auc']
    delta_pr = metrics_B['pr_auc'] - metrics_A['pr_auc']
    delta_ks = metrics_B['ks'] - metrics_A['ks']

    print("\nğŸ¯ è¯„ä¼°æ ‡å‡†ï¼ˆè‡³å°‘æ»¡è¶³ä¸€é¡¹ï¼‰:")
    print(f"  ROC-AUC æå‡ â‰¥ +0.010: {delta_auc:+.4f} {'âœ…' if delta_auc >= 0.010 else 'âŒ'}")
    print(f"  PR-AUC æå‡ â‰¥ +0.008:  {delta_pr:+.4f} {'âœ…' if delta_pr >= 0.008 else 'âŒ'}")
    print(f"  KS æå‡ â‰¥ +1.0:        {delta_ks:+.2f} {'âœ…' if delta_ks >= 1.0 else 'âŒ'}")

    if comparison['significant']:
        print(f"\nâœ… ç»Ÿè®¡æ˜¾è‘—æ€§: p={comparison['delong_p']:.4f} < 0.05")
    else:
        print(f"\nâš ï¸  ç»Ÿè®¡æ˜¾è‘—æ€§: p={comparison['delong_p']:.4f} â‰¥ 0.05 (ä¸æ˜¾è‘—)")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
