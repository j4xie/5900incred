{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Five (OCEAN) Personality Features Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import project modules\n",
    "from utils.seed import set_seed, get_seed\n",
    "from utils.metrics import compute_all_metrics, delong_test, bootstrap_ci, compute_lift\n",
    "from utils.io import load_lending_club_data, prepare_binary_target\n",
    "from text_features.personality import OceanScorer, OCEAN_DIMS\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Text Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = kagglehub.dataset_download(\"ethon0426/lending-club-20072020q1\")\n",
    "file_path = path + \"/Loan_status_2007-2020Q3.gzip\"\n",
    "\n",
    "# Start with 10k samples (adjust as needed)\n",
    "ROW_LIMIT = 10000\n",
    "\n",
    "df = load_lending_club_data(file_path, row_limit=ROW_LIMIT)\n",
    "df = prepare_binary_target(df, target_col=\"loan_status\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available text fields\n",
    "text_fields = ['desc', 'title', 'purpose', 'emp_title']\n",
    "available_fields = [f for f in text_fields if f in df.columns]\n",
    "\n",
    "print(f\"Available text fields: {available_fields}\\n\")\n",
    "\n",
    "# Coverage analysis\n",
    "coverage_stats = {}\n",
    "for field in available_fields:\n",
    "    non_null = df[field].notna().sum()\n",
    "    coverage = non_null / len(df) * 100\n",
    "    avg_len = df[field].dropna().str.len().mean()\n",
    "    coverage_stats[field] = {\n",
    "        'non_null': non_null,\n",
    "        'coverage_pct': coverage,\n",
    "        'avg_length': avg_len\n",
    "    }\n",
    "    print(f\"{field}: {non_null}/{len(df)} ({coverage:.1f}%) | Avg length: {avg_len:.1f} chars\")\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_stats).T\n",
    "coverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge text fields for OCEAN scoring\n",
    "# Use title (always available) + emp_title (high coverage)\n",
    "df['title_clean'] = df['title'].fillna('').astype(str).str.strip()\n",
    "df['emp_title_clean'] = df['emp_title'].fillna('').astype(str).str.strip()\n",
    "\n",
    "# Create combined text for analysis\n",
    "df['text_merged'] = df['title_clean'] + ' | ' + df['emp_title_clean']\n",
    "df['text_length'] = df['text_merged'].str.len()\n",
    "\n",
    "print(f\"Text length statistics:\")\n",
    "print(df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Text Length')\n",
    "\n",
    "# Box plot by loan grade\n",
    "if 'grade' in df.columns:\n",
    "    grade_order = sorted(df['grade'].dropna().unique())\n",
    "    sns.boxplot(data=df, x='grade', y='text_length', order=grade_order, ax=axes[1])\n",
    "    axes[1].set_xlabel('Loan Grade')\n",
    "    axes[1].set_ylabel('Text Length')\n",
    "    axes[1].set_title('Text Length by Loan Grade')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/results/text_coverage_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OCEAN Personality Scoring\n",
    "\n",
    "We'll start in **offline mode** (deterministic fallback) to build the pipeline, then optionally enable API mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OCEAN scorer\n",
    "# Set offline_mode=False and provide OPENAI_API_KEY to use real LLM scoring\n",
    "scorer = OceanScorer(\n",
    "    cache_dir=\"../artifacts/persona_cache\",\n",
    "    offline_mode=True,  # Set to False to enable API calls\n",
    "    max_chars=800\n",
    ")\n",
    "\n",
    "print(\"OCEAN Scorer initialized (offline mode)\")\n",
    "print(f\"Dimensions: {OCEAN_DIMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scoring on a few samples\n",
    "print(\"Testing OCEAN scorer on sample data:\\n\")\n",
    "\n",
    "sample_df = df.head(5)[['title_clean', 'emp_title_clean']]\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "    scores = scorer.score(row['title_clean'], row['emp_title_clean'])\n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"  Title: {row['title_clean'][:50]}\")\n",
    "    print(f\"  Emp: {row['emp_title_clean'][:50]}\")\n",
    "    print(f\"  Scores: {scores}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch scoring on full dataset\n",
    "print(f\"Scoring {len(df)} samples...\\n\")\n",
    "\n",
    "titles = df['title_clean'].tolist()\n",
    "emp_titles = df['emp_title_clean'].tolist()\n",
    "\n",
    "ocean_scores = scorer.score_batch(titles, emp_titles, rate_limit_delay=0.5)\n",
    "\n",
    "# Convert to DataFrame\n",
    "ocean_df = pd.DataFrame(ocean_scores)\n",
    "\n",
    "print(f\"\\nScoring complete!\")\n",
    "print(f\"Stats: {scorer.get_stats()}\")\n",
    "print(f\"\\nOCEAN scores preview:\")\n",
    "print(ocean_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add OCEAN features to main dataframe\n",
    "for dim in OCEAN_DIMS:\n",
    "    df[dim] = ocean_df[dim]\n",
    "\n",
    "print(\"OCEAN features added to dataset\")\n",
    "print(f\"New columns: {OCEAN_DIMS}\")\n",
    "print(f\"\\nOCEAN descriptive statistics:\")\n",
    "print(df[OCEAN_DIMS].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OCEAN distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, dim in enumerate(OCEAN_DIMS):\n",
    "    axes[i].hist(df[dim], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[i].set_xlabel(dim.capitalize())\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Distribution of {dim.capitalize()}')\n",
    "    axes[i].axvline(df[dim].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[dim].mean():.2f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/results/ocean_distributions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of OCEAN features\n",
    "ocean_corr = df[OCEAN_DIMS].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(ocean_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('OCEAN Features Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/results/ocean_correlation.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline features (from your original notebooks)\n",
    "numeric_features = [\n",
    "    \"loan_amnt\", \"int_rate\", \"installment\", \"annual_inc\", \"dti\",\n",
    "    \"inq_last_6mths\", \"open_acc\", \"pub_rec\", \"revol_bal\", \"revol_util\",\n",
    "    \"total_acc\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"term\", \"grade\", \"sub_grade\", \"emp_length\", \"home_ownership\",\n",
    "    \"verification_status\", \"purpose\", \"application_type\"\n",
    "]\n",
    "\n",
    "# Filter to available columns\n",
    "numeric_features = [c for c in numeric_features if c in df.columns]\n",
    "categorical_features = [c for c in categorical_features if c in df.columns]\n",
    "\n",
    "# Clean percentage columns\n",
    "for col in [\"int_rate\", \"revol_util\"]:\n",
    "    if col in df.columns and df[col].dtype == object:\n",
    "        df[col] = pd.to_numeric(df[col].astype(str).str.rstrip(\"%\"), errors=\"coerce\")\n",
    "\n",
    "print(f\"Baseline numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Baseline categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets for A/B comparison\n",
    "# A: Baseline features only\n",
    "features_baseline = numeric_features + categorical_features\n",
    "\n",
    "# B: Baseline + OCEAN\n",
    "features_with_ocean = numeric_features + OCEAN_DIMS + categorical_features\n",
    "\n",
    "print(f\"Feature Set A (Baseline): {len(features_baseline)} features\")\n",
    "print(f\"Feature Set B (Baseline + OCEAN): {len(features_with_ocean)} features\")\n",
    "print(f\"\\nAdded OCEAN features: {OCEAN_DIMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (same seed as baseline)\n",
    "X = df[features_with_ocean].copy()\n",
    "y = df[\"target\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Default rate (train): {y_train.mean():.3f}\")\n",
    "print(f\"Default rate (test): {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training: A/B Comparison\n",
    "\n",
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logreg_pipeline(numeric_cols, categorical_cols):\n",
    "    \"\"\"Build LogReg pipeline with preprocessing.\"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"clf\", LogisticRegression(solver=\"lbfgs\", max_iter=500, class_weight=\"balanced\", random_state=42))\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: Baseline LogReg\n",
    "print(\"Training Model A: LogReg Baseline\\n\")\n",
    "\n",
    "X_train_a = X_train[features_baseline]\n",
    "X_test_a = X_test[features_baseline]\n",
    "\n",
    "numeric_a = [f for f in numeric_features if f in features_baseline]\n",
    "categorical_a = [f for f in categorical_features if f in features_baseline]\n",
    "\n",
    "model_logreg_a = build_logreg_pipeline(numeric_a, categorical_a)\n",
    "model_logreg_a.fit(X_train_a, y_train)\n",
    "\n",
    "y_proba_logreg_a = model_logreg_a.predict_proba(X_test_a)[:, 1]\n",
    "metrics_logreg_a = compute_all_metrics(y_test, y_proba_logreg_a)\n",
    "\n",
    "print(\"Model A Results:\")\n",
    "for k, v in metrics_logreg_a.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B: LogReg with OCEAN\n",
    "print(\"Training Model B: LogReg + OCEAN\\n\")\n",
    "\n",
    "X_train_b = X_train[features_with_ocean]\n",
    "X_test_b = X_test[features_with_ocean]\n",
    "\n",
    "numeric_b = [f for f in numeric_features + OCEAN_DIMS if f in features_with_ocean]\n",
    "categorical_b = [f for f in categorical_features if f in features_with_ocean]\n",
    "\n",
    "model_logreg_b = build_logreg_pipeline(numeric_b, categorical_b)\n",
    "model_logreg_b.fit(X_train_b, y_train)\n",
    "\n",
    "y_proba_logreg_b = model_logreg_b.predict_proba(X_test_b)[:, 1]\n",
    "metrics_logreg_b = compute_all_metrics(y_test, y_proba_logreg_b)\n",
    "\n",
    "print(\"Model B Results:\")\n",
    "for k, v in metrics_logreg_b.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LogReg A vs B\n",
    "print(\"\\n=== LogReg: Baseline vs Baseline+OCEAN ===\")\n",
    "print(f\"{'Metric':<15} {'A (Baseline)':<15} {'B (+OCEAN)':<15} {'Delta':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for key in ['roc_auc', 'pr_auc', 'ks', 'brier', 'ece']:\n",
    "    a_val = metrics_logreg_a[key]\n",
    "    b_val = metrics_logreg_b[key]\n",
    "    delta = b_val - a_val\n",
    "    print(f\"{key:<15} {a_val:<15.4f} {b_val:<15.4f} {delta:+.4f}\")\n",
    "\n",
    "# Statistical test\n",
    "z_stat, p_val = delong_test(y_test, y_proba_logreg_a, y_proba_logreg_b)\n",
    "print(f\"\\nDeLong test: z={z_stat:.3f}, p={p_val:.4f}\")\n",
    "print(f\"Statistically significant at α=0.05: {p_val < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xgb_pipeline(numeric_cols, categorical_cols):\n",
    "    \"\"\"Build XGBoost pipeline with preprocessing.\"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "\n",
    "    # Calculate scale_pos_weight\n",
    "    pos = int((y_train == 1).sum())\n",
    "    neg = int((y_train == 0).sum())\n",
    "    scale_pos_weight = neg / max(1, pos)\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"clf\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            tree_method=\"hist\",\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=42,\n",
    "            eval_metric=\"auc\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: Baseline XGBoost\n",
    "print(\"Training Model A: XGBoost Baseline\\n\")\n",
    "\n",
    "model_xgb_a = build_xgb_pipeline(numeric_a, categorical_a)\n",
    "model_xgb_a.fit(X_train_a, y_train)\n",
    "\n",
    "y_proba_xgb_a = model_xgb_a.predict_proba(X_test_a)[:, 1]\n",
    "metrics_xgb_a = compute_all_metrics(y_test, y_proba_xgb_a)\n",
    "\n",
    "print(\"Model A Results:\")\n",
    "for k, v in metrics_xgb_a.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B: XGBoost with OCEAN\n",
    "print(\"Training Model B: XGBoost + OCEAN\\n\")\n",
    "\n",
    "model_xgb_b = build_xgb_pipeline(numeric_b, categorical_b)\n",
    "model_xgb_b.fit(X_train_b, y_train)\n",
    "\n",
    "y_proba_xgb_b = model_xgb_b.predict_proba(X_test_b)[:, 1]\n",
    "metrics_xgb_b = compute_all_metrics(y_test, y_proba_xgb_b)\n",
    "\n",
    "print(\"Model B Results:\")\n",
    "for k, v in metrics_xgb_b.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare XGBoost A vs B\n",
    "print(\"\\n=== XGBoost: Baseline vs Baseline+OCEAN ===\")\n",
    "print(f\"{'Metric':<15} {'A (Baseline)':<15} {'B (+OCEAN)':<15} {'Delta':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for key in ['roc_auc', 'pr_auc', 'ks', 'brier', 'ece']:\n",
    "    a_val = metrics_xgb_a[key]\n",
    "    b_val = metrics_xgb_b[key]\n",
    "    delta = b_val - a_val\n",
    "    print(f\"{key:<15} {a_val:<15.4f} {b_val:<15.4f} {delta:+.4f}\")\n",
    "\n",
    "# Statistical test\n",
    "z_stat, p_val = delong_test(y_test, y_proba_xgb_a, y_proba_xgb_b)\n",
    "print(f\"\\nDeLong test: z={z_stat:.3f}, p={p_val:.4f}\")\n",
    "print(f\"Statistically significant at α=0.05: {p_val < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold CV for XGBoost (more stable estimates)\n",
    "print(\"Running 5-Fold Cross-Validation...\\n\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Baseline\n",
    "X_baseline = df[features_baseline].copy()\n",
    "cv_scores_a = cross_val_score(model_xgb_a, X_baseline, df['target'], \n",
    "                               cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# With OCEAN\n",
    "X_ocean = df[features_with_ocean].copy()\n",
    "cv_scores_b = cross_val_score(model_xgb_b, X_ocean, df['target'], \n",
    "                               cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(f\"Baseline ROC-AUC: {cv_scores_a.mean():.4f} ± {cv_scores_a.std():.4f}\")\n",
    "print(f\"With OCEAN ROC-AUC: {cv_scores_b.mean():.4f} ± {cv_scores_b.std():.4f}\")\n",
    "print(f\"Mean improvement: {(cv_scores_b.mean() - cv_scores_a.mean()):.4f}\")\n",
    "\n",
    "# Paired t-test\n",
    "from scipy import stats\n",
    "t_stat, t_pval = stats.ttest_rel(cv_scores_b, cv_scores_a)\n",
    "print(f\"\\nPaired t-test: t={t_stat:.3f}, p={t_pval:.4f}\")\n",
    "print(f\"Improvement is significant at α=0.05: {t_pval < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# ROC Curve\n",
    "fpr_a, tpr_a, _ = roc_curve(y_test, y_proba_xgb_a)\n",
    "fpr_b, tpr_b, _ = roc_curve(y_test, y_proba_xgb_b)\n",
    "axes[0, 0].plot(fpr_a, tpr_a, label=f\"Baseline (AUC={auc(fpr_a, tpr_a):.3f})\", linewidth=2)\n",
    "axes[0, 0].plot(fpr_b, tpr_b, label=f\"+ OCEAN (AUC={auc(fpr_b, tpr_b):.3f})\", linewidth=2)\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[0, 0].set_xlabel('False Positive Rate')\n",
    "axes[0, 0].set_ylabel('True Positive Rate')\n",
    "axes[0, 0].set_title('ROC Curve Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec_a, rec_a, _ = precision_recall_curve(y_test, y_proba_xgb_a)\n",
    "prec_b, rec_b, _ = precision_recall_curve(y_test, y_proba_xgb_b)\n",
    "axes[0, 1].plot(rec_a, prec_a, label=f\"Baseline (PR-AUC={metrics_xgb_a['pr_auc']:.3f})\", linewidth=2)\n",
    "axes[0, 1].plot(rec_b, prec_b, label=f\"+ OCEAN (PR-AUC={metrics_xgb_b['pr_auc']:.3f})\", linewidth=2)\n",
    "axes[0, 1].set_xlabel('Recall')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].set_title('Precision-Recall Curve Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Lift Curve\n",
    "lift_a = compute_lift(y_test, y_proba_xgb_a, n_deciles=10)\n",
    "lift_b = compute_lift(y_test, y_proba_xgb_b, n_deciles=10)\n",
    "deciles = np.arange(1, 11)\n",
    "axes[1, 0].plot(deciles, lift_a, 'o-', label='Baseline', linewidth=2)\n",
    "axes[1, 0].plot(deciles, lift_b, 's-', label='+ OCEAN', linewidth=2)\n",
    "axes[1, 0].axhline(1.0, color='k', linestyle='--', linewidth=1)\n",
    "axes[1, 0].set_xlabel('Decile')\n",
    "axes[1, 0].set_ylabel('Lift')\n",
    "axes[1, 0].set_title('Lift Curve by Decile')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Metric Comparison Bar Chart\n",
    "metrics_names = ['ROC-AUC', 'PR-AUC', 'KS']\n",
    "baseline_vals = [metrics_xgb_a['roc_auc'], metrics_xgb_a['pr_auc'], metrics_xgb_a['ks']/100]\n",
    "ocean_vals = [metrics_xgb_b['roc_auc'], metrics_xgb_b['pr_auc'], metrics_xgb_b['ks']/100]\n",
    "\n",
    "x_pos = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x_pos - width/2, baseline_vals, width, label='Baseline', alpha=0.8)\n",
    "axes[1, 1].bar(x_pos + width/2, ocean_vals, width, label='+ OCEAN', alpha=0.8)\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(metrics_names)\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Metric Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/results/model_comparison_plots.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to CSV\n",
    "from datetime import datetime\n",
    "\n",
    "results_summary = pd.DataFrame([\n",
    "    {'model': 'LogReg_Baseline', **metrics_logreg_a},\n",
    "    {'model': 'LogReg_OCEAN', **metrics_logreg_b},\n",
    "    {'model': 'XGB_Baseline', **metrics_xgb_a},\n",
    "    {'model': 'XGB_OCEAN', **metrics_xgb_b}\n",
    "])\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_path = f'../artifacts/results/metrics_{timestamp}.csv'\n",
    "results_summary.to_csv(results_path, index=False)\n",
    "print(f\"Results saved to {results_path}\")\n",
    "\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model (XGBoost + OCEAN)\n",
    "import os\n",
    "os.makedirs('../artifacts', exist_ok=True)\n",
    "\n",
    "model_path = '../artifacts/xgb_ocean_model.joblib'\n",
    "joblib.dump(model_xgb_b, model_path)\n",
    "print(f\"Best model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Conclusions\n",
    "\n",
    "### Acceptance Criteria Check\n",
    "\n",
    "**Target**: Meet at least one of:\n",
    "- ROC-AUC improvement ≥ +0.010\n",
    "- PR-AUC improvement ≥ +0.008\n",
    "- KS improvement ≥ +1.0\n",
    "\n",
    "**Results**: (Fill in after running)\n",
    "- ROC-AUC delta: _____\n",
    "- PR-AUC delta: _____\n",
    "- KS delta: _____\n",
    "\n",
    "### Next Steps\n",
    "1. If metrics improve: Move to [04_explain_shap.ipynb](04_explain_shap.ipynb) for interpretability analysis\n",
    "2. If using offline mode: Enable API mode (`offline_mode=False`) and re-run with real LLM scores\n",
    "3. Scale to full dataset (100k+ samples) for production validation\n",
    "4. Consider alternative text sources or enrichment strategies\n",
    "\n",
    "### Data Limitations\n",
    "- Dataset lacks borrower descriptions (original Yu et al. 2023 paper used self-written text)\n",
    "- Using loan title + employment title as weak personality proxies\n",
    "- Results should be interpreted as **proof-of-concept** for the technical framework\n",
    "- For production use, richer text data would be needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
